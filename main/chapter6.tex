%!TEX root = ../my_thesis.tex

\graphicspath{{main/chapter6/fig/}}

\chapter{Embedded Domain Specific Language for the Software Defined Radio}

This chapter presents a new embedded Domain Specific Language (eDSL) dedicated
to the Software Defined Radio (SDR). The first section discusses of the existing
models and solutions and motivates the need of a new dedicated language for the
SDR.

In a second part a description of the proposed eDSL is given and separated in
two sub-sections: the elementary components are presented first and then the
parallel components are described.

The third part is focusing on the actual implementations of the previously
presented components. Among others, the sequence duplication technique and the
pipeline implementation are discussed.

Finally, the last part shows a concrete use case of the proposed eDSL on the
well-spread DVB-S2 standard. A full digital transceiver has been realized in
software. The DVB-S2 standard is presented from an applicative point of view
(transmitter and receiver) and then it is evaluated on a specific CPU target.

\vspace*{\fill}
\minitoccustom
\vspace*{\fill}

% \begin{itemize}
%   \item \xmark~Algorithmes de synchronisation avant la démodulation
%   \item \xmark~Tâches séquentielles (avec état)
% \end{itemize}

\section{Related Works}

Traditionally the communication systems are implemented on dedicated hardware
(ASIC) targeting high throughputs, low latencies and energy efficiency.
However, the implementations of such solutions have a long time to market, are
expensive and are specific by nature~\cite{Palkovic2010,Palkovic2012}.

The new standards like the 5G are coming with very large specifications and
multiple possible configurations~\cite{ETSI2018}. Small objects that need to
communicate very few data at low rates will live together with 4K video
streaming for mobile phone games which will require high throughputs as well as
low latencies~\cite{Rost2014}.

To meet those various specifications the transceivers will have to be able to
adapt quickly to new configurations. There is a growing need for flexible,
re-configurable and programmable solutions. To match those constraints, there is
a growing interest for the SDR which consists in processing both the Physical
(PHY) and Medium Access Control (MAC) layers in software~\cite{Mitola1993}, as
opposed to the traditionally hardware-based solutions. Short time to market,
lower development costs, interoperability, readiness to adapt to future updates
and new protocols are the mains advantages of the SDR~\cite{Akeela2018}.

The SDR can be implemented on various targets like Field Programmable Gate
Arrays (FPGAs)~\cite{Coulton2004,Skey2006,Dutta2010,Shaik2013,Maheshwarappa2015,
Nivin2016}, Digital Signal Processors (DSPs)~\cite{Kaur2008,Karlsson2013,
Shaik2013} or General Purpose Processors (GPPs)~\cite{Yoge2012,Bang2014,
Meshram2019,Grayver2020}. Many elementary blocks of digital communication
systems has been optimized to run fast on x86 and ARM
CPUs~\cite{Cassagne2015c,Cassagne2016a,Cassagne2016b,Cassagne2018,Leonardon2019,
Ghaffari2019} and have been packaged in AFF3CT~\cite{Cassagne2017a,
Cassagne2019a}. Even if there is some interesting results in term of throughput
on GPUs~\cite{Xianjun2013,Li2014,LeGal2014a,Giard2016b,Keskin2017a}, the
achieved latency on this architecture is still too high to meet real time
constraints and cannot compete with existing CPU
implementations~\cite{LeGal2015a,Cassagne2015c,Giard2016b,Cassagne2016a,
LeGal2017,Leonardon2019,LeGal2019a}. This is mainly due to data transfers
between the host (CPUs) and the device (GPUs), and to the intrinsic nature of
the GPU architecture which is not optimized for latency efficiency.

\subsection{Dataflow Model}

The Shanon's communication model presented before can be refined in a way
that the transmitter and the receiver are decomposed into many processing
blocks. Those blocks are mainly connected to each other in an directed graph.
This perfectly matches the dataflow model~\cite{Dennis1980,Ackerman1982}: the
blocks are the filters and the binding links between the blocks represent the
data movements. The dataflow model allows to describe the system from a high
level point of view and to perform optimizations independently of the system
designer. In the case of the SDR, simpler models than the generalized dataflow
can be used like the synchronous dataflow~\cite{Lee1987} or the cyclo-static
dataflow~\cite{Engels1994,Bilsen1995}. This allows to perform aggressive
simplifications like a static scheduling of the filters
execution~\cite{Parks1995}.

\subsection{Dedicated Languages}

Many languages dedicated to streaming applications have been
developed~\cite{Buck2004,Amarasinghe2005,Liao2006,Black-Schaffer2010,Glitia2010,
Thies2010,DeOliveiraCastro2017}. Streaming applications are most of the time
represented with the dataflow model and the dedicated languages often support
the general or the cyclo-static dataflow model. They also very often come with
automatic parallelism mechanisms like pipelining and forks/joins.

\subsection{GNU Radio}

There are few solutions targeting specifically the SDR sub-domain yet. The most
famous is GNU Radio~\cite{GNURadio} which is open source and largely adopted by
the community. The software is bundled with a large variety of algorithms used
in real life systems. GNU Radio models the digital communication systems at the
symbol level: this philosophy is very close to the algorithms descriptions that
can be found in the signal community literature. Thus, it allows quick
implementation of new algorithms. Still, this is a limitation to meet high
throughputs and low latencies constraints on current GPPs architectures.

\section{Description of the Proposed Embedded Domain Specific Language}

This section details, through \AFFECT, an embedded domain specific language
(eDSL) working on sets of symbols (aka frames). \AFFECT implements a form of the
synchronous dataflow (SDF) model specialized to the relevant characteristics of
FEC communication chains enabling to perform more aggressive optimizations than
GNU Radio, at the cost of lower generality.

Many \Cxx eDSLs are based on the template meta-programming technique. This
allows to perform computations at the compilation time instead of during the
program execution. In \AFFECT we decided to not use the \Cxx template
meta-programming for the eDSL. This choice has been made to match the existing
internal organization of the project and more precisely the \emph{modules},
\emph{tasks} and \emph{sockets}. It was also comforted by one of the main draw
back of the template-based eDSLs: the errors display and management. A
non-negligible part on the proposed eDSL is dedicated to error verifications and
clear messages display. Of course the static scheduling implied by the
synchronous dataflow model could theoretically be resolved at the compilation
time, instead of that, the scheduling of the tasks is precomputed at the
runtime and the execution order is stored in an array of function pointers
(\verb|std::function|). The cost of the execution of the tasks is very cheap
(consecutive function calls): the overhead is negligible compared to a
template-based solution.

\subsection{Elementary Components}

% \begin{itemize}
%   \item \xmark~routeurs = permet d'aiguiller vers différentes sous-séquences
%   \item \cmark~limitation : dire que les boucles ne peuvent pas être séparées
%     dans plusieurs étages du pipeline.
% \end{itemize}

The proposed eDSL comes with a set of elementary components: the
\emph{sequence}, \emph{module}, \emph{task} and \emph{socket}. The
\emph{module}, \emph{task} and \emph{socket} have been reused and enriched
from the \AFFECT library (cf Section~\ref{sec:soft_archi}).

In the eDSL the \emph{task} is the central component, it is also known as the
\emph{filter} in the standard dataflow model. A task is an elementary component,
it can be an encoder, a decoder or a modulator processing for instance. A task,
unlike a dataflow filter, can have an internal state and a private memory to
store temporary data for instance. If the life time of the private data exceeds
the task execution then the data will be contained by the \emph{module}.
Additionally a set of tasks can share the same internal/private memory, in that
particularly case, multiple tasks are regrouped in a single module. This
behavior is not recommended by the standard dataflow model, and should be
avoided in a fully dataflow-compliant model. The main problem with internal
memory is that the tasks cannot be executed safely by many threads in parallel
because of data races. However, in many situations the writing of a task or a
set of tasks can be simplified by relieving this constraint. Furthermore,
storing private data in the module can be, in some cases, a way to avoid to
allocate memory for each task execution which is a very expensive operation.

A task can consume and produce public data. To this purpose, each task exposes
input and/or output \emph{sockets}. The action of connecting the sockets of
different tasks is called the \emph{binding} (cf.
Fig.~\ref{fig:soft_archi_com_chain_task_module}).
The binding determines the tasks execution order.

Tasks that have been bound together can be regrouped in what we call a
\emph{sequence} of tasks. The sequence notion reminds us that the tasks are
executed sequentially one by one in a pre-defined static order (like in the SDF
model). To create a sequence the user has to give the first tasks and the
last tasks to execute. Then the connected tasks will be analyzed and a sequence
object will be built. The analysis is a deep traversal of the tasks graph. The
order in which the tasks have been traversed is memorized in the sequence.
When the user call the \verb|exec| method on a sequence, the tasks constituting
it are executed one by one in the memorized order (static scheduling).

The proposed eDSL is targeting streaming applications and more precisely signal
processing and digital communication chains. In this type of applications, the
processing is repeated indefinitely on a batch of frames when the system is on.
So, a sequence is executed in loop, when the last task is executed, the next
task is the first one. The user can control if the sequence should restart by
giving a \emph{condition function} when calling the \verb|exec| method on the
sequence. The prototype of the function is \verb|bool cond_func(void)|: if the
function returns \emph{false}, the sequence is repeated, else, if the function
returns \emph{true}, the sequence is stopped.

\begin{figure}[htp]
  \centering
  \subfloat[][Simple chain sequence.]{\includegraphics[width=0.485\textwidth]{dsl/sequence/sequence_chain}\label{fig:dsl_sequence_chain}}
  \quad
  \subfloat[][Sequence with multiple first and last tasks.]{\includegraphics[width=0.485\textwidth]{dsl/sequence/sequence_generic}\label{fig:dsl_sequence_generic}}
  \caption{Example of sequences.}
  \label{fig:dsl_sequence}
\end{figure}

Fig.~\ref{fig:dsl_sequence} shows two examples of sequence.
Fig.~\ref{fig:dsl_sequence_chain} is a simple chain of tasks, the user only
needs to specify the first task of the chain ($t_1$) and the sequence analysis
will automatically follow the binding until the last task ($t_4$).
Fig.~\ref{fig:dsl_sequence_generic} is more complicated, there are bound tasks
before and after the wanted sequence. There is also two \emph{first} tasks
($t_1$ and $t_3$) and two \emph{last} tasks ($t_5$ and $t_6$). In this case, the
user needs to explicitly specify that $t_1$ and $t_3$ are first tasks, if $t_1$
is given before $t_3$ then $t_1$ will be executed first and $t_3$ after. In fact
the analysis will start from $t_1$ and continue to traverse new tasks if
possible. In  the example, $t_2$ can be executed directly after $t_1$ but $t_4$
cannot because it depends on $t_3$, so the analysis will stop after $t_2$ and
will restart from $t_3$. The index $i$ of the $t_i$ task represents the
execution order. The $t_5$ and $t_6$ last tasks have to be given by the user
because there are other tasks bound to their output sockets: the analysis cannot
guess the end of the sequence alone.

In general, all the tasks of a sequence are repeated in the same predefined
order, but it some particular cases, a task can raise the
\verb|tools::processing_aborted| exception to restart the sequence before its
end. For instance, in Fig.~\ref{fig:dsl_sequence_chain} if the $t_3$ task raises
the \verb|tools::processing_aborted| exception, the next executed task will be
$t_1$ instead of $t_4$.

Some digital communication scenarios include repeated schemes. To map a repeated
scheme, we introduced a specific type of task: the \emph{loop}. This task is
executed one or more times depending on a \emph{condition}. A loop is bound to
two sub-sequences of tasks, the first one is executed and repeated while the
condition is \emph{false} and the second one is taken when the condition is
\emph{true}.
% During the sequence analysis, the type of the tasks are tested
% (\verb|dynamic_cast|), if a loop is detected, then the sequence is split in 3
% sub-sequences of tasks. When the \verb|exec| method is called then the first
% sub-sequence is executed then the loop task is evaluated in a ``real'' \Cxx
% \emph{for-loop}, if the loop task returns \emph{false} the sub-sequence 2 is
% executed until the loop task returns \emph{true} and then the sub-sequence 3 is
% executed.

\begin{figure}[htp]
  \centering
  \includegraphics[width=1.00\textwidth]{dsl/loop/loop}
  \caption
    [Example of a sequence of tasks with a loop.]
    {Example of a sequence of tasks with a loop.}
  \label{fig:dsl_loop}
\end{figure}

Fig.~\ref{fig:dsl_loop} illustrates a sequence of tasks with a loop. In this
particular case, the loop condition is based on an input socket so the condition
evaluation is dynamic and depends on the runtime values of this input socket.
In a first place, the sub-sequence 1 will be executed; then the loop condition
is evaluated; if the condition return \emph{false} the sub-sequence 2 is
executed and the loop condition is re-evaluated until it returns \emph{true}. At
this point the sub-sequence 3 will be executed. To enable a static binding, the
number of input and output sockets of the loop are duplicated. After the
execution on the sub-sequence 1, the convention in the loop in to use the input
sockets 1 and 3. The input socket 1 is only used for the condition evaluation
whereas the input socket 3 is simply forwarded to the output socket 1 and 2. If
the sub-sequence 2 is executed, the input sockets 2 and 4 will be used until the
end of the loop.

In the example the loop uses an input socket to evaluate the condition, this is
common in iterative demodulation/decoding schemes when the overall system can
have an early termination criterion like a CRC detection. Of course it is also
possible to override the loop behavior by inheriting from it, and the condition
evaluation process can be modified by the user. The loop can simply be a
predicate: in this case, the condition evaluation does not require any input
socket. The predicate can simply be a counter, each time the condition is
evaluated the counter is incremented and when the counter reaches a given value
the condition evaluation returns \emph{true} (\emph{for-loop} behavior).

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.85]{dsl/nested_loops/nested_loops}
  \caption
    [Nested loops.]
    {Nested loops.}
  \label{fig:dsl_nested_loops}
\end{figure}

The overall system also supports nested loops. The idea is to regroup the tasks
in sub-sequences: one before the loop and two after. This is implemented as a
binary tree, each time a loop is encountered, two new paths are created.
Fig.~\ref{fig:dsl_nested_loops} shows an example with two loops ($L_1$ and
$L_2$). Five sub-sequences of tasks are created ($SS_x$), one before $L_1$
($SS_1$), two after $L_1$ ($SS_2$ and $SS_5$) and two after $L_2$ ($SS_3$ and
$SS_4$). In this example $L_2$ is a nested loop because it is an inner loop
within the body of an outer one ($L_1$).

\subsection{Parallel Components}

\begin{figure}[htp]
  \centering
  \includegraphics{dsl/sequence_dup/sequence_dup}
  \caption
    [Sequence duplication for multi-threaded execution.]
    {Sequence duplication for multi-threaded execution.}
  \label{fig:dsl_sequence_dup}
\end{figure}

A sequence can be duplicated when created, this way many threads can execute the
sequence in parallel. The $t$ number of duplications (= number of threads) is a
parameter of its constructor. As shown in Fig.~\ref{fig:dsl_sequence_dup}, one
thread will be affected to one duplicated chain. This way a sequence is able to
take advantage of the multi-core architectures. For instance, the \AFFECT
simulator, extensively uses the sequence duplication feature to parallelize
automatically the processing. The duplication strategy is efficient since there
is no need for synchronization between the threads. Each threaded sequence can
be executed on one dedicated core and the public data transfers remain on this
core for the data reuse in the caches. However this parallelism is only possible
if the tasks themselves can be duplicated.

In some particular cases like in the signal synchronization processing, the
tasks can have a dependency on themselves. It is then impossible to duplicate
the sequence because of the sequential nature of these tasks. To overcome this
issue the well-known pipelining strategy can be used to increase the sequence
throughput up to the slowest task throughput. The proposed eDSL comes with a
specific \emph{pipeline} component to this purpose. The pipeline takes multiple
sequences as input. Each sequence of the pipeline is called a \emph{stage}. A
pipeline stage is run on one thread. For instance, a 4-stage pipeline will
create 4 threads, one thread per stage. A pipeline stage can be combined with
the sequence duplication strategy: there will be nested threads in the current
stage thread. Be aware that the pipelining strategy comes with an extra
synchronization cost between the stage threads. The implementation details will
be discussed in the next section. It worth mentioning that is is not possible to
split a loop in separated pipeline stages.

\section{Implementation Details}

\subsection{Implicit Rules}

In the proposed eDSL, an input socket can only be bound to one output socket
while an output socket can be bound to multiple input sockets. A task can only
be executed if all its input sockets are bound. In a sequence, the scheduling of
the tasks is defined by the binding order. The general rule in the sequence
analysis is to add a task to the array of function pointers when its last input
socket is discovered in the deep traversal of the tasks graph. After that, the
output sockets of the current task are followed to reach new tasks. The new
tasks are discovered in the order in which they were bound by the user.

\subsection{Sequence Duplication}

In order to duplicate sequences, we implemented a \emph{clone} method in the
modules. The \emph{clone} method is polymorphic and defined in the \emph{Module}
abstract class. It relies on the implicit copy constructors and a \emph{deep
copy} protected method (overridable). The clone method prototype is
\verb|module::Module* clone() const|. In an implementation (\emph{ModuleImpl})
of the abstract \emph{Module} class, a covariant return type is used:
\verb|module::ModuleImpl* clone() const|. The \emph{clone} method implementation
first calls the implicit copy constructor of the \emph{ModuleImpl} class and
secondly calls the \emph{deep copy} protected method. It is the responsibility
of the \emph{ModuleImpl} developer to correctly override the \emph{deep copy}
method.

The \emph{deep copy} method helps with pointer and reference members. If the
pointer/reference members are read-only (\verb|const|), then the implicit copy
constructor will copy the memory addresses automatically. The problem comes
when there is writable pointer/reference members. If the current
\emph{ModuleImpl} class possesses one ore more writable references this means
that the module can't be cloned. The tasks of the module will be sequential. In
the particular case of a writable pointer member, the developer can explicitly
allocate a new pointer in the \emph{deep copy} method. Note that if a task does
not implement the \emph{clone} method, the eDSL outputs an error message during
the sequence analysis if the user tries to make a duplication.

In a fully dataflow-compliant model, there is no need to duplicate the sequence
because a filter (or task) is always thread-safe. In the proposed eDSL, we had
to introduce the \emph{clone} method because a task can have an internal state
and use private memory (stored in the module) and so a task is not thread-safe.

\subsection{Processes}

A sequence encapsulates a set of tasks and gives the opportunity to execute
these tasks in a predefined order. The user can also execute tasks explicitly,
outside sequences. When using a sequence, the analysis is able to match specific
patterns (known configurations of bound tasks) and replace them by a more
efficient source code. To this purpose the notion of \emph{process} has been
introduced: in a sequence each task is encapsulated in a \emph{process} (this
has nothing to do with OS processes). For the majority of the tasks, the
\emph{process} just executes the task. But for some specific patterns, the task
execution source code is replaced by a more efficient one. The pattern detection
is based on a \Cxx introspection feature: during the analysis, for each parsed
task, we try to cast (\verb|dynamic_cast|) the corresponding module in a
specific class.

\subsection{Pipeline}

\begin{figure}[htp]
  \centering
  \subfloat[][Description of a pipeline: tasks creation, tasks binding and
              sequences/stages definition (with the corresponding number of
              threads).]{
    \includegraphics[scale=1]{dsl/pipeline/pipeline_usr}
    \label{fig:dsl_pipeline_usr}}
  \\
  \subfloat[][Automatic parallelization of a pipeline description: sequence
              duplications, $1$ to $n$ and $n$ to $1$ adaptors creation and
              binding.]{
    \includegraphics[width=1\textwidth]{dsl/pipeline/pipeline_adp}
    \label{fig:dsl_pipeline_adp}}
  \caption{Example of a pipeline description and the associate transformation
    with adaptors.}
  \label{fig:dsl_pipeline}
\end{figure}

\begin{listing}[htp]
  \inputminted[frame=lines,linenos]{C++}{main/chapter6/src/pipeline/pipeline.cpp}
  \caption{\AFFECT \Cxx eDSL source code of the pipeline described in
    Fig.~\ref{fig:dsl_pipeline}.}
  \label{lst:dsl_pipeline}
\end{listing}

Fig.~\ref{fig:dsl_pipeline} shows the difference between a pipeline description
(cf. Fig.~\ref{fig:dsl_pipeline_usr}) and its actual instantiation (cf.
Fig.~\ref{fig:dsl_pipeline_adp}). In Fig.~\ref{fig:dsl_pipeline} we suppose
that the $t_1$, $t_5$ and $t_6$ tasks cannot be duplicated (filled in blue). The
user knows that the execution time of the $t_1$ task is higher than the cumulate
execution time of $t_5$ and $t_6$ tasks. We assume that the cumulate execution
time of $t_2$, $t_3$ and $t_4$ is approximatively four times higher than $t_1$.
This knowledge motivates the cutting of the stages 1, 2 and 3. There is no need
to split the $t_5$ and $t_6$ tasks in two stages because the overall throughput
is limited by the slowest stage ($t_1$ here). The stage 2 is duplicated four
times to increase its throughput by four as we know that its latency is
approximatively four times the sequence 1. In general, a primarily profiling
phase of the sequential code is required to delineate the pipeline stages.
Listing~\ref{lst:dsl_pipeline} presents the \Cxx eDSL source code corresponding
to the pipeline description in Fig.~\ref{fig:dsl_pipeline_usr}. Each $t_i$ task
is contained (as a method) in the $M_i$ module (or class). The four main steps
are: 1) the creation of the modules; 2) the binding of the tasks; 3) the
creation of the pipeline; 4) the pipeline execution.

Fig.~\ref{fig:dsl_pipeline_adp} presents the internal structure of the pipeline.
As we can see, new tasks have been automatically added: \emph{push 1},
\emph{pull n} shared by a \emph{1 to n adaptor} module and \emph{push n},
\emph{pull 1} shared by a \emph{n to 1 adaptor} module. The binding as been
modified to insert the tasks of the adaptors. For instance, in the initial
pipeline description, $t_1$ was bound to $t_2$. In a parallel pipelined
execution this not possible anymore because many threads are running
concurrently, one for $t_1$, four for $t_2$ and one for $t_3$ in the example. To
this purpose, the adaptors implement a producer-consumer algorithm, the yellow
diamonds represent the buffers of the producer-consumer algorithm. The
\emph{push 1} and \emph{pull 1} tasks can only be executed by a unique thread
while the \emph{pull n} and \emph{push n} tasks support to be executed by
multiple threads concurrently. The \emph{push 1} task copies its input socket in
one buffer each time it is called. There is one buffer per duplicated sequence
(or thread). To guarantee that the order of the input frames will be conserved,
a round-robin scheduling has been adopted (at the first call, a copy to the
first buffer is performed, at the second call, a copy to the second buffer is
performed and so on). On the other side, the \emph{pull 1} task is copying, with
the same round-robin scheduling, the data from the buffers to its output socket.

At the pipeline creation, it is possible to select the size of the
synchronization buffers in the adaptors. The default buffer size is one (the
buffers can only contain the data of one \emph{push 1} input sockets). During
the copy of the input sockets data in one of the buffers, the corresponding
thread cannot access the data until the copy is finished (the synchronization is
automatically managed by the framework). When the \emph{push 1} task (producer)
finishes the copy, it increments a \emph{head} counter by one then the
corresponding \emph{pull n} task (consumer) checks that the \emph{head}
counter is higher than the \emph{tail} counter and copies the data from the
buffer to its output socket. When this copy is over, the producer increments the
\emph{tail} counter by one. If the buffer is full, the producer (\emph{push 1
and n} tasks) has to wait, same for the consumer (\emph{pull 1 and n} tasks) if
the buffer is empty. We implemented both active and passing waiting.

% The active waiting is a \emph{while-loop} over a condition (empty or full
% buffer). The active waiting is the most efficient waiting strategy but it
% requires to actively dedicate one core to the waiting. This is why, depending on
% the hardware resources and the implemented pipeline, it may be better to use the
% passive waiting alternative. The implementation of the passive waiting has been
% achieved with the \Cxy{11} condition variables. In this case, when the current
% thread is calling the \emph{wait} method over a condition variable, then the
% core attached to thread is released and can perform other tasks during the
% waiting. Waking up a waiting thread is made with the \emph{notify} method of the
% condition variable. Waking up a thread come with an extra cost because of
% required context switch, this is the main drawback of this method compared to
% the active waiting. This overhead can be mitigated by using larger
% synchronization buffers to overlap the thread context switch incompressible
% time.

In the previously described implementation the copies from and to the buffers
can take a non-negligible amount of time. Thanks to the processes encapsulation
detailed before, we were able to replace the copies by pointers switches. The
idea is to dynamically re-bind the tasks just before and just after the
\emph{push} and \emph{pull} tasks. We also need to bypass the regular execution
in the \emph{push 1}, \emph{pull n}, \emph{push n} and \emph{pull 1} tasks. The
processes that encapsulate these tasks dynamically replace the source code of
the data buffer copy by a simple pointer copy. The pointers are exchanged
cyclically.

In Fig.~\ref{fig:dsl_pipeline_adp}, the pipeline threads are pinned to specific
CPU cores. This is the direct consequence of the lines 38-43 in
Listing~\ref{lst:dsl_pipeline}. The \emph{hwloc} library~\cite{Broquedis2010}
has been used and integrated in \AFFECT to pin the software threads to
processing units (PUs or hardware threads). In the given example, we assume that
the CPU cores can only execute one hardware thread (SMT off) and so an
\emph{hwloc} PU is equal to a CPU core. The threads pinning is given by the user
an can help to improve the multi-threading performance when dealing with NUMA
architectures.

% \begin{itemize}
%   \item \xmark~parler du cas où un binding saute un voire plusieurs étages du
%     pipeline
%   \item \xmark~expliquer la flexibilité des adaptors qui peuvent synchroniser
%     autant de sockets que nécessaire en même temps
%   \item \xmark~expliquer les cas particuliers et tricky du mode 0-copy (quand il
%     y a plusieurs sockets et tâches connectées à un adaptor)
%   \item \xmark~donner un algo du pipeline ?
% \end{itemize}

Fig.~\ref{fig:dsl_pipeline} is an example of a simple chain of tasks. More
complicated task graphs can have more than two tasks to synchronize between two
pipeline stages. The adaptors implementation can manage multiple sockets
synchronization, the key idea is to deal with a 2-dimensional array of buffers.
An other pathologic case is when a task $t_1$ is in stage 1 and possesses an
output socket bound to an other task $t_{x}$ which is located in the stage 4.
To work, the pipeline adaptors between the stage 1 and 2 and 2 and 3
automatically synchronize the data of the $t_1$ output socket.

\section{Application on the DVB-S2 Standard}

The second generation of Digital Video Broadcasting standard for Satellite
(DVB-S2)~\cite{ETSI2005} is a flexible standard designed for broadcast
applications. The DVB-S2 is typically used for the digital television (HDTV
with H.264 source coding). In this section, a concrete use case of the \AFFECT
eDSL is studied. The full DVB-S2 transmitter and receiver are implemented in a
SDR-compliant system. Two Universal Software Radio Peripherals (USRPs)
N320\footnote{USRP N320: \url{https://www.ettus.com/all-products/usrp-n320/}.}
have been used for the analogical signal transmission and reception where all
the digital treatments of the system have been implemented under \AFFECT. The
purpose of this section is not to detail precisely all the implemented tasks
but to expose the system as a whole. Some specific focuses will be given to
describe the main encountered problems and the adopted solutions.

\subsection{Transmitter}

\begin{figure}[htp]
  \centering
  \includegraphics[width=1.0\linewidth]{dvbs2/transmitter/transmitter}
  \caption
    [DVB-S2 transmitter.]
    {DVB-S2 transmitter.}
  \label{fig:dvbs2_transmitter}
\end{figure}

Fig.~\ref{fig:dvbs2_transmitter} shows the DVB-S2 transmitter decomposition in
tasks. The tasks filled in blue are intrinsically sequential and cannot be
duplicated. The initial information bits are read from a binary file
($t^\text{Tx}_1$). Then the DVB-S2 coding scheme rests upon the serial
concatenation of a BCH ($t^\text{Tx}_3$) and a LDPC code ($t^\text{Tx}_4$). The
selected modulation ($t^\text{Tx}_6$) is a Phase-Shift Keying (PSK). The
scrambler tasks ($t^\text{Tx}_2$ and $t^\text{Tx}_8$) apply predefined repeated
sequences of \emph{xor} to the frame in order to avoid too long sequences of the
same bit or symbol in the frames sent by the radio ($t^\text{Tx}_{10}$).
Depending on the DVB-S2 configuration (MODCOD) the frame can be interleaved
($t^\text{Tx}_5$) after the encoding process. If there is no interleaver, then
the frame is simply copied. After the modulation, Pay Load Header (PLH) and
pilots are inserted ($t^\text{Tx}_7$). These extra data are used by the
synchronization tasks in the receiver. Before the radio transmission
($t^\text{Tx}_{10}$), the signal bandwidth is rescaled in the shaping filter
($t^\text{Tx}_9$).

\begin{table}[htp]
  \centering
  \caption
    [Selected DVB-S2 configurations (MODCOD).]
    {Selected DVB-S2 configurations (MODCOD).}
  \label{tab:dvbs2_modcod}
  % {\resizebox{\linewidth}{!}{
  \begin{tabular}{c c c c c c c c}
    \textbf{Config.} & \textbf{Modulation} & \textbf{Rate} $\bm{R}$ & $\bm{K_\text{\textbf{BCH}}}$ & $\bm{K_\text{\textbf{LDPC}}}$ & $\bm{N_\text{\textbf{LDPC}}}$ & $\bm{N_\text{\textbf{PLH}}}$ & \textbf{Interleaver}\\
    \hline \hline
    MODCOD 1 &  QPSK & 3/5 &  9552 &  9720 & 16200 & 16740 & no\\
    MODCOD 2 &  QPSK & 8/9 & 14232 & 14400 & 16200 & 16740 & no\\
    MODCOD 3 & 8-PSK & 8/9 & 14232 & 14400 & 16200 & 16740 & column/row\\
  \end{tabular}
  % }}
\end{table}

The DVB-S2 defines 32 different configurations or \emph{MODCODs}. This work
focuses on the 3 MODCODs given in Table.~\ref{tab:dvbs2_modcod}. Depending on
the MODCOD, the PSK modulation and the LDPC code rate $R$ vary. In the MODCOD 1
and 2 there is no interleaver and the MODCOD 3 uses a column/row interleaver.
$K_\text{BCH}$ or $K$ is the number of information bits and the input size of
the BCH encoder. $N_\text{BCH}$ or $K_\text{LDPC}$ is the output size of the
BCH encoder and the input size of the LDPC encoder. $N_\text{LDPC}$ is the
output size of the LDPC encoder. $N_\text{PLH}$ or $N$ is the frame size
containing $N_\text{LDPC}$ bits plus the PLH and pilots bits.

The DVB-S2 transmitter has been split in 3 pipeline stages, the stages 1 and 3
are sequential and the stage 2 is parallel. The transmitter is not the most
resources consuming part. A standard Intel\R Core\TM i7 CPU with 4 cores (SMT
on) has been used. One core has been reserved for the radio thread (stage 3),
one hardware thread for the \emph{source} (stage 2) and the five
remaining hardwares threads have been dedicated to the stage 2.

\subsection{Receiver}

\begin{figure}[htp]
  \centering
  \subfloat[][Waiting phase and learning phase 1 \& 2.]{
    \includegraphics[width=1.0\linewidth]{dvbs2/receiver/receiver_learning}
    \label{fig:dvbs2_receiver_learning}}
  \\
  \subfloat[][Learning phase 3 \& transmission phase.]{
    \includegraphics[width=1.0\linewidth]{dvbs2/receiver/receiver_transmission}
    \label{fig:dvbs2_receiver_transmission}}
  \caption
    [DVB-S2 receiver.]
    {DVB-S2 receiver.}
  \label{fig:dvbs2_receiver}
\end{figure}

Fig.~\ref{fig:dvbs2_receiver} presents the tasks decomposition of the DVB-S2
receiver with the five distinct phases. The first one is called the
\emph{waiting phase} (cf. Fig.~\ref{fig:dvbs2_receiver_learning}), it consists
in waiting until a transmitter starts to emit. The \emph{Synchronizer
Frame} task ($t^\text{Rx}_8$) possesses a frame detection criterion. When a
signal is detected, the \emph{learning phase 1} (cf.
Fig.~\ref{fig:dvbs2_receiver_learning}) is run during 150 frames, after that the
\emph{learning phase 2} (cf. Fig.~\ref{fig:dvbs2_receiver_learning}) is also run
during 150 frames. After the \emph{learning phase 1 and 2} the tasks have to be
re-bound for the \emph{learning phase 3}
(cf. Fig.~\ref{fig:dvbs2_receiver_transmission}). This last learning phase is
made over 200 frames. After 500 frames for all the learning phases, the final
\emph{transmission phase} is established (cf.
Fig~\ref{fig:dvbs2_receiver_transmission}).

In a real life communication systems, the internal clocks of the radios can
differ slightly and specific processing has to be added in order to be
resilient: this is achieved by the the \emph{Synchronizer Timing} tasks
($t^\text{Rx}_5$ and $t^\text{Rx}_6$). Similarly, the radio transmitter
frequency is not exactly the same as the receiver frequency, the
\emph{Synchronizer Frequency} tasks ($t^\text{Rx}_3$, $t^\text{Rx}_{10}$ and
$t^\text{Rx}_{11}$) recalibrate the signal to recover the emitted symbols.
Finally the LDPC is a block coding scheme and requires to know precisely the
first and last bits of the codeword. The \emph{Synchronizer Frame} task
($t^\text{Rxt}_8$) uses the \emph{Pay Load Headers} (PLH) and pilots bits
inserted by the transmitter \emph{Framer PLH} task ($t^\text{Tx}_8$) to recover
the first and last symbols. One can notice that the \emph{Synchronizer Timing}
module is composed by two separated tasks (\emph{synchronize} or $t^\text{Rx}_5$
and \emph{extract} or $t^\text{Rx}_6$). This behavior is different from the
other \emph{Synchronizer} modules. The \emph{synchronize} task ($t^\text{Rx}_5$
or $t^\text{Rx}_{3,4,5}$) has two output sockets, one for the regular data and
an other for a mask. The regular data and the mask are then used by the
\emph{extract} task ($t^\text{Rx}_5$) to filter which data will be selected or
not for the next task. This specific implementation has been retained for two
reasons: 1) the \emph{Synchronizer Timing} tasks ($t^\text{Rx}_5$ and
$t^\text{Rx}_6$) have a high latency compared to the others tasks and splitting
the treatment in two tasks is a way to increase the throughput of the the
pipeline (this will be discussed more precisely after) and 2) the \emph{extract}
task ($t^\text{Rx}_6$) introduces a new possible behavior: in some case the task
does not have enough samples to produce a frame. In this particular case the
\emph{extract} task raises the \verb|tools::processing_aborted| exception.
The exception is caught and the sequence restarts from the first task
($t^\text{Rx}_1$). This implies to manage a buffer of samples in the
\emph{extract} task ($t^\text{Rx}_6$), if the buffer contains more than one
frame then the next task ($t^\text{Rx}_7$ ) can be executed, else the sequence
needs to restart.

During the waiting and learning phases 1 and 2, the \emph{Synchronizer Freq.
Coarse}, the \emph{Filter Matched} and a part of the \emph{Synchronizer Timing}
have to work symbol by symbol, and have been regrouped in the \emph{Synchronizer
Pilot Feedback} task ($t^\text{Rx}_{3,4,5}$). $t^\text{Rx}_{3,4,5}$ also
requires a feedback input from the \emph{Synchronizer Frame} task
($t^\text{Rx}_8$) to work. This behavior is no longer required in the next
phases and so the $t^\text{Rx}_{3,4,5}$ task has been split in $t^\text{Rx}_3$,
$t^\text{Rx}_4$ and $t^\text{Rx}_5$ and the feedback from the $t^\text{Rx}_8$
second output socket is left unbound.

\begin{figure}[htp]
  \centering
  \includegraphics[width=1.0\linewidth]{dvbs2/bfer/bfer}
  \caption
    [DVB-S2 BER and FER decoding performance.]
    {DVB-S2 BER/FER decoding performance (LDPC BP h-layered, min-sum, 10 ite.).}
  \label{fig:dvbs2_bfer}
\end{figure}

Fig.~\ref{fig:dvbs2_bfer} shows the BER and FER decoding performance of the
3 selected MODCODs. The forms represent the channel conditions: the squares
stand for a standard simulated AWGN channel, the triangles are also a simulated
AWGN channel in which frequency shift, phase shift and symbol delay have been
taken into account and the circles are the real conditions measured performances
with the USRPs. We noticed a 0.2 dB inaccuracy in the noise estimated by the
$t^\text{Rx}_{13}$ task, this is symbolized by the extra horizontal bars over
the circles. The {\color{Paired-1} MODCOD 1} is represented by dashed lines; the
{\color{Paired-3} MODCOD 2} is represented by dotted lines; the
{\color{Paired-5} MODCOD 3} is represented by solid lines. For each MODCOD, the
LDPC decoder is the belief propagation algorithm with horizontal layered
scheduling (10 iterations) and with the min-sum node update rules. One can
notice that each DVB-S2 configuration has a well-separated SNR predilection
zone.

\subsection{Evaluations}

This section discusses about the receiver part of the system. We did not bench
the transmitter part as it is not the most compute intensive part and high
throughputs are much more easier to reach. All the presented results have been
obtained by running the code on a high-end machine composed by two Intel\R
Xeon\TM Platinum 8168 CPUs and 128 GB of RAM. The frequency of the CPUs is 2.70
GHz and the \emph{Turbo Boost} mode has been disabled for the reproducibility of
the results. Each CPU is composed by 24 cores that support the SMT technology
but we disabled it in our tests to get the lowest possible tasks latency per
core. 48 cores (or hardware threads) are available for the DVB-S2 receiver. All
input and output data are represented by 32-bit floating-point or integer
numbers. Knowing that the Platinum 8168 CPU supports the AVX-512 ISA, sixteen
32-bit elements can be processed in one SIMD instruction. Most of the receiver
tasks have been accelerated with \MIPP SIMD functions. On this occasion, \MIPP
has even been enriched with new functions to improve the complex numbers
support.

Knowing that the LDPC decoding is one of the most compute intensive task of the
receiver, we decided to use the efficient inter-frame SIMD implementation
presented before in the document (the early termination criteria has been
switched on). This choice has the effect of computing sixteen frames instead of
one in each task of the receiver. As discussed before, it negatively affect the
overall latency of the system (by a factor of sixteen) but it is not important
in the video streaming targeted application. The \emph{Decoder LDPC} task
($t^\text{Rx}_{16}$) is the only one in the receiver to take advantage of the
inter-frame SIMD technique, the other tasks simply process sixteen frames
sequentially.

\begin{table}[htp]
  \centering
  \caption
    [Tasks sequential throughputs and latencies of the DVB-S2 receiver.]
    {Tasks sequential throughputs and latencies of the DVB-S2 receiver
    (transmission phase, 16288 frames, inter-frame level = 16, MODCOD 2,
    error-free SNR zone).
    The sequential tasks are represented by \colorbox{Paired-7!15}{orange} rows,
    the slowest \emph{sequential} stage is in \colorbox{Paired-5!15}{red} and
    the slowest of all stages is in \colorbox{Paired-1!15}{blue}.}
  \label{tab:dvbs2_tasks_thr_lat}
  {\resizebox{\linewidth}{!}{
  \begin{tabular}{r | r r r r  | r r r | r}
    \multicolumn{1}{r |}{\textbf{Task or Stage}}  & \multicolumn{4}{c |}{\textbf{Throughput} (MS/s)} & \multicolumn{3}{c |}{\textbf{Latency} ($\mu$s)} & \textbf{Time} \\
                                                                              &     \textbf{Avg} &     \textbf{Min.} &     \textbf{Max.} & $ \bm{\mathcal{N}}_\text{\textbf{Avg}}$ &      \textbf{Avg} &     \textbf{Min.} &     \textbf{Max.} & (\%)           \\ \hline \hline
    \rowcolor{Paired-7!15}
                            Radio -       \emph{receive} ($t^\text{Rx}_{1}$)  &         1015.86  &           234.20  &          1093.98  &                                 431.83  &           527.32  &           489.66  &          2287.24  &          0.94  \\ \hline
                                                             \textbf{Stage 1} & \textbf{1015.86} &   \textbf{234.20} &  \textbf{1093.98} &                         \textbf{431.83} &   \textbf{527.32} &   \textbf{489.66} &  \textbf{2287.24} &  \textbf{0.94} \\ % T = 16*33480/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
                   Multiplier AGC -     \emph{imultiply} ($t^\text{Rx}_{2}$)  &          864.41  &           420.05  &           935.71  &                                 367.45  &           619.71  &           572.49  &          1275.28  &          1.11  \\
    \rowcolor{Paired-7!15}
              Synch. Freq. Coarse -   \emph{synchronize} ($t^\text{Rx}_{3}$)  &         1979.17  &           665.98  &          2237.38  &                                 841.32  &           270.66  &           239.42  &           804.35  &          0.48  \\
    \rowcolor{Paired-7!15}
                   Filter Matched -        \emph{filter} ($t^\text{Rx}_{4}$)  &          273.85  &           121.60  &           275.25  &                                 116.41  &          1956.08  &          1946.13  &          4405.09  &          3.49  \\ \hline
                                                             \textbf{Stage 2} &  \textbf{188.19} &    \textbf{82.61} &   \textbf{194.22} &                          \textbf{80.00} &  \textbf{2846.45} &  \textbf{2758.04} &  \textbf{6484.72} & \textbf{5.08}  \\ % T = 16*33480/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
    \rowcolor{Paired-7!15}
                   Synch. Timing  -   \emph{synchronize} ($t^\text{Rx}_{5}$)  &          130.38  &            58.97  &           131.31  &                                  55.42  &          4108.52  &          4079.39  &          9084.64  &          7.34  \\ \hline
    \rowcolor{Paired-5!15}                                   \textbf{Stage 3} &  \textbf{130.38} &    \textbf{58.97} &   \textbf{131.31} &                          \textbf{55.42} &  \textbf{4108.52} &  \textbf{4079.39} &  \textbf{9084.64} &  \textbf{7.34} \\ % T = 16*33480/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
    \rowcolor{Paired-7!15}
                    Synch. Timing -       \emph{extract} ($t^\text{Rx}_{6}$)  &          331.50  &           151.54  &           354.62  &                                 281.83  &           807.97  &           755.28  &          1767.48  &          1.44  \\
                   Multiplier AGC -     \emph{imultiply} ($t^\text{Rx}_{7}$)  &          806.31  &           442.69  &           877.19  &                                 685.51  &           332.18  &           305.34  &           605.02  &          0.59  \\
    \rowcolor{Paired-7!15}
                     Synch. Frame -   \emph{synchronize} ($t^\text{Rx}_{8}$)  &          187.50  &           120.17  &           193.25  &                                 159.41  &          1428.51  &          1386.01  &          2228.76  &          2.55  \\ \hline
                                                             \textbf{Stage 4} &  \textbf{104.27} &    \textbf{58.21} &   \textbf{109.47} &                          \textbf{88.65} &  \textbf{2568.66} &  \textbf{2446.63} &  \textbf{4601.26} &  \textbf{4.58} \\ % T = 16*16740/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
                 Scrambler Symbol -    \emph{descramble} ($t^\text{Rx}_{9}$)  &         1979.41  &           668.85  &          2649.55  &                                 1682.89 &           135.31  &           101.09  &           400.45  &          0.24  \\
    \rowcolor{Paired-7!15}
           Synch. Freq. Fine L\&R -   \emph{synchronize} ($t^\text{Rx}_{10}$) &         1466.55  &           596.19  &          1741.72  &                                 1246.85 &           182.63  &           153.78  &           449.25  &          0.33  \\
            Synch. Freq. Fine P/F -   \emph{synchronize} ($t^\text{Rx}_{11}$) &          132.40  &            62.59  &           140.88  &                                  112.56 &          2022.98  &          1901.24  &          4279.30  &          3.61  \\ \hline
                                                             \textbf{Stage 5} &  \textbf{114.42} &    \textbf{52.22} &   \textbf{124.22} &                          \textbf{97.27} &  \textbf{2340.92} &  \textbf{2156.11} &  \textbf{5129.00} &  \textbf{4.18} \\ % T = 16*16740/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
                       Framer PLH -        \emph{remove} ($t^\text{Rx}_{12}$) &         1148.07  &           427.71  &          1180.59  &                                1008.60  &           225.77  &           219.55  &           606.02  &          0.40  \\
                  Noise Estimator -      \emph{estimate} ($t^\text{Rx}_{13}$) &          626.12  &           151.24  &           656.09  &                                 550.06  &           413.98  &           395.07  &          1713.87  &          0.74  \\ \hline
                                                             \textbf{Stage 6} &  \textbf{405.16} &   \textbf{111.73} &   \textbf{421.72} &                         \textbf{355.94} &   \textbf{639.75} &   \textbf{614.62} &  \textbf{2319.89} &  \textbf{1.14} \\ % T = 16*16200/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
                        Modem PSK -    \emph{demodulate} ($t^\text{Rx}_{14}$) &           46.07  &            42.12  &            46.28  &                                  40.47  &          5626.34  &          5600.83  &          6153.50  &         10.05  \\
                      Interleaver -  \emph{deinterleave} ($t^\text{Rx}_{15}$) &         1533.54  &           518.95  &          1582.97  &                                1347.25  &           169.02  &           163.74  &           499.47  &          0.30  \\
                     Decoder LDPC -   \emph{decode SIHO} ($t^\text{Rx}_{16}$) &          166.15  &            69.12  &           171.59  &                                 164.21  &          1386.74  &          1342.74  &          3333.34  &          2.48  \\
                      Decoder BCH -   \emph{decode HIHO} ($t^\text{Rx}_{17}$) &            6.92  &             6.15  &             6.96  &                                   6.92  &         32905.37  &         32705.15  &         36998.15  &         58.79  \\
                 Scrambler Binary -    \emph{descramble} ($t^\text{Rx}_{18}$) &           91.11  &            47.74  &            91.73  &                                  91.11  &          2499.41  &          2482.41  &          4770.24  &          4.47  \\ \hline
    \rowcolor{Paired-1!15}                                   \textbf{Stage 7} &    \textbf{5.35} &     \textbf{4.40} &     \textbf{5.38} &                           \textbf{5.35} & \textbf{42586.88} & \textbf{42294.87} & \textbf{51754.70} & \textbf{76.09} \\ % T = 16*14232/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\
    \rowcolor{Paired-7!15}
                 Sink Binary File -          \emph{send} ($t^\text{Rx}_{19}$) &         1838.31  &            25.30  &          2100.47  &                                1838.31  &           123.87  &           108.41  &          9001.34  &          0.22  \\ \hline
                                                             \textbf{Stage 8} & \textbf{1838.31} &    \textbf{25.30} &  \textbf{2100.47} &                        \textbf{1838.31} &   \textbf{123.87} &   \textbf{108.41} &  \textbf{9001.34} &  \textbf{0.22} \\ % T = 16*14232/L
                                                                              &                  &                   &                   &                                         &                   &                   &                   &                \\ \hline \hline
                                                               \textbf{Total} &    \textbf{4.09} &     \textbf{2.51} &     \textbf{4.14} &                           \textbf{4.09} & \textbf{55742.37} & \textbf{54947.73} & \textbf{90662.79} & \textbf{99.57} \\ % T = 16*14232/L
  \end{tabular}
  }}
\end{table}

Table~\ref{tab:dvbs2_tasks_thr_lat} presents the measured tasks throughputs and
latencies for a sequential execution of the MODCOD 2 in the transmission phase.
The tasks have been regrouped per stage in order to introduce the future
decomposition when the parallelism will be implemented. The throughput is given
in mega samples per second (MS/s), this is because some of the tasks are not
working on bits but on samples and a bit is the smallest possible sample. The
average (Avg), minimum (Min.) and maximum (Max.) throughputs are calculated with
the number of output samples. Depending on the task, the number of output
samples can drastically vary and this is not possible to directly compare the
tasks throughputs with each others. This is why we introduced the normalized
average throughput ($\mathcal{N}_\text{Avg}$) which is the average throughput
considering the $K$ information bits independently of the task output socket
size. In a first observation, one may notice that the latencies of the tasks are
very high compared to the ones presented in the simulator evaluation (cf.
Section~\ref{sec:eval_simu}). This is mainly due to the very large frame size
$N = 16740$ and to the inter-frame level, each task processes approximatively
$16740 \times 16$ samples when in the simulator evaluation each task processes
approximatively $2048$ bits (there is a factor of $\approx 130$). The
applicative context is very different and it is directly observable from the
resulting latencies.

The stage 7 takes 76\% of the time with especially the \emph{Decoder BCH} task
($t^\text{Rx}_{17}$) that takes 59\% of the time. $t^\text{Rx}_{17}$ should not
take so many time compared to the other tasks but we chose to do not spend too
much time in optimizing the BCH decoding process as the stage 7 throughput can
easily be increased with the sequence duplication technique. Theoretically, with
an infinite number of cores and memory bandwidth, the throughput of the stage 7
could be infinite too.
The second slower stage in the stage 3, this stage is the main hotspot of the
presented receiver. The stage 3 contains only one synchronization task
($t^\text{Rx}_{5}$) and in the current implementation this task cannot be
duplicated (or parallelized) because there is an internal data dependency with
the previous frame. The stage 3 will be the real limiting factor of the receiver
and considering a machine with an infinite number of cores, the maximum
reachable information throughput will be 55.42 Mb/s.

We did not try to parallelize the waiting and the learning phases. We measured
that the whole learning phase (1, 2 and 3) takes about one second. During the
learning phase, the receiver is not fast enough to process the received samples
in real time. To fix this problem, the samples are buffered in the \emph{Radio -
receive} task ($t^\text{Rx}_{1}$). Once the learning phase is done, the
transmission phase is parallelized and the receiver became fast enough to absorb
the radio buffer and samples in real time. During the transmission phase the
receiver has been split in 8 stages presented in
Fig.~\ref{fig:dvbs2_receiver_transmission}. This decomposition have been
motivated by the nature of the tasks (sequential or parallel) and by the
sequential measured throughput. The number of stages has been minimized in order
to limit the pipeline overhead, consequently, sequential and parallel tasks
has been regrouped in common stages. The slowest sequential task
($t^\text{Rx}_{5}$) has been isolated in the dedicated stage 3, the other
sequential stages have been formed to always have a higher normalized throughput
than the stage 3. The sequential throughput of the stage 7 (5.35 Mb/s) is lower
than the throughput of the stage 3 (55.42 Mb/s) this is why the sequence
duplication has been used. The stage 7 has been parallelize over 28 threads.
This looks like overkill but the machine was dedicated to the DVB-S2 receiver
and the throughput of the \emph{Decoder LDPC} task ($t^\text{Rx}_{16}$) varies
depending on the SNR because the early termination criterion was enabled.
When the signal quality is very good, the \emph{Decoder LDPC} task runs fast
and the threads can spend a lot of time in waiting. With the passive waiting
version of the adaptor \emph{push} and \emph{pull} tasks, the CPU dynamically
adapt the cores charge and energy can be saved. In
Table~\ref{tab:dvbs2_tasks_thr_lat} the presented \emph{Decoder LDPC} task
throughputs and latencies are optimistic because we are in a SNR error-free
zone. All the threads are pinned to a single core with the \emph{hwloc} library.
The 28 threads of the stage 7 are pinned in round-robin between the CPU sockets.
This way the memory bandwidth is maximized thanks to the two NUMA memory banks.
The strategy of the stage 7 parallelism is to maximize the throughput. During
the duplication process (modules clones) the threads pinning is known and the
memory is copied into the right memory bank (first touch policy). All the other
pipeline stages (1, 2, 3, 4, 5, 6 and 8) are run on a single thread. Because of
the synchronizations between the pipeline stages (adaptor pushes and pulls), the
threads have been pinned on the same socket. The idea is to minimize the
pipeline stages latencies in maximizing the CPU caches performance. It avoids
the extra-cost of moving the cache data between the sockets.

The receiver program uses around 1.3 GB of the global memory when running in
sequential while it uses around 30 GB in parallel. The memory usage increases
because of the sequence duplications in the stage 7. The duplication operation
takes about 20 seconds and is made at the very beginning of the program. It is
worth mentioning, that the amount of memory was not a critical resource so we
did not try to reduce it overall occupancy.

\begin{figure}[htp]
  \centering
  \subfloat[][Data copy (stage throughput is 40 Mb/s).]{\includegraphics[width=0.485\linewidth]{dvbs2/pipeline_copy/pipeline_copy_dat}\label{plot:dvbs2_pipeline_copy_dat}}
  \quad
  \subfloat[][Pointer copy (stage throughput is 55 Mb/s).]{\includegraphics[width=0.485\linewidth]{dvbs2/pipeline_copy/pipeline_copy_ptr}\label{plot:dvbs2_pipeline_copy_ptr}}
  \caption
    [Comparison of the different pipeline implementations in the DVB-S2
     receiver.]
    {Comparison of the different pipeline implementations in the DVB-S2 receiver
     (MODCOD 2).}
  \label{plot:dvbs2_pipeline}
\end{figure}

Fig.~\ref{plot:dvbs2_pipeline} presents the repartition of the time in the
pipeline stages (MODCOD 2). The receiver is running over 35 threads.
Fig.~\ref{plot:dvbs2_pipeline_copy_dat} shows the pipeline implementation with
data copies and Fig.~\ref{plot:dvbs2_pipeline_copy_ptr} shows the pipeline
implementation with pointer copies. \emph{Push wait} and \emph{Pull wait} are
the percentage of time spent in passive or active waiting. \emph{Push copy} and
\emph{Pull copy} are the percentage of time spent in copying the data to and
from the adaptors buffers. \emph{Standard tasks} is the cumulative percentage of
time spent by the tasks presented in Fig.~\ref{fig:dvbs2_receiver_transmission}.
In both implementations the pipeline stages throughput adapt to the slowest one.
In Fig.~\ref{plot:dvbs2_pipeline_copy_dat} the measured throughput per stage is
40 Mb/s whereas in Fig.~\ref{plot:dvbs2_pipeline_copy_ptr} the the measured
throughput is 55 Mb/s. The pointer copy implementation throughput is $\approx$
27\% higher than the data copy implementation.
Fig.~\ref{plot:dvbs2_pipeline_copy_dat} shows that the copy overhead is
non-negligible and the 27\% slowdown is directly due to these copies in the
stage 3. It largely justifies the pointer copy implementation. In
Fig.~\ref{plot:dvbs2_pipeline_copy_ptr} and in the stage 3, 100\% of time is
taken by the $t^\text{Rx}_{5}$ task. This is also confirmed by the measured
throughput (55 Mb/s) which is very close the the sequential throughput (55.42
Mb/s) reported in Table~\ref{tab:dvbs2_tasks_thr_lat}.

\begin{table}[htp]
  \centering
  \caption
    [Throughput performances depending of the selected DVB-S2 configurations.]
    {Throughput performances depending of the selected DVB-S2 configurations.}
  \label{tab:dvbs2_thr_modcod}
  % {\resizebox{\linewidth}{!}{
  \begin{tabular}{c | c c | c c}
    \multirow{3}{*}{\textbf{Config.}} & \multicolumn{4}{c }{\textbf{Throughput} (Mb/s)} \\
                                      & \multicolumn{2}{c |}{\textbf{Sequential}} & \multicolumn{2}{c }{\textbf{Parallel}} \\
                                      & \textbf{Info.} & \textbf{Coded} & \textbf{Info.} & \textbf{Coded} \\
    \hline \hline
    MODCOD 1 &  3.4 & 5.7 & 37 & 62 \\
    MODCOD 2 &  4.1 & 4.6 & 55 & 62 \\
    MODCOD 3 &  4.0 & 4.5 & 80 & 90 \\
  \end{tabular}
  % }}
\end{table}

Table~\ref{tab:dvbs2_thr_modcod} summarized the obtained throughputs for the 3
MODCODs presented in Table~\ref{tab:dvbs2_modcod}. Each time, sequential and
parallel throughput are given. To measure to maximum achievable throughput, the
real radio has been removed and replaced by the read of samples pre-registered
in a binary file. This is because the pipeline stages are naturally adapting to
the slowest one and in real communication the throughput of the radio is always
configured to be just a little bit slower than the slowest stage: this is
necessary for real time communication otherwise the radio task has to
indefinitely bufferize the samples while the amount of available memory in the
machine is clearly not infinite. The information ($K$) throughput (info.) is the
final useful throughput for the user while the coded ($N$) throughput is here
for observations. Between the MODCOD 1 and 2, only the LDPC code rate varies
($R=3/5$ and $R=8/9$ resp.). In the parallel implementation, it has a direct
impact on the information throughput while the coded throughput is unchanged. It
fact this is because the \emph{Decoder LDPC} task ($t^\text{Rx}_{16}$) is
parallelized in the stage 7. This stage is capable to adapt to the charge
automatically. In the sequential implementation, the coded throughput is
negatively impacted when $R=8/9$. Between the MODCOD 2 and 3, the modulation
varies (QPSK and 8-PSK resp.) and the frames have to be deinterleaved
(column/row interleaver). High order modulation reduces the amount of samples
treated in the \emph{Synchronizer Timing} task ($t^\text{Rx}_{5}$): this results
in higher throughput (80 Mb/s for the 8-PSK) in the slowest stage 3. In the
parallel implementation, the pipeline stages throughput are adapting to the
slowest stage 3 and it results in an important speedup. In the sequential
implementation it results in a little slowdown, this is because the additional
time spent in the  \emph{deinterleave} task ($t^\text{Rx}_{15}$) is higher than
the time saved in the \emph{Synchronizer Timing} task ($t^\text{Rx}_{5}$).

This results demonstrate the benefit of the parallelized implementation of the
receiver. The throughput speedup rangs from 10 to 20 compared to the sequential
implementation. It is also important to note that the selected configurations
are efficient in different SNR zones (as shown in Fig.~\ref{fig:dvbs2_bfer}).
Depending on the signal quality, different MODCOD can be selected and it will
have a direct impact on the system throughput. For instance, the MODCOD 1 is
adapted for noisy environment (3 dB) but the information throughput is limited
to 37 Mb/s while the MODCOD 3 is more adapted to clearer signal conditions
(7.5 dB) and the information throughput reaches 80 Mb/s. The MODCOD 2 is
in-between.

\subsection{Related Works}

% \begin{itemize}
%   \item \xmark~donner un point de comparaison avec un récepteur DVB-S2 matériel.
% \end{itemize}

Some other works are focusing on the SDR implementation of a DVB-S2 transceiver.
Here is a list of the projects we found:
\begin{itemize}
  \item \textbf{gr-dvbs2rx}~\cite{gr-dvbs2rx} is an open source extension to GNU
    Radio. The project sounds promising but lacks efficiency, his main
    maintainer affirms that the receiver is not able to meet the satellite real
    time constraints (30 to 50 Mb/s) on a Xeon\TM Gold/Platinum series
    processor\footnote{\url{https://lists.gnu.org/archive/html/discuss-gnuradio/2019-01/msg00196.html}.}.
    He advises to use a dedicated GPU or FPGA for the LDPC decoding.
  \item \textbf{leansdr}~\cite{leansdr} is a standalone open source project. The
    project creation was motivated to reach higher receiver throughput than GNU
    Radio even if it results in decoding performance degradations. For instance,
    a low complexity LDPC bit-flipping decoder~\cite{Ryan2009} is used. At the
    time of the writing, the project does not support multi-threading and SIMD
    instructions.
  \item \textbf{Grayver and Utter} recently published a paper~\cite{Grayver2020}
    in which they succeed to build a 10 Gb/s DVB-S2 receiver on a cluster of
    server-class CPUs. On a comparable CPU, their work is able to double or even
    triple the throughput of our implementation. This is mainly due to the use
    of a high speed SIMD LDPC decoder~\cite{LeGal2016,Grayver2019} and to new
    algorithmic improvements in the synchronization tasks. For instance, they
    were able to express more parallelism than us in the \emph{Synchronizer
    Timing} task ($t^\text{Rx}_{5}$), this is very promising. However, we also
    tried some aggressive optimizations in the \emph{Synchronizer Timing} task
    but we never succeeded to keep the same level of BER/FER decoding
    performance. It could be interesting to check if the Grayver and Utter work
    comes with no penalty in term of decoding performance. Unlike our work,
    Grayver and Utter are focusing on a single DVB-S2 MODCOD (8-PSK, $N = 64800$
    and $R = 1/2$).
\end{itemize}

\section{Discussion}

% \begin{itemize}
%   \item \cmark~possibilités d'interfaçage avec MATLAB (testé par Romain)
%   \item \cmark~outil graphique pour générer le code C++
% \end{itemize}

In this chapter a new eDSL dedicated to the SDR has been presented. Main
components has been designed to satisfy the SDR needs in term of 1)
expressiveness with sequences, tasks and loops; 2) performance with the sequence
duplication technique and the pipelining strategy. After that, the proposed eDSL
is evaluated in an applicative context: the implementation of the DVB-S2
standard. The results demonstrate the efficiency of the \AFFECT eDSL: the
proposed solution matches satellite real time constraints (30 $\thicksim$ 50
Mb/s). This is the consequence of two main factors: 1) the tasks level
optimizations, for instance the fast LDPC decoder has been used (cf.
Chapter~\ref{chap:opt}); 2) the quasi zero overhead eDSL, with among others, an
efficient implementation of the pipeline.

In future works, it could be very interesting to combine the parallel features
of the \AFFECT eDSL with high level languages like Python or MATLAB. The signal
specialists, which are often not familiar with the \Cxx language, could use the
existing high-speed \Cxx tasks and develop their own in the high level
language. Linking with the \AFFECT library, it would be very valuable if the
tasks written in the high level language could be inserted in the \AFFECT
sequences. This way the whole system would automatically be parallelized.
Happily, this is technically possible in our model. As the static scheduling is
evaluated at the runtime, it is possible to encapsulate a function from a high
level language in a \AFFECT task (with a callback) and to start the sequence
analysis (= perform the static scheduling resolution) after that. In comparison,
it would be more complicated to make the languages interfacing with GNU Radio as
the static scheduling is resolved at the compilation time and flatten in the
Python language. When the generated Python code is executed, it performs calls
to the tasks written and compiled in a \Cxx library. Moreover, it could be very
useful to develop a graphical user interface like \emph{GNU Radio Companion} to
facilitate the tasks creation and binding as well as the parallelism definition
(sequence duplication, pipeline stages). The graphical user interface would then
generate a ready to compile \Cxx code similar to the one presented in
Listing~\ref{lst:dsl_pipeline}.