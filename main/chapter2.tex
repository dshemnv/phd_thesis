\graphicspath{{main/chapter2/fig/}}

\chapter{Algorithms and Efficient Methods of Digital Communication Receiver}

% \vspace*{\fill}
\minitoccustom
% \vspace*{\fill}

\section{Polar Decoders}
\label{sec:alg_polar_decoders}

\subsection{\Arikan's Kernel}

\subsubsection{Successive Cancellation Algorithm}

\begin{figure}[htp]
  \centering
  % \begin{scaletikzpicturetowidth}{0.70\linewidth}
  \begin{tikzpicture}%[scale=\tikzscale]

    \newcommand\La{5} % number of layers in the tree
    \newcommand\VS{1.95} % vertical space between the tree layer

    \pgfmathtruncatemacro\Ll{\La -1}

    \foreach \l in {\Ll,...,0} {

      \pgfmathtruncatemacro\ll{2^\l-1}
      \pgfmathtruncatemacro\rl{\La-\l}
      \pgfmathtruncatemacro\rlmo{\rl -1}
      \pgfmathtruncatemacro\ee{2^\rlmo}
      \pgfmathtruncatemacro\eemo{\ee-1}

      ["\ifthenelse{\l=0}{}
      {
        \draw[dashed] (-2.00, \rlmo*\VS+\VS/2) -- (11.75, \rlmo*\VS+\VS/2);
      }", sloped]

      \node[text width=1.5cm, text centered] at (10.7, \rlmo*\VS) {\l};

      \foreach \g in {0,...,\ll} {

        \ifthenelse{\l=\Ll}{}
        {
          \draw (0.2*\eemo/2*1.75 + \ee*\g/2*1.3 + \eemo/2*0.3, \rlmo*\VS) node{\tiny{\textbullet}};
        }

        \foreach \e in {0,...,\eemo} {

          \ifthenelse{\e=0 \AND \g=0}
          {
            \node[draw=black, minimum height=0.8cm, minimum width=0.3cm, text=black, label={[black]left:\small{$\ee$ (LLR, $\hat{s}$)}}] (l\l_g\g_e\e) at (0.2*\eemo/2*1.75 + \ee*\g/2*1.3 + \e*0.3, \rlmo*\VS) {};
          }
          {
            \node[draw=black, minimum height=0.8cm, minimum width=0.3cm, text=black] (l\l_g\g_e\e) at (0.2*\eemo/2*1.75 + \ee*\g/2*1.3 + \e*0.3, \rlmo*\VS) {};
          }
        }

        \ifthenelse{\l=\Ll}{}
        {
          \pgfmathtruncatemacro\ln{\l+1}
          \pgfmathtruncatemacro\gn{\g*2}
          \pgfmathtruncatemacro\gnpo{\g*2+1}
          \pgfmathtruncatemacro\eee{\eemo/2}
          \pgfmathtruncatemacro\eeemo{\eee-1}
          \pgfmathtruncatemacro\eemt{\eemo-1}
          \ifthenelse{\l<2}
          {
            \draw[->,>=latex, line width=1.0pt, Paired-5        ] (l\l_g\g_e1.south)          -- (l\ln_g\gn_e1.north)           node [midway, text=Paired-5, fill=white] {\small{$f$}};
            \draw[->,>=latex, line width=1.0pt, Paired-1, dashed] (l\l_g\g_e\eemt.south)      -- (l\ln_g\gnpo_e\eeemo.north)    node [midway, text=Paired-1, fill=white] {\small{$g$}};
            \draw[<-,>=latex, line width=1.0pt, Paired-3, dotted] (l\l_g\g_e0.south west)     -- (l\ln_g\gn_e0.north west)      node [midway, fill=white] {\small{$h$}};
            \draw[<-,>=latex, line width=1.0pt, Paired-3, dotted] (l\l_g\g_e\eemo.south east) -- (l\ln_g\gnpo_e\eee.north east) node [midway, fill=white] {\small{$h$}};
          }
          {
            \draw[->,>=latex, line width=1.0pt, Paired-5        ] (l\l_g\g_e0.south)          --                (l\ln_g\gn_e0.north)           node [midway, text=Paired-5, fill=white] {\small{$f$}};
            \draw[->,>=latex, line width=1.0pt, Paired-1, dashed] (l\l_g\g_e\eemo.south)      --                (l\ln_g\gnpo_e\eee.north)      node [midway, text=Paired-1, fill=white] {\small{$g$}};
            \draw[<-,>=latex, line width=1.0pt, Paired-3, dotted] (l\l_g\g_e0.south west)     to[bend right=30] (l\ln_g\gn_e0.north west)      node [] {};
            \draw[<-,>=latex, line width=1.0pt, Paired-3, dotted] (l\l_g\g_e\eemo.south east) to[bend left=30]  (l\ln_g\gnpo_e\eee.north east) node [] {};
          }
        }
      }
    }
    \pgfmathtruncatemacro\H{\VS*\La}
    \pgfmathtruncatemacro\HH{8.9}
    \draw[->,>=latex] (11.25, \HH) -- (11.25, -0.2) node [midway, text=black, fill=white,rotate=90] {\small{Layer (tree depth)}};
  \end{tikzpicture}
  % \end{scaletikzpicturetowidth}
  \caption{Full SC decoding tree ($N = 16$)}
  \label{fig:polar_sc_decoder}
\end{figure}

The SC decoding algorithm can be seen as the traversal of a binary tree starting
from the root node. For a code length $N=2^m$, the corresponding tree thus
includes $m + 1$ node layers, indexed from $d=0$ (root node layer) down to
$d=m$ (leaf nodes layers). As the tree is initially full, each layer $d$
contains $2^d$ nodes, each node of that layer $d$  containing $2^{m-d}$ LLRs
($\lambda$) and $2^{m-d}$ binary values denoted as \textit{partial sums} ($s$).
At initialization, LLRs received  from the channel ($Y$) are stored in the root
node. Then, the decoder performs a pre-order traversal of the tree. When a node
is visited in the downward direction, LLRs of the node are updated. In the
upward direction, partial sums are updated. Fig.~\ref{fig:polar_sc_decoder}
summarizes the computations performed in both directions. The update functions
are:
\begin{eqnarray}
\left\{\begin{array}{l c l c l}
\lambda_c &=& f(\lambda_a,\lambda_b) &=& sign(\lambda_a.\lambda_b).\min(|\lambda_a|,|\lambda_b|)\\
\lambda_c &=& g(\lambda_a,\lambda_b,s)&=&(1-2s)\lambda_a+\lambda_b\\
(s_{c}, s_{d}) &=& h(s_{a}, s_{b}) &=& (s_{a} \oplus s_{b}, s_{b}).
\end{array}\right.
\label{eq:polar_f_g_h}
\end{eqnarray}
The $f$ and $g$ functions both generate a single LLR. The $h$ function provides
a couple of partial sums.

Before recursively calling itself on the left node, the algorithm apply the $f$
function, respectively, before calling itself on the right node the $g$ function
is applied. At the end (after the recursive call on the right node) the $h$
function is applied. The $f$ and $g$ functions use the LLRs (read only mode)
from the current node $n_i$ in order to produce the new LLR values into
respectively left and right $n_{i+1}$ nodes. The $h$ function, in the general
case (non-terminal case), reads the bits from the left and right $n_{i+1}$ nodes
in order to update the bit values of the $n_i$ node. For the terminal case, the
$h$ function reads the LLRs from itself and decides the bit values.

Leaf nodes are of two kinds: \emph{information bit} nodes and \emph{frozen bit}
nodes. When a frozen bit leaf node is reached, its binary value is
unconditionally set to zero. Instead, when an information leaf node is reached,
its binary value is set according to the \emph{sign} of its LLR (0 if LLR is
positive, 1 otherwise). Once every node in the tree has been visited in both
directions, the decoder eventually updates partial sums in the root node and the
decoding process is terminated. At this point, the decoding result is stored in
the root node in the form of a $N$-bit partial sum vectors.

\subsubsection{Successive Cancellation List Algorithm}

\begin{itemize}
  \item \xmark~Prendre le schéma de Mathieu pour mieux décrire l'algo SCL
\end{itemize}

\begin{algorithm}
  \caption{SCL decoding algorithm}\label{alg:polar_scl}

  % \small
  \SetKwProg{Fn}{Function}{}{}

  % \KwIn{$N$ is the frame size.}
  % \KwIn{$L$ is the number of lists (or paths) to maintain.}
  \KwData{$\lambda$ is a 2D buffer ($[L][2N]$) to store the LLRs.}
  \KwData{$\hat{s}$ is a 2D buffer ($[L][N]$) to store the bits.}

  \Fn{SCL\_decode ($N, o_{\lambda}, o_{\hat{s}}$)}
  {
    $N_{\frac{1}{2}} = N / 2$

    \uIf(// not a leaf node){$N > 1$}
    {
      \For(// loop over the paths){$p=0$ \textbf{to} $L-1$}
      {
        \For(// apply the $f$ function){$i=0$ \textbf{to} $N_{\frac{1}{2}}-1$}
        {
          $\lambda[p][o_\lambda + N + i] = \bm{f}(\lambda[p][o_\lambda + i], \lambda[p][o_\lambda + N_{\frac{1}{2}} + i])$
        }
      }

      \textit{SCL\_decode ($N_{\frac{1}{2}}, o_{\lambda} + N, o_{\hat{s}}$)}

      \For{$p=0$ \textbf{to} $L-1$}
      {
        \For(// apply the $g$ function){$i=0$ \textbf{to} $N_{\frac{1}{2}}-1$}
        {
          $\lambda[p][o_\lambda + N + i] = \bm{g}(\lambda[p][o_\lambda + i], \lambda[p][o_\lambda + N_{\frac{1}{2}} + i], \hat{s}[p][o_{\hat{s}} + i])$
        }
      }

      \textit{SCL\_decode ($N_{\frac{1}{2}}, o_{\lambda} + N, o_{\hat{s}} + N_{\frac{1}{2}}$)}

      \For{$p=0$ \textbf{to} $L-1$}
      {
        \For(// update the partial sums){$i=0$ \textbf{to} $N_{\frac{1}{2}}-1$}
        {
          $\hat{s}[p][o_{\hat{s}} + i] = \bm{h}(\hat{s}[p][o_{\hat{s}} + i], \hat{s}[p][o_{\hat{s}} + N_{\frac{1}{2}} + i])$
        }
      }
    }
    \Else(// a leaf node)
    {
      \textit{update\_paths ()} // update, create and delete paths
    }
  }

  \textit{SCL\_decode ($N, 0, 0$)} // launch the decoder

  \textit{select\_best\_path ()}
\end{algorithm}

The SCL algorithm is summarized in Algorithm~\ref{alg:polar_scl}. Unlike the SC
algorithm, the SCL decoder builds a list of candidate codewords along the
decoding. At each call of the \textit{update\_paths()} sub-routine
(Alg.~\ref{alg:polar_scl}, l.16), $2L$ candidates are generated. A path metric
is then evaluated to keep only the $L$ best candidates among the $2L$ paths. The
path metrics are calculated as in \cite{Balatsoukas-Stimming2015}. At the end of
the decoding process, the candidate codeword with the best path metric is
selected in the \textit{select\_best\_path()} sub-routine
(Alg.~\ref{alg:polar_scl}, l.18). The decoding complexity of the SCL algorithm
grows as $O(LN\log_2N)$. This linear increase in complexity with L leads to
significant improvements in BER/FER performances, especially for small code
lengths.

\subsubsection{CRC concatenation scheme}

The authors in~\cite{Tal2011} observed that when a decoding error occurs, the
right codeword is often in the final list, but not with the best path metric.
They proposed to concatenate a CRC to the codeword in order to discriminate the
candidate codewords at the final stage of the SCL decoding. Indeed, this
technique drastically improves the FER performance of the decoder. We denote
this algorithm CA-SCL and its simplified version CA-SSCL. In terms of
computational complexity, the overhead consists in the computation of $L$ CRC at
the end of each decoding.

\subsubsection{Simplified Decoders}
\label{sec:alg_polar_simplified_decoders}

\begin{figure}[htp]
  \centering
  % \begin{scaletikzpicturetowidth}{0.70\linewidth}
  \begin{tikzpicture}%[scale=\tikzscale]

  \tikzset{ any/.style ={draw=gray,     circle, minimum height=0.6cm, text=black, fill=gray!40                                                                              } }
  \tikzset{ frzn/.style={draw=black,    circle, minimum height=0.6cm, text=black                                                                                            } }
  \tikzset{ info/.style={draw=black,    circle, minimum height=0.6cm, text=black, fill=black                                                                                } }
  \tikzset{ r0/.style  ={draw=Paired-1, circle, minimum height=0.6cm, text=black, preaction={fill=Paired-1!40}, pattern=north west lines, pattern color=black!80!Paired-1!70} }
  \tikzset{ r1/.style  ={draw=Paired-3, circle, minimum height=0.6cm, text=black, preaction={fill=Paired-3!40}, pattern=north east lines, pattern color=black!80!Paired-3!70} }
  \tikzset{ rep/.style ={draw=Paired-7, circle, minimum height=0.6cm, text=black, preaction={fill=Paired-7!40}, pattern=crosshatch dots,  pattern color=black!80!Paired-7!70} }
  \tikzset{ spc4/.style={draw=Paired-5, circle, minimum height=0.6cm, text=black, preaction={fill=Paired-5!40}, pattern=horizontal lines, pattern color=black!80!Paired-5!70} }
  \tikzset{ spc/.style ={draw=Paired-9, circle, minimum height=0.6cm, text=black, preaction={fill=Paired-9!40}, pattern=grid,             pattern color=black!80!Paired-9!70} }

  \node[frzn                                                        ] (l3_g0) at (0.0, 0.0) {};
  \node[frzn                                                        ] (l3_g1) at (1.0, 0.0) {};
  \node[frzn                                                        ] (l3_g2) at (2.0, 0.0) {};
  \node[info                                                        ] (l3_g3) at (3.0, 0.0) {};
  \node[frzn                                                        ] (l3_g4) at (4.0, 0.0) {};
  \node[info                                                        ] (l3_g5) at (5.0, 0.0) {};
  \node[info                                                        ] (l3_g6) at (6.0, 0.0) {};
  \node[info                                                        ] (l3_g7) at (7.0, 0.0) {};

  \node[r0,   label={[black]above:\small{\texttt{R0}}}              ] (l2_g0) at (0.5, 1.5) {};
  \node[rep,  label={[black]above:\small{\texttt{REP}}}             ] (l2_g1) at (2.5, 1.5) {};
  \node[rep,  label={[black]above:\small{\texttt{REP}}}             ] (l2_g2) at (4.5, 1.5) {};
  \node[r1,   label={[black]above:\small{\texttt{R1}}}              ] (l2_g3) at (6.5, 1.5) {};

  \node[rep,  label={[black]above:\small{\texttt{REP}}}             ] (l1_g0) at (1.5, 3.0) {};
  \node[spc4, label={[black]above:\small{\texttt{SPC$_\texttt{4}$}}}] (l1_g1) at (5.5, 3.0) {};

  \node[any,  label={[black]above:\small{\texttt{?}}}               ] (l0_g0) at (3.5, 4.5) {};

  \draw[->,>=latex] (l2_g0) -- (l3_g0) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g0) -- (l3_g1) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g1) -- (l3_g2) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g1) -- (l3_g3) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g2) -- (l3_g4) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g2) -- (l3_g5) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g3) -- (l3_g6) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l2_g3) -- (l3_g7) node [midway, text=black, sloped] {|};

  \draw[->,>=latex] (l1_g0) -- (l2_g0) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l1_g0) -- (l2_g1) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l1_g1) -- (l2_g2) node [midway, text=black, sloped] {|};
  \draw[->,>=latex] (l1_g1) -- (l2_g3) node [midway, text=black, sloped] {|};

  \draw[->,>=latex] (l0_g0) -- (l1_g0) node [midway, above, text=black] {$f$};
  \draw[->,>=latex] (l0_g0) -- (l1_g1) node [midway, above, text=black] {$g$};
  \draw[->,>=latex] (l0_g0) to [out=0,in=60,looseness=8] (l0_g0) node [right, text=black, xshift=0.6cm, yshift=0.2cm] {$xor$};

  \draw[->,>=latex, black, line width=1.0pt] (7.5, 2.25) -> (8.5, 2.25);

  \node[rep,  label={[black]above:\small{\texttt{REP}}}             ] (l1_g0_cut) at ( 9.5, 1.5) {};
  \node[spc4, label={[black]above:\small{\texttt{SPC$_\texttt{4}$}}}] (l1_g1_cut) at (13.5, 1.5) {};
  \node[any,  label={[black]above:\small{\texttt{?}}}               ] (l0_g0_cut) at (11.5, 3.0) {};

  \draw[->,>=latex] (l0_g0_cut) -- (l1_g0_cut) node [midway, above, text=black] {$f$};
  \draw[->,>=latex] (l0_g0_cut) -- (l1_g1_cut) node [midway, above, text=black] {$g$};
  \draw[->,>=latex] (l0_g0_cut) to [out=0,in=60,looseness=8] (l0_g0_cut) node [right, text=black, xshift=0.6cm, yshift=0.2cm] {$xor$};

  \draw[->,>=latex] (l1_g0_cut) to [out=-120,in=-60,looseness=8] (l1_g0_cut) node [left, text=black, xshift=0.3cm, yshift=-0.85cm] {$rep$};
  \draw[->,>=latex] (l1_g1_cut) to [out=-120,in=-60,looseness=8] (l1_g1_cut) node [left, text=black, xshift=0.3cm, yshift=-0.85cm] {$spc$};

  \end{tikzpicture}
  % \end{scaletikzpicturetowidth}
  \caption
    [Example of polar tree pruning on a small binary tree ($N = 8$).]
    {Example of tree pruning on a small binary tree ($N = 8$). The tree is cut
    and the computations are versioned according to the location of the frozen
    bits.}
  \label{fig:tree_pruning_example}
\end{figure}

Frozen bits fully define the decoders leaf values, hence some part of the
traversal can be cut and its computation avoided, depending on the location of
the frozen bits. More generally, the tree computation can be versioned depending
on these bits. In~\cite{Alamdar-Yazdi2011}, a tree pruning technique called the
Simplified SC (SSC) was applied to SC decoding. An improved version was proposed
in~\cite{Sarkis2014a}. This technique relies on the fact that, depending on the
frozen bits location in the leaves of the tree, the definition of dedicated
nodes enables to prune the decoding tree: Rate-0 nodes (\verb|R0|) correspond to
a sub-tree whose all leaves are frozen bits, Rate-1 nodes (\verb|R1|) correspond
to a sub-tree in which all leaves are information bits, REPetition (\verb|REP|)
and Single Parity Check (\verb|SPC|) nodes correspond to repetition and SPC
codes sub-trees. These special nodes, originally defined for SC decoding, can be
employed in the case of SCL decoding as long as some modifications are made in
the path metric calculation~\cite{Sarkis2016}. This tree-pruned version of the
algorithm is called Simplified SCL (SSCL). The tree pruning technique can
drastically reduce the amount of computation in the decoding process. The
Fig.~\ref{fig:tree_pruning_example} shows that more than half of the tree nodes
can be removed for $N = 8$.

\subsubsection{Adaptive SCL decoding algorithm}

The presence of the CRC can be further used to reduce the decoding time by
gradually increasing $L$. This variation of SCL is called Adaptive SCL
(A-SCL)~\cite{Li2012}. The first step of the A-SCL algorithm is to decode the
received frame with the SC algorithm. Then, the decoded polar codeword is
checked with a CRC. If the CRC is not valid, the SCL algorithm is applied with
$L=2$. If no candidate in the list satisfies the CRC, $L$ is gradually doubled
until it reaches the value $L_{max}$. In this paper, we call this version of the
A-SCL decoding the Fully Adaptive SCL (FA-SCL) as opposed to the Partially
Adaptive SCL (PA-SCL), in which the $L$ value is not gradually doubled but
directly increased from $1$ (SC) to $L_{max}$. The simplified versions of these
algorithms are denoted PA-SSCL and FA-SSCL. In order to simplify the algorithmic
range, in the remainder of the paper, only the simplified versions are
considered. The use of either FA-SSCL or PA-SSCL algorithmic improvement
introduces no BER or FER performance degradation as long as the CRC length is
adapted to the polar code length. If the CRC length is too short, the decoding
performance may be degraded because of false detections. These adaptive versions
of SSCL can achieve higher throughputs. Indeed, a large proportion of frames can
be decoded with a single SC decoding. This is especially true when the SNR is
high. This will be further discussed in Section~\ref{sec:polar_genericity}.

\subsubsection{Algorithmic Comparison}

\begin{table}[htp]
  \centering
  \caption{Throughput and latency comparison of polar decoding algorithms.}
  \label{tab:polar_algos}
  % {\small
   \begin{tabular}{r c c c}
    \textbf{Decoding}  & \textbf{BER \& FER}   & \multirow{1}{*}{\textbf{Throughput}} & \textbf{Max. Latency}        \\
    \textbf{Algorithm} & \textbf{Performances} & ($\bm{\mathcal{T}}$)                 & ($\bm{\mathcal{L}_{worst}}$) \\
    \hline
    \hline
    SC      & poor      & medium & medium \\
    SSC     & poor      & high   & low    \\
    SCL     & good      & low    & high   \\
    SSCL    & good      & low    & medium \\
    CA-SSCL & very good & low    & medium \\
    PA-SSCL & very good & high   & medium \\
    FA-SSCL & very good & high   & high   \\
  \end{tabular}
  % }
\end{table}

In order to better distinguish all the algorithmic variations, we compare their
main features in Table~\ref{tab:polar_algos}. Each algorithm is characterized in
terms of decoding performance, throughput, and worst case latency for a software
implementation. The non-simplified versions of the adaptive SCL algorithms are
not included in the Table for readability.

The SC and especially the SSC algorithms achieve very high throughput and low
latency with poor BER and FER performances. The SCL algorithm improves the
decoding performance compared to the SC algorithm, but its computational
complexity leads to an increased latency and a lower throughput. The SSCL
algorithm improves the decoding throughput and latency without any impact in
terms of BER and FER performances, as long as the tree pruning is not too deep,
as will be discussed in Section~\ref{sec:polar_genericity}. Therefore, tree
pruning is applied to all the following algorithms, namely CA-SSCL, FA-SSCL and
PA-SSCL. By applying CRC to the SCL algorithm, one can achieve better BER and
FER performances at the cost of computational complexity overhead. The Adaptive
SCL algorithms reduce the decoding time with no impact on BER and FER
performances. Furthermore, a tradeoff between throughput and worst case latency
is possible with the use of either PA-SSCL or FA-SSCL decoding algorithms.

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.7\textwidth]{polar/algos_comparison/algos_comparison}
  \caption
    [Decoding performance comparison between CA-SCL and SC decoders.]
    {Decoding performance comparison between CA-SCL and SC decoders.
    Code rate $R = 1/2$, and 32-bit CRC (GZip).}
  \label{plot:polar_algos_comparison}
\end{figure}

CA-SCL decoding performances for large code lengths ($N > 2^{14}$) combined with
large list sizes ($L > 8$) are rarely presented in the literature. This is
probably due to the long simulation time. The proposed decoders are integrated
in the \AFFECT toolbox. Therefore, multi-threaded and multi-nodes simulations
are enabled to handle such computation-demanding simulations. All the presented
simulations use the Monte Carlo method with a Binary Phase-Shift Keying (BPSK)
modulation. The communication channel is an Additive White Gaussian Noise (AWGN)
channel based on the Mersenne Twister pseudo-random number generator
(MT19937)~\cite{Matsumoto1998} and the Box-Muller transform~\cite{Box1958}.
Figure~\ref{plot:polar_algos_comparison} compares the BER/FER performances of
CA-SCL with SC decoding for a large range of code lengths. As expected, it
appears that the coding gain brought by the SCL algorithm decreases for larger
$N$ values. In the case of $N=2^{16}$, the improvement caused by the use of the
CA-SCL algorithm with $L=32$ and a 32-bit GZip CRC (\verb|0x04C11DB7|
polynomial) instead of SC is about $0.75$ dB compared to $1.2$ dB with a polar
code of size $N=2^{12}$. For larger polar codes, $N=2^{20}$, the gain is reduced
to $0.5$ dB, even with a list depth of $128$ that is very costly in terms of
computational complexity.

The tradeoffs between speed and decoding performance show some general trends.
However, the efficiency of each decoding algorithm is strongly dependent on the
polar code length, code rate, list depth and code construction. It is expected
that the best tradeoff is not always obtained with a single algorithm and
parameter set combination. It is consequently highly relevant to use a generic
and flexible decoder, that supports all variants of the decoding algorithms.
Thus, it is possible to switch from one to another as shown in the following
section.

\subsubsection{Generic and Flexible Polar Decoders}
\label{sec:polar_genericity}

\begin{itemize}{}
  \item \xmark~Revoir le discours pour qu'il soit moins parti pris pour les
    décodeurs "Génériques/dynamiques"
  \item \xmark~Si on laisse ça ici il faut une section sur les décodeurs
    "générés/déroulés" au même niveau que la
    Section~\ref{sec:alg_polar_genericity} (tous les décodeurs proposés (SC et
    SCL) sont flexibles mais pas génériques)
  \item \xmark~Lever l'ambiguité entre génériques/dynamiques/flexibles dans tout
    le document
\end{itemize}

The main contribution of this work lies in the flexibility and the genericity of
the proposed software decoder. These terms need to be clearly defined in order
to circumvent possible ambiguity. In the remainder of the paper, the
\textit{genericity} of the decoder concerns all the parameters that define the
supported polar code such as the codeword length, the code rate, the frozen bits
set, the puncturing patterns and the concatenated CRC. These parameters are
imposed by the telecommunication standard or the communication context. In the
wireless communications context, these are constantly adapted by AMC
methods~\cite{Dahlman2013}. In this work, a decoder is considered
\textit{generic} if it is able to support any combination of these parameters
that can be changed during a real time execution. On the other hand, the
\textit{flexibility} of a decoder includes all the customizations that can be
applied to the decoding algorithm for a given polar code: variant of the
decoding algorithm, data quantization, list size $L$, tree pruning strategy, ...
These customizations are not enforced by a standard. The flexibility gives some
degrees of freedom to the decoder in order to find the best tradeoff between
decoding performance, throughput or latency for a given polar code.

\paragraph{Genericity}
\label{sec:alg_polar_genericity}

In the context of wireless communications, the standards enforce several
different code lengths $N$ that have to be supported to share bandwidth between
different users. This is also the case for the code rate $R$ that needs to be
adapted to the quality of the transmission channel. Therefore, a practical
implementation should be adapted to both $N$ and $R$ in real-time in order to
limit latency.

A polar code is completely defined by $N$ and the frozen bits set
$\bm{u}_{\mathcal{A}^c}$. Several methods exist to generate some "good" sets of
frozen bits~\cite{Tal2013,Trifonov2012}. The code rate $R$ depends on the size
of $\bm{u}_{\mathcal{A}^c}$. In their original form, polar code lengths are only
powers of two. The puncturing and shortening techniques
in~\cite{Wang2014,Niu2013,Miloslavskaya2015} enable to construct polar codes of
any length at the cost of slightly degraded decoding performance. The coding
scheme can be completed with the specification of a CRC.

In~\cite{Sarkis2016}, the unrolling method is used: a specific description of
the decoder has to be generated for a specific polar code parameter set of $N$,
$K$, $R$, frozen bits set, puncturing pattern, CRC. This approach leads to very
fast software decoders at the price of the genericity, since a new source code
should be generated and compiled every time the modulation and coding scheme
(MCS) changes. This method is not adapted to wireless communication standards,
in which these parameters have to be adapted not only over time, but also for
the different users.

The proposed decoder does not use the unrolling method and is completely generic
regarding the code dimension $K$, the code length $N$, the frozen bits set
$\bm{u}_{\mathcal{A}^c}$ and the puncturing patterns. All of them are dynamic
parameters of the decoder and can be defined in input files. All CRC listed
in~\cite{CRCWiki2017} are available along with the possibility to define others.
It is shown in~\cite{Zhang2017} that custom CRCs for polar codes can have a very
good impact on the decoding performance.

Relying on an unique software description also implies that the tree pruning
technique also has to be dynamically defined. Indeed, this technique depends on
the frozen bits set $\bm{u}_{\mathcal{A}^c}$. Not sacrificing throughput or
latency while maintaining the genericity imposed by wireless communication
standards is at the core of the proposed implementation. Flexibility in terms of
decoding algorithms, described in the following, along with improvements
presented in Section~\ref{sec:polar_implem}, is necessary to deal with this
challenge.

\paragraph{Flexibility}

On one hand, the reason for the decoder genericity is the compliance to the
telecommunication standards. On the other hand, the flexibility of the decoder
regroups several algorithmic variations that are discussed in the following.
These variations allow several tradeoffs of multiple sorts, whatever the
standard. They are all included in a single source code.

In the proposed decoders the following parameters can be changed dynamically
without re-compilation: the list size $L$, the tree pruning strategy, the
quantization of the LLRs and the different SCL variants. Each of these
adjustments can be applied to access to different tradeoffs between throughput,
latency, and error rate performance. As a consequence, one can easily fine-tune
the configuration of the software decoder for any given polar code.

\subparagraph{List size}

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.70\textwidth]{polar/scl_l/scl_l}
  \caption
    [Tradeoffs between CA-SSCL decoding and throughput performances.]
    {Tradeoffs between CA-SSCL decoding and throughput performances depending on
    $L$. $N=2048$, $R=0.5$, and 32-bit CRC (GZip). For $L=1$, the SSC decoder is
    used with a ($2048$,$1024$) polar code.}
  \label{plot:polar_scl_l}
\end{figure}

As mentioned earlier, the list size $L$ impacts both speed and decoding
performance. In Figure~\ref{plot:polar_scl_l}, the throughput as well as BER and
FER performances of the CA-SSCL algorithm are shown for different $L$ values. A
($2048$,$1024$) polar code with a 32-bit CRC is considered. The computational
complexity increases linearly with $L$: the throughput is approximately halved
when $L$ is doubled, except for the case of the SC algorithm ($L=1$) which is
much faster. Indeed, there is no overhead due to the management of different
candidate paths during the decoding. For $L\geq4$ and $E_b/N_0=2$, the FER is
also approximately halved when the list size $L$ is doubled.

\subparagraph{Tree pruning strategy}

A second degree of flexibility is the customization of the SCL tree pruning. The
authors in~\cite{Alamdar-Yazdi2011,Sarkis2016} defined dedicated nodes to prune
the decoding tree and therefore to reduce the computational complexity. In this
proposed decoder, each dedicated node can be activated separately. The ability
to activate dedicated nodes at will is useful in order to explore the
contribution of each node type on the throughput.

\subparagraph{LLR Quantization}

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.70\textwidth]{polar/scl_bfer/scl_bfer_rep}
  \caption
    [Impact of the \texttt{REP} node size on fixed-point SSCL decoding.]
    {Impact of the \texttt{REP} node size on fixed-point SSCL decoding.
    Code ($2048$,$1723$), $L=32$.}
  \label{plot:polar_scl_bfer_rep}
\end{figure}

Another important parameter in both software and hardware implementations is the
quantization of data in the decoder. More specifically, the quantization of LLRs
and partial sums in the decoder have an impact on decoding performance.
Quantized implementations of the SC algorithm have already been proposed
in~\cite{Giard2016} but to the best of our knowledge, the proposed decoder is
the first SCL software implementation that can benefit from the 8-bit and 16-bit
fixed-point representations of LLRs and internal path metrics. In the 8-bit mode
LLRs and path metrics are saturated between $-127$ and $+127$ after each
operation. Moreover, to avoid overflows, the path metrics are normalized after
each \textit{update\_paths()} call (cf. Alg.~\ref{alg:polar_scl}) by subtracting
the smallest metric to each one of them. Figure~\ref{plot:polar_scl_bfer_rep}
shows the BER and FER performances of the CA-SSCL decoder for 32-bit
floating-point, 16-bit and 8-bit fixed-point representations. One can observe
that the \verb|REP| nodes degrade the decoding performance in a 8-bit
representation because of accumulation (red triangles curve). Indeed, it is
necessary to add all the LLRs of a \verb|REP| node together in order to process
it, which may lead to an overflow in the case of fixed-point representation. It
can happen when the size of the repetition nodes is not limited
($\texttt{REP}_\texttt{2+}$). However, the size limitation of the repetition
nodes to 8 ($\texttt{REP}_\texttt{8-}$) fixes this issue.

\subparagraph{Supporting different variants of the decoding algorithms}

Besides the $L$ values, the tree pruning and quantization aspects, the proposed
software polar decoder supports different variants of the SCL algorithm:
CA-SSCL, PA-SSCL, FA-SSCL.

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.70\textwidth]{polar/scl_adaptive/scl_adaptive}
  \caption
    [FER and throughput of the Fully and Partially Adaptive SSCL decoders.]
    {Frame Error Rate (FER) performance and throughput of the Fully and
    Partially Adaptive SSCL decoders (FA and PA). Code ($2048$,$1723$) and
    32-bit CRC (GZip). 32-bit floating-point representation.}
  \label{plot:polar_scl_adaptive}
\end{figure}

As shown in~\cite{Sarkis2016}, the adaptive version of the SCL algorithm yields
significant speedups, specially for high SNR. The original adaptive SCL
described in~\cite{Li2012}, denoted as Fully Adaptive SCL (FA-SSCL) in this
paper, gradually doubles the list depth $L$ of the SCL decoder when the CRC is
not valid for any of the generated codewords at a given stage until the value
$L_{max}$. By contrast, the adaptive decoding algorithm implemented
in~\cite{Sarkis2016}, called in this paper Partially Adaptive SCL (PA-SSCL),
directly increases the list depth from $1$ (SC) to $L_{max}$. In
Figure~\ref{plot:polar_scl_adaptive}, the two versions (FA-SSCL and PA-SSCL) are
compared on a ($2048$,$1723$) polar code and 32-bit CRC (GZip). The LLRs values
are based on a 32-bit floating point representation. Note that as the FER
performance of PA-SSCL and FA-SSCL are exactly the same, the related error
performance plots completely overlap. The throughput of the FA-SSCL algorithm is
higher than that of the PA-SSCL algorithm for some SNR values, depending on the
code parameters. Considering typical FER values for wireless communication
standards ($10^{-3}$ to $10^{-5}$), in the case of a ($2048$,$1723$) polar code,
the throughput of FA-SSCL is double that of PA-SSCL with $L = 8$, while it is
multiplied by a factor of $7$ with $L=32$. The drawback of FA-SSCL is that
although the average latency decreases, the worst case latency increases.

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.70\textwidth]{polar/scl_bfer/scl_bfer_crc}
  \caption{Impact of the CRC size on SCL and A-SCL decoding. Code ($2048$,
    $1723$), $L=32$.}
  \label{plot:polar_scl_bfer_crc}
\end{figure}

The adaptive versions of the algorithm achieve better throughputs, but CA-SCL
may also be chosen depending on the CRC. One may observe in
Figure~\ref{plot:polar_scl_bfer_crc} that an adaptive decoder dedicated to an
8-bit CRC with a ($2048$,$1723$) polar code and $L=32$ leads to a loss of $0.5$
dB for a FER of $10^{-5}$ compared to its non adaptive counterpart.

Both polar code genericity and decoding algorithm flexibility are helpful to
support the recommendations of wireless communications in an SDR or cloud RAN
context. The code and decoder parameters can be dynamically changed in the
proposed decoder, while maintaining competitive throughput and latency. The
following section introduces algorithmic and implementation improvements applied
in the proposed decoders to keep a low decoding time.

\subsection{Generic Kernels and Multi-Kernel}

\subsubsection{Related Works}

\subsubsection{Others}

\begin{itemize}
  \item \xmark~factorisation automatique de kernels et génération du code source
    associé
  \item \xmark~décodeurs multi-kernels génériques SC, SCL, CA-SCL et ASCL (non
    systématique et systématique)
\end{itemize}

\section{Turbo Decoders}

\subsection{Overview of the Turbo decoding process}
\label{sec:turbo_overview}

\begin{itemize}
  \item \xmark~prendre le shémas du treillis de Thibaud pour illustrer le BCJR
    LTE (3G/4G)
  \item \xmark~prendre le schémas du décodeur Turbo de Thibaud pour mieux
    expliquer le décodeur complet
\end{itemize}

% SHORT VERSION ---------------------------------------------------------------
% The turbo-decoding process is an iterative process in which two soft input soft
% output (SISO) decoders exchange extrinsic information. Each SISO decoder uses
% the channel information and \textit{a priori} extrinsic information to compute
% \textit{a posteriori} extrinsic information. The \textit{a posteriori}
% information becomes the \textit{a priori} information for the other SISO decoder
% and is exchanged via interleaver/deinterleaver.

% In turbo-coding, the two component codes are convolutional codes, the associated
% decoding modules perform the BCJR or forward-backward algorithm~\cite{Bahl1974}
% which is optimal for the maximum a posteriori (MAP) decoding of convolutional
% codes. In order to calculate the extrinsic information for a bit, a BCJR SISO
% decoder first computes the probability that a trellis transition occurred during
% the encoding process. The branch metrics associated with states
% $s_i^k$ and $s_j^{k+1}$ are computed as:
% \begin{equation}
% \label{eq:turbo_gamma}
%   \gamma(s_i^k, s_j^{k+1}) = 0.5(L_{sys}^k + L_a^k)u^k + 0.5(L_p^k p^k).
% \end{equation}
% Here, $L_{sys}^k$ and $L_a^k$ are the systematic channel LLR and the a-priori
% LLR for $k^{th}$ trellis section, respectively. In addition, the parity LLRs for
% the $k^{th}$ trellis step are $L_p^k = L_{p0}^k$ for MAP decoder 0 and
% $L_p^k = L_{p1}^k$ for MAP decoder 1. We do not need to evaluate the branch
% metric $\gamma(s^k , s^{k+1})$ for all 16 possible branches, as there are only
% four different branch metrics:
% $\gamma^k_0 = 0.5(L_{sys}^k + L_a^k + L_p^k)$,
% $\gamma^k_1 = 0.5(L_{sys}^k + L_a^k - L_p^k)$, $-\gamma^k_0$, and $-\gamma^k_1$.
% After that, the SISO decoder computes forward and backward recursions over the
% trellis representation of the convolutional code. In this work, we use the
% Enhanced max-log-MAP algorithm~\cite{Robertson1995,Vogt2000}. For each state
% $j$ of section $k$ of the trellis, the forward ($\alpha$) and backward ($\beta$)
% metrics are computed as follows:
% \begin{equation}
% \label{eq:turbo_alpha}
%   \alpha_j^{k+1} = max_{i \epsilon F} \{ \alpha_i^k + \gamma(s_i^k, s_j^{k+1}) \}
% \end{equation}
% \begin{equation}
% \label{eq:turbo_beta}
%   \beta_j^k = max_{i \epsilon B} \{ \beta_i^{k+1} + \gamma(s_j^k, s_i^{k+1}) \}.
% \end{equation}
% Then, the extrinsic information for each bit at position $k$ is:
% \begin{equation}
% \label{eq:turbo_ext}
% \begin{aligned}
%   L_e^k = max_{\{s_k, s_{k+1}\}\epsilon U^1}\{ \alpha_i^k + \beta_j^{k+1} + \gamma(s_i^k, s_j^{k+1}) \} \\
%   - max_{\{s_k, s_{k+1}\}\epsilon U^{-1}}\{ \alpha_i^k + \beta_j^{k+1} + \gamma(s_i^k, s_j^{k+1}) \} \\
%   - L_{sys}^k - L_a^k,
% \end{aligned}
% \end{equation}
% Finally, $L_e$ is scaled by a fixed factor of $0.75$.
% %As the Enhanced max-log-MAP is implemented, $L_e$ is re-scaled by a factor of
% %$0.75$ (fixed scaling factor).

% LONG VERSION ----------------------------------------------------------------
The turbo decoder consists of two concatenated component decoders (denoted as
decoder 0 and decoder 1) exchanging soft information in terms of the
log-likelihood ratio (LLR) for each transmitted information bit through an
interleaver and a deinterleaver.

In this paper we will only consider rate-1/3 codewords. $K$ represents the
number of information bits and $N$ is the codeword size: $K = N / 3$.

\subsubsection{Algorithm outline}

Turbo decoding is carried out in multiple iterations where each iteration
consists of two component decoding phases. In each phase, a component
decoder performs maximum a-posteriori (MAP) decoding using the BCJR algorithm
\cite{Bahl1974}, which generates so-called extrinsic LLRs given the LLRs
obtained by the detector and a-priori LLRs obtained from the other component
decoder. The BCJR algorithm consists of one forward and one backward traversal
on a trellis, which is defined by the underlying code. Specifically, to decode a
codeword of $K$ information bits, the BCJR algorithm performs the following
steps: (i) In the forward traversal step, it iteratively computes $K$ sets of
forward state metrics for each transmitted information bit. (ii) In the backward
traversal step, it iteratively computes $K$ sets of backward state metrics
for each transmitted information bit. (iii) To compute the extrinsic LLRs, the
BCJR algorithm then combines the forward and backward state metrics.

\subsubsection{Branch-metric computations}

LTE-Advanced operates on a 8-state trellis. Let $s_j^{k+1}$ be the $j^{th}$
state associated with information bit $k+1$. There are two incoming branches
into state $s_j^{k+1}$. Each incoming branch is associated with values $u^k$ and
$p^k$, the $k^{th}$ information bit and the parity bit (both $\pm1$),
respectively. The branch metrics associated with states $s_i^k$ and $s_j^{k+1}$
are computed as follows:
\begin{equation}
\label{eq:turbo_gamma}
 \gamma(s_i^k, s_j^{k+1}) = 0.5(L_{sys}^k + L_a^k)u^k + 0.5(L_p^k p^k).
\end{equation}
Here, $L_{sys}^k$ and $L_a^k$ are the systematic channel LLR and the a-priori
LLR for $k^{th}$ trellis step, respectively. In addition, the parity LLRs for
the $k^{th}$ trellis step are $L_p^k = L_{p0}^k$ for MAP decoder 0 and
$L_p^k = L_{p1}^k$ for MAP decoder 1. Note that we do not need to evaluate the
branch metric $\gamma(s^k , s^{k+1})$ for all 16 possible branches, as there are
only four different branch metrics:
$\gamma^k_0 = 0.5(L_{sys}^k + L_a^k + L_p^k)$,
$\gamma^k_1 = 0.5(L_{sys}^k + L_a^k - L_p^k)$, $-\gamma^k_0$, and $-\gamma^k_1$.

\subsubsection{Forward and backward state metric computations}

The forward state metrics can be computed iteratively from trellis step to
trellis step. The forward state metrics of step $k+1$ correspond to the vector
$\bm{\alpha^{k+1}} = [\alpha_0^{k+1}, ... ,\alpha_7^{k+1}]$, where the
$j^{th}$ forward state metric $\alpha_j^{k+1}$ only depends on two forward
state metrics of stage $k$. These state metrics are computed by:
\begin{equation}
  \label{eq:turbo_alpha}
  \alpha_j^{k+1} =
  max^*_{i \epsilon F} \{ \alpha_i^k + \gamma(s_i^k, s_j^{k+1}) \}
\end{equation}
where the set $F$ contains the two indices of the states in step $k$ connected
to state $s_j^{k+1}$ (as defined by the trellis). The $max^*\{·\}$ operator is
defined as:
\begin{equation}
   max^*\{a,b\} = max\{a,b\} + log(1 + exp(-|a-b|)),
\end{equation}
where $log(1 + exp(-|a-b|))$ is a correction term. For the max-log
approximation, we approximate $max^*$ by:
\begin{equation*}
   max^*(a, b) \approx max(a, b).
\end{equation*}
In this case, one can scale the extrinsic LLRs by a factor of 0.75 to partially
recover the error-rate performance loss induced by the approximation (see, e.g.,
\cite{Vogt2000}, \cite{Studer2011} for additional details). We will call this
decoding algorithm the \emph{Enhanced} max-log-MAP (EML-MAP).

Computation of the backward state metrics is similar to that of the forward
trellis traversal in Eq.~\ref{eq:turbo_alpha}. The vector of backward state
metrics, denoted by $\bm{\beta^k} = [\beta_0^k, ..., \beta_7^k]$, is
computed as:
\begin{equation}
  \label{eq:turbo_beta}
  \beta_j^k =
  max^*_{i \epsilon B} \{ \beta_i^{k+1} + \gamma(s_j^k, s_i^{k+1}) \}.
\end{equation}
Here, the set $B$ contains the indices of states in step $k+1$ connected to
state $s_j^k$ as defined by the trellis.

\subsubsection{LLR computations}

After the forward and backward iterations have been carried out, the extrinsic
LLRs for the $k^{th}$ bit are computed as:
\begin{equation}
  \label{eq:turbo_ext}
  \begin{aligned}
  L_e^k = max_{\{s_k, s_{k+1}\}\epsilon U^1}^*\{ \alpha_i^k + \beta_j^{k+1} +
  \gamma(s_i^k, s_j^{k+1}) \} \\
  - max_{\{s_k, s_{k+1}\}\epsilon U^{-1}}^*\{ \alpha_i^k + \beta_j^{k+1} +
  \gamma(s_i^k, s_j^{k+1}) \} \\
  - L_{sys}^k - L_a^k,
  \end{aligned}
\end{equation}
where the sets $U^1$ and $U^{-1}$ designate the set of states connected by paths
where $u^k=1$ and the set of states connected by paths where $u^k=-1$,
respectively (because of the BPSK modulation).

\begin{algorithm}
  \caption{Standard BCJR implementation}
  \label{alg:turbo_bcjr}

  % \small
  \For(// Sequential loop){$all~frames$}
  {
    \For(// Parallel loop){$k=0;~k<K;~k=k+1$}
    {
      $\boldsymbol{\gamma}^k\gets \computeGamma(L_{sys}^k, L_{p}^k, L_{e}^k)$
    }

    $\boldsymbol{\alpha}^0\gets \initAlpha()$

    \For(// Sequential loop){$k=1;~k<K;~k=k+1$}
    {
      $\boldsymbol{\alpha}^k\gets \computeAlpha(\boldsymbol{\alpha}^{k-1}, \boldsymbol{\gamma}^{k-1})$
    }

    $\boldsymbol{\beta}^{K-1}\gets \initBeta()$

    \For(// Sequential loop){$k=K-2;~k \geq 0;~k=k-1$}
    {
      $\boldsymbol{\beta}^k\gets \computeBeta(\boldsymbol{\beta}^{k+1}, \boldsymbol{\gamma}^{k})$
    }

    \For(// Parallel loop){$k=0;~k<K;~k=k+1$}
    {
      $L_e^k\gets \computeExtrinsic(\boldsymbol{\alpha}^k, \boldsymbol{\beta}^{k}, \boldsymbol{\gamma}^{k})$
    }
  }
\end{algorithm}

\subsubsection{BER/FER performance}

\begin{figure}[htp]
  \centering
  \includegraphics[width=0.7\textwidth]{turbo/bfer/bfer}
  \caption
    [BER and FER of the turbo decoder for $K = 6144$ (6 iters) and $R=1/3$.]
    {Bit Error Rate (BER) and Frame Error Rate (FER) of the decoder for $K =
    6144$ (6 iters) and $R=1/3$. Enhanced max-log-MAP algorithm (scaling
    factor = 0.75). BPSK modulation and AWGN channel were used.}
  \label{plot:turbo_bfer}
\end{figure}

Fig.~\ref{plot:turbo_bfer} shows the decoding performance of the proposed
software turbo-decoder for the $K = 6144$ rate-1/3, LTE-specified turbo-code.
The decoding performance of a floating-point decoder is provided as a reference.
Unlike~\cite{Wu2013}, the proposed 16-bit implementation does not degrade the
decoding performance. The 8-bit version of our decoder shows a 0.15 dB
degradation. The limited dynamic of 8-bit format together with early saturation
inside the decoder are responsible for this small performance loss.

\section{LDPC Decoders}

\begin{itemize}
  \item \xmark~tout est à faire pour présenter les décodeurs LDPC BP
\end{itemize}

\subsection{BP Algorithms}

LDPC codes is a family of channel codes that is well spread in current digital
communication systems. They have been chosen in many communication standards
(Wifi, WiMAX, DVB-S2, 10Gbps Ethernet, etc.). They were also selected for the
future 5G standard data transport.

\begin{figure}[htp]
  \centering
  % \begin{scaletikzpicturetowidth}{0.70\linewidth}
  \begin{tikzpicture}%[scale=\tikzscale]

  \node[draw=black, circle, minimum height=0.6cm, text=black] (v1) at (0.0, 2.5) {\small{1}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v2) at (1.0, 2.5) {\small{2}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v3) at (2.0, 2.5) {\small{3}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v4) at (3.0, 2.5) {\small{4}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v5) at (4.0, 2.5) {\small{5}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v6) at (5.0, 2.5) {\small{6}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v7) at (6.0, 2.5) {\small{7}};
  \node[draw=black, circle, minimum height=0.6cm, text=black] (v8) at (7.0, 2.5) {\small{8}};

  \node[draw=black, rounded corners=2pt, minimum height=1cm, label={[black]above:\small{Variable nodes ($V_{N}$)}}, dashed, fit=(v1) (v2) (v3) (v4) (v5) (v6) (v7) (v8)] (vn) {};

  \node[draw=black, minimum width=0.6cm, minimum height=0.6cm, text=black] (ca) at (2.0, 0.0) {\small{a}};
  \node[draw=black, minimum width=0.6cm, minimum height=0.6cm, text=black] (cb) at (3.0, 0.0) {\small{b}};
  \node[draw=black, minimum width=0.6cm, minimum height=0.6cm, text=black] (cc) at (4.0, 0.0) {\small{c}};
  \node[draw=black, minimum width=0.6cm, minimum height=0.6cm, text=black] (cd) at (5.0, 0.0) {\small{d}};

  \node[draw=black, rounded corners=2pt, minimum height=1cm, label={[black]below:\small{Check nodes ($C_{N}$)}}, dashed, fit=(ca) (cb) (cc) (cd)] (cn) {};

  \draw (v1.south) -- (ca.north) node [midway, text=black] {};
  \draw (v1.south) -- (cc.north) node [midway, text=black] {};
  \draw (v2.south) -- (cb.north) node [midway, text=black] {};
  \draw (v2.south) -- (cd.north) node [midway, text=black] {};
  \draw (v3.south) -- (cb.north) node [midway, text=black] {};
  \draw (v3.south) -- (cc.north) node [midway, text=black] {};
  \draw (v4.south) -- (ca.north) node [midway, text=black] {};
  \draw (v4.south) -- (cd.north) node [midway, text=black] {};
  \draw (v5.south) -- (ca.north) node [midway, text=black] {};
  \draw (v5.south) -- (cd.north) node [midway, text=black] {};
  \draw (v6.south) -- (cb.north) node [midway, text=black] {};
  \draw (v6.south) -- (cc.north) node [midway, text=black] {};
  \draw (v7.south) -- (ca.north) node [midway, text=black] {};
  \draw (v7.south) -- (cb.north) node [midway, text=black] {};
  \draw (v7.south) -- (cd.north) node [midway, text=black] {};
  \draw (v8.south) -- (ca.north) node [midway, text=black] {};
  \draw (v8.south) -- (cc.north) node [midway, text=black] {};

  \node[text width=2.2cm, text centered, fill=white] (M) at (6.3, 1.00) {\small{Messages ($M$)}};

  \end{tikzpicture}
  % \end{scaletikzpicturetowidth}
  \caption{Tanner graph of a simple LDPC parity check $H$ matrix.}
  \label{fig:ldpc}
\end{figure}

In this section the Min-Sum decoder for LDPC codes is presented. As shown in
Figure~\ref{fig:ldpc}, an LDPC code can be represented in the form of a Tanner
graph. The circles, denoted as variable nodes, represent the LLRs (the noisy
estimation of the bits in the received frames). The squares, denoted as parity
check nodes, represent the parity constraints that the variable nodes have to
verify. For instance, the check node $a$ ($CN_a$) is connected to the variable
nodes $1$, $4$, $5$, $7$ and $8$ ($VN_1, VN_4, VN_5, VN_7, VN_8$). It means that
the corresponding bits $U_1, U_4, U_5, U_7, U_8$ have to respect a parity
constraint: $U_1 \oplus U_4 \oplus U_5 \oplus U_7 \oplus U_8 = 0$. A codeword is
valid only if it respects all the parity constraints defined by the check nodes.
The LDPC code can be also represented by a \textit{parity check matrix}:
{ \begin{equation*}
H =
\begin{bmatrix}
  1&0&0&1&1&0&1&1\\
  0&1&1&0&0&1&1&0\\
  1&0&1&0&0&1&0&1\\
  0&1&0&1&1&0&1&0
\end{bmatrix}.
\end{equation*}
}
The Min-Sum decoder is an iterative message passing algorithm based on the
Tanner graph representation. Probabilistic messages ($M$) are exchanged between
the variable nodes and check nodes iteratively. Variable nodes and check nodes
apply an \textit{update rule} to compute the outgoing messages from the incoming
messages. In this section, the Min-Sum update rule is considered as well as an
horizontal layered scheduling. The original version of the Min-Sum algorithm
works on floating-point values, but it has been shown that fixed-point
simplifications have very similar decoding performance. Moreover, a fixed-point
representation enables to pack more elements into SIMD registers.

\subsection{Generic Implementation}

\begin{itemize}
  \item \xmark~décodeurs génériques avec "separation of concerns":
    1) "scheduling" (BP-flooding/HL/VL), 2) "update nodes" (SPA, log-SPA, MS,
    ...)
\end{itemize}

\section{SCMA Demodulators}

\begin{itemize}
  \item \cmark~si on laisse le SCMA, je crois qu'il y a tout
  \item \xmark~refaire les vilains schémas
\end{itemize}

\subsection{Introduction and Related Works}

Non-orthogonal Multiple Access (NOMA) mechanisms are investigated as means to
improve the fifth-generation mobile communication systems (5G)~\cite{Islam2017}
to realize massive connectivity and to reduce bit error rates. Sparse Code
Multiple Access (SCMA) is a NOMA mechanism that offers better bit error rate
performance and higher spectral efficiency, while the sparsity of the codebooks
ensures lower complexity of decoding compared to other non-orthogonal
modulations~\cite{Nikopour2013}. SCMA is a promising candidate for 5G
communication systems since it provides up to 300\% more connectivity by
spreading information of each user's codebook over sets of shared OFDM frequency
tones~\cite{Altera2015}. According to the NGMN white paper~\cite{Alliance2015},
5G is seriously considered to fulfill more diverse scenarios compared to 4G.
Applications can be broadband support in dense areas, low latency connectivity
for Augmented Reality (AR) and reliable communication for intelligent industrial
controls, Internet of Things (IoT) or Internet of Mission Critical Things
(IoMCT). Unfortunately, massive connectivity and spectral efficiency of SCMA
come at the cost of high complexity in the decoder, making the design of high
throughput and low complexity decoders a challenge for systems exploiting
SCMA~\cite{Lu2015}.

Exploiting sparsity of the codebooks, Belief Propagation (BP) or Message Passing
Algorithm (MPA) decoders were introduced to achieve near Maximum Likelihood
performance with lower complexity~\cite{Zhang2014a}. Substantial research works
were conducted on improving SCMA decoders to satisfy the uplink requirements of
5G. Indeed, MPA is populated with many exponential computations to calculate the
extrinsic information and probabilities of the received signal. This is based on
modeling the channel noise with a Gaussian probability density function (PDF). A
classical improvement to this bottleneck is the computation of extrinsic
information in the logarithm domain, which led to develop the log-MPA decoder.
In~\cite{Liu2016}, fixed point and floating-point implementations of the MPA and
log-MPA on FPGA are studied. The bit error rate performance and complexity of
the MPA and log-MPA are compared and it is concluded that using log-MPA with 4
message passing iterations achieves a good tradeoff between performance and
complexity. In~\cite{Bayesteh2015}, several complexity reduction techniques are
proposed to increase the system throughput. These techniques are 1) SCMA
codebook design with minimum number of projections, 2) clustered MPA (CMPA)
which defines sub-graphs in MPA and runs MPA on them, and 3) selected
exponential computations. In~\cite{Du2016} an adaptive Gaussian approximation
is used to unselect the edges of the graph with smaller modulus. In addition,
mean and variance feedbacks are employed to compensate information loss caused
by unselected edges. User's codebooks play an important role for fast
convergence of the MPA or log-MPA. As investigated in~\cite{Taherzadeh2014,
Peng2017,Yan2017}, revisiting codebook design can help to reduce the number of
iterations needed for MPA decoding of SCMA. In~\cite{Jia2018}, an improved MPA
is proposed which eliminates determined user codewords after certain number of
iterations and continue the iterations for undetermined user's codewords.
Similarly, in~\cite{Yang2016}, a belief threshold is set to choose the most
reliable edge probabilities and continue the iterations for the others. A
Shuffled MPA (S-MPA) is introduced in~\cite{Du2016a}. S-MPA is based on
shuffling the messages between function nodes and variable nodes. As a result,
the convergence rate is accelerated. A Monte Carlo Markov Chain Method is
proposed in~\cite{Chen2016} to decode SCMA signals and sphere decoding is also
explored in~\cite{Yang2017,Wei2017} for SCMA receiver design.

The main difference between this work and previously cited works is that the
present paper combines an analytic view of MPA complexity with hardware and
resource aware programming, exploiting hardware features available in general
purpose processors (GPPs). The SCMA decoding algorithms are revised in light of
the needs of Cloud Radio Access Networks (C-RANs) and to take full advantage of
key hardware features available in GPPs such as their SIMD engine. In the early
2000s, the performance of many processors improved significantly due to clock
rate increases. This increase of performance needed very minimal if any
programming effort, however the drawbacks of high clock rate was more power and
energy consumption, overheating of processors, leakage currents and signal
integrity problems. These disadvantages led designers to follow new paradigms
such as thread-level and data-level parallelisms that provide good performance
at lower clock speeds. Another challenge was data access efficiency in cache and
RAM for performance critical algorithms. Higher performance also came from
improved cache access efficiency of heterogeneous processors and parallel access
to the L1 cache through vectorized instructions. Therefore, complicated and
control heavy algorithms such as MPA have to be adapted for efficient execution
on heterogeneous architectures and their exploitable parallelism must be
expressed at every level of the code, whether in arithmetic or memory access
instructions. Particularly, various Single Instruction Multiple Data (SIMD)
extensions and thread-level parallelism are used to increase the throughput of
MPA decoding on various platforms.

This paper reports on two contributions that can be useful for any variation of
the aforementioned MPA. First, MPA and log-MPA have been adapted to use SIMD
extensions leveraging the available data-level parallelism. The algorithms are
revised to have aligned and contiguous access to memory, which is crucial to
achieve high memory throughput. Various SIMD instruction set architectures
(ISAs) such as Advanced Vector Extensions (AVX), Streaming SIMD Extension (SSE),
Knights Corner Instruction (KNCI) and ARM\R NEON are used to enhance the
performance of various parts of the algorithm. Multi-threaded programming
technique and power efficiency are also studied in this paper.

Second, efforts have been made to reduce the high dynamic ranges and high
storage burden that are induced by the numerous calculations of the exponential
function embedded in MPA, which is one of its main points of congestion. To
eliminate calculations of the exponentials in the MPA, this paper uses
approximate modeling of noise. Indeed, a Gaussian Probability Density Function
(PDF) is estimated with sub-optimal, bell shaped, polynomial PDFs. Using
polynomial PDFs enables a significant throughput improvement with a very small
degradation on the bit error rate performance. In addition, this technique
enables to use vectorized instructions for the calculation of the probabilities,
as opposed to log-MPA. Details will be explained in the sequel. The impacts of
turbo codes~\cite{Berrou1993}, polar codes~\cite{Arikan2009} and LDPC
codes~\cite{Gallager1962,MacKay1995} are investigated.

In this paper, symbols $\mathbb{B}$, $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{R}$
and $\mathbb{C}$ denote binary, natural, integer, real and complex numbers.
Scalar, vector and matrix are presented as $x$, $\bm{x}$, $\bm{X}$ respectively.
The n'th element of a vector denoted by $\bm{x}_n$ and $\bm{X}_{n,m}$ is the
element of n'th row and m'th column of matrix $\bm{X}$. Notation $\diag(x)$
shows a diagonal matrix where its n'th diagonal element is $\bm{x_n}$. In
addition, the transpose of a matrix is expressed as $\bm{X^T}$. The paper is
organized as follows, Section~\ref{sec:scma_detection} introduces the SCMA
algorithm. Maximum Likelihood, MPA and log-MPA decoding methods are explained in
this section as a background to this research.
Section~\ref{sec:opt_scma} elaborates on proposed improvements such as
vectorizing the algorithm, exponential estimations, contiguous access to memory
and other hardware oriented techniques. Section~\ref{sec:scma_perf} explores the
bit error rate performance as well as the throughput, the latency, the power
consumption, and the energy efficiency of the proposed MPA and log-MPA
implementations. Some profiling metrics are given to better understand the
results. Section~\ref{sec:scma_fec} is dedicated to study the effects of
suggested techniques on block error rate after channel coding. Finally, the main
findings of this research are summarized in Section~\ref{sec:scma_conclusion}.

\subsection{Overview of the SCMA System Model}
\label{sec:scma_overview}

\begin{figure}[htp]
  \centering
  \subfloat[][SCMA encoder with 6 users (layers) and 4 physical resources.]{
  \label{fig:scma_enc}
  \begin{tikzpicture}[baseline]
    \tikzset{ cbemp/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=white       } }
    \tikzset{ cbue1/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-1!40 } }
    \tikzset{ cbue2/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-3!40 } }
    \tikzset{ cbue3/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-5!40 } }
    \tikzset{ cbue4/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-7!40 } }
    \tikzset{ cbue5/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-9!40 } }
    \tikzset{ cbue6/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-11!40} }
    \tikzset{ cbadd/.style={draw=black, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=black!40    } }

    \newcommand\yshft{2.00}

    \node[cbemp] (ue1_cb_0_0) at (0.00+0.00, \yshft+0.00) {};
    \node[cbemp] (ue1_cb_1_0) at (0.00+0.25, \yshft+0.00) {};
    \node[cbemp] (ue1_cb_2_0) at (0.00+0.50, \yshft+0.00) {};
    \node[cbemp] (ue1_cb_3_0) at (0.00+0.75, \yshft+0.00) {};
    \node[cbemp] (ue1_cb_0_1) at (0.00+0.00, \yshft+0.25) {};
    \node[cbemp] (ue1_cb_1_1) at (0.00+0.25, \yshft+0.25) {};
    \node[cbemp] (ue1_cb_2_1) at (0.00+0.50, \yshft+0.25) {};
    \node[cbemp] (ue1_cb_3_1) at (0.00+0.75, \yshft+0.25) {};
    \node[cbue1] (ue1_cb_0_2) at (0.00+0.00, \yshft+0.50) {};
    \node[cbue1] (ue1_cb_1_2) at (0.00+0.25, \yshft+0.50) {};
    \node[cbue1] (ue1_cb_2_2) at (0.00+0.50, \yshft+0.50) {};
    \node[cbue1] (ue1_cb_3_2) at (0.00+0.75, \yshft+0.50) {};
    \node[cbue1] (ue1_cb_0_3) at (0.00+0.00, \yshft+0.75) {};
    \node[cbue1] (ue1_cb_1_3) at (0.00+0.25, \yshft+0.75) {};
    \node[cbue1] (ue1_cb_2_3) at (0.00+0.50, \yshft+0.75) {};
    \node[cbue1] (ue1_cb_3_3) at (0.00+0.75, \yshft+0.75) {};

    \node[cbue2] (ue2_cb_0_0) at (1.50+0.00, \yshft+0.00) {};
    \node[cbue2] (ue2_cb_1_0) at (1.50+0.25, \yshft+0.00) {};
    \node[cbue2] (ue2_cb_2_0) at (1.50+0.50, \yshft+0.00) {};
    \node[cbue2] (ue2_cb_3_0) at (1.50+0.75, \yshft+0.00) {};
    \node[cbue2] (ue2_cb_0_1) at (1.50+0.00, \yshft+0.25) {};
    \node[cbue2] (ue2_cb_1_1) at (1.50+0.25, \yshft+0.25) {};
    \node[cbue2] (ue2_cb_2_1) at (1.50+0.50, \yshft+0.25) {};
    \node[cbue2] (ue2_cb_3_1) at (1.50+0.75, \yshft+0.25) {};
    \node[cbemp] (ue2_cb_0_2) at (1.50+0.00, \yshft+0.50) {};
    \node[cbemp] (ue2_cb_1_2) at (1.50+0.25, \yshft+0.50) {};
    \node[cbemp] (ue2_cb_2_2) at (1.50+0.50, \yshft+0.50) {};
    \node[cbemp] (ue2_cb_3_2) at (1.50+0.75, \yshft+0.50) {};
    \node[cbemp] (ue2_cb_0_3) at (1.50+0.00, \yshft+0.75) {};
    \node[cbemp] (ue2_cb_1_3) at (1.50+0.25, \yshft+0.75) {};
    \node[cbemp] (ue2_cb_2_3) at (1.50+0.50, \yshft+0.75) {};
    \node[cbemp] (ue2_cb_3_3) at (1.50+0.75, \yshft+0.75) {};

    \node[cbemp] (ue3_cb_0_0) at (3.00+0.00, \yshft+0.00) {};
    \node[cbemp] (ue3_cb_1_0) at (3.00+0.25, \yshft+0.00) {};
    \node[cbemp] (ue3_cb_2_0) at (3.00+0.50, \yshft+0.00) {};
    \node[cbemp] (ue3_cb_3_0) at (3.00+0.75, \yshft+0.00) {};
    \node[cbue3] (ue3_cb_0_1) at (3.00+0.00, \yshft+0.25) {};
    \node[cbue3] (ue3_cb_1_1) at (3.00+0.25, \yshft+0.25) {};
    \node[cbue3] (ue3_cb_2_1) at (3.00+0.50, \yshft+0.25) {};
    \node[cbue3] (ue3_cb_3_1) at (3.00+0.75, \yshft+0.25) {};
    \node[cbemp] (ue3_cb_0_2) at (3.00+0.00, \yshft+0.50) {};
    \node[cbemp] (ue3_cb_1_2) at (3.00+0.25, \yshft+0.50) {};
    \node[cbemp] (ue3_cb_2_2) at (3.00+0.50, \yshft+0.50) {};
    \node[cbemp] (ue3_cb_3_2) at (3.00+0.75, \yshft+0.50) {};
    \node[cbue3] (ue3_cb_0_3) at (3.00+0.00, \yshft+0.75) {};
    \node[cbue3] (ue3_cb_1_3) at (3.00+0.25, \yshft+0.75) {};
    \node[cbue3] (ue3_cb_2_3) at (3.00+0.50, \yshft+0.75) {};
    \node[cbue3] (ue3_cb_3_3) at (3.00+0.75, \yshft+0.75) {};

    \node[cbue4] (ue4_cb_0_0) at (4.50+0.00, \yshft+0.00) {};
    \node[cbue4] (ue4_cb_1_0) at (4.50+0.25, \yshft+0.00) {};
    \node[cbue4] (ue4_cb_2_0) at (4.50+0.50, \yshft+0.00) {};
    \node[cbue4] (ue4_cb_3_0) at (4.50+0.75, \yshft+0.00) {};
    \node[cbemp] (ue4_cb_0_1) at (4.50+0.00, \yshft+0.25) {};
    \node[cbemp] (ue4_cb_1_1) at (4.50+0.25, \yshft+0.25) {};
    \node[cbemp] (ue4_cb_2_1) at (4.50+0.50, \yshft+0.25) {};
    \node[cbemp] (ue4_cb_3_1) at (4.50+0.75, \yshft+0.25) {};
    \node[cbue4] (ue4_cb_0_2) at (4.50+0.00, \yshft+0.50) {};
    \node[cbue4] (ue4_cb_1_2) at (4.50+0.25, \yshft+0.50) {};
    \node[cbue4] (ue4_cb_2_2) at (4.50+0.50, \yshft+0.50) {};
    \node[cbue4] (ue4_cb_3_2) at (4.50+0.75, \yshft+0.50) {};
    \node[cbemp] (ue4_cb_0_3) at (4.50+0.00, \yshft+0.75) {};
    \node[cbemp] (ue4_cb_1_3) at (4.50+0.25, \yshft+0.75) {};
    \node[cbemp] (ue4_cb_2_3) at (4.50+0.50, \yshft+0.75) {};
    \node[cbemp] (ue4_cb_3_3) at (4.50+0.75, \yshft+0.75) {};

    \node[cbue5] (ue5_cb_0_0) at (6.00+0.00, \yshft+0.00) {};
    \node[cbue5] (ue5_cb_1_0) at (6.00+0.25, \yshft+0.00) {};
    \node[cbue5] (ue5_cb_2_0) at (6.00+0.50, \yshft+0.00) {};
    \node[cbue5] (ue5_cb_3_0) at (6.00+0.75, \yshft+0.00) {};
    \node[cbemp] (ue5_cb_0_1) at (6.00+0.00, \yshft+0.25) {};
    \node[cbemp] (ue5_cb_1_1) at (6.00+0.25, \yshft+0.25) {};
    \node[cbemp] (ue5_cb_2_1) at (6.00+0.50, \yshft+0.25) {};
    \node[cbemp] (ue5_cb_3_1) at (6.00+0.75, \yshft+0.25) {};
    \node[cbemp] (ue5_cb_0_2) at (6.00+0.00, \yshft+0.50) {};
    \node[cbemp] (ue5_cb_1_2) at (6.00+0.25, \yshft+0.50) {};
    \node[cbemp] (ue5_cb_2_2) at (6.00+0.50, \yshft+0.50) {};
    \node[cbemp] (ue5_cb_3_2) at (6.00+0.75, \yshft+0.50) {};
    \node[cbue5] (ue5_cb_0_3) at (6.00+0.00, \yshft+0.75) {};
    \node[cbue5] (ue5_cb_1_3) at (6.00+0.25, \yshft+0.75) {};
    \node[cbue5] (ue5_cb_2_3) at (6.00+0.50, \yshft+0.75) {};
    \node[cbue5] (ue5_cb_3_3) at (6.00+0.75, \yshft+0.75) {};

    \node[cbemp] (ue6_cb_0_0) at (7.50+0.00, \yshft+0.00) {};
    \node[cbemp] (ue6_cb_1_0) at (7.50+0.25, \yshft+0.00) {};
    \node[cbemp] (ue6_cb_2_0) at (7.50+0.50, \yshft+0.00) {};
    \node[cbemp] (ue6_cb_3_0) at (7.50+0.75, \yshft+0.00) {};
    \node[cbue6] (ue6_cb_0_1) at (7.50+0.00, \yshft+0.25) {};
    \node[cbue6] (ue6_cb_1_1) at (7.50+0.25, \yshft+0.25) {};
    \node[cbue6] (ue6_cb_2_1) at (7.50+0.50, \yshft+0.25) {};
    \node[cbue6] (ue6_cb_3_1) at (7.50+0.75, \yshft+0.25) {};
    \node[cbue6] (ue6_cb_0_2) at (7.50+0.00, \yshft+0.50) {};
    \node[cbue6] (ue6_cb_1_2) at (7.50+0.25, \yshft+0.50) {};
    \node[cbue6] (ue6_cb_2_2) at (7.50+0.50, \yshft+0.50) {};
    \node[cbue6] (ue6_cb_3_2) at (7.50+0.75, \yshft+0.50) {};
    \node[cbemp] (ue6_cb_0_3) at (7.50+0.00, \yshft+0.75) {};
    \node[cbemp] (ue6_cb_1_3) at (7.50+0.25, \yshft+0.75) {};
    \node[cbemp] (ue6_cb_2_3) at (7.50+0.50, \yshft+0.75) {};
    \node[cbemp] (ue6_cb_3_3) at (7.50+0.75, \yshft+0.75) {};

    \newcommand\xshft{1.875}

    \node[cbemp] (ue1_cb_0) at (\xshft+0.00+0.00, 0.00+0.00) {};
    \node[cbemp] (ue1_cb_1) at (\xshft+0.00+0.00, 0.00+0.25) {};
    \node[cbue1] (ue1_cb_2) at (\xshft+0.00+0.00, 0.00+0.50) {};
    \node[cbue1] (ue1_cb_3) at (\xshft+0.00+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (p1) at (\xshft+0.00+0.325, 0.00+0.375) {\small{$+$}};

    \node[cbue2] (ue2_cb_0) at (\xshft+0.75+0.00, 0.00+0.00) {};
    \node[cbue2] (ue2_cb_1) at (\xshft+0.75+0.00, 0.00+0.25) {};
    \node[cbemp] (ue2_cb_2) at (\xshft+0.75+0.00, 0.00+0.50) {};
    \node[cbemp] (ue2_cb_3) at (\xshft+0.75+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (p2) at (\xshft+0.75+0.325, 0.00+0.375) {\small{$+$}};

    \node[cbemp] (ue3_cb_0) at (\xshft+1.50+0.00, 0.00+0.00) {};
    \node[cbue3] (ue3_cb_1) at (\xshft+1.50+0.00, 0.00+0.25) {};
    \node[cbemp] (ue3_cb_2) at (\xshft+1.50+0.00, 0.00+0.50) {};
    \node[cbue3] (ue3_cb_3) at (\xshft+1.50+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (p3) at (\xshft+1.50+0.325, 0.00+0.375) {\small{$+$}};

    \node[cbue4] (ue4_cb_0) at (\xshft+2.25+0.00, 0.00+0.00) {};
    \node[cbemp] (ue4_cb_1) at (\xshft+2.25+0.00, 0.00+0.25) {};
    \node[cbue4] (ue4_cb_2) at (\xshft+2.25+0.00, 0.00+0.50) {};
    \node[cbemp] (ue4_cb_3) at (\xshft+2.25+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (p4) at (\xshft+2.25+0.325, 0.00+0.375) {\small{$+$}};

    \node[cbue5] (ue5_cb_0) at (\xshft+3.00+0.00, 0.00+0.00) {};
    \node[cbemp] (ue5_cb_1) at (\xshft+3.00+0.00, 0.00+0.25) {};
    \node[cbemp] (ue5_cb_2) at (\xshft+3.00+0.00, 0.00+0.50) {};
    \node[cbue5] (ue5_cb_3) at (\xshft+3.00+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (p5) at (\xshft+3.00+0.325, 0.00+0.375) {\small{$+$}};

    \node[cbemp] (ue6_cb_0) at (\xshft+3.75+0.00, 0.00+0.00) {};
    \node[cbue6] (ue6_cb_1) at (\xshft+3.75+0.00, 0.00+0.25) {};
    \node[cbue6] (ue6_cb_2) at (\xshft+3.75+0.00, 0.00+0.50) {};
    \node[cbemp] (ue6_cb_3) at (\xshft+3.75+0.00, 0.00+0.75) {};

    \node[text width=0.2cm] (a1) at (\xshft+3.75+0.325, 0.00+0.375) {\small{$\rightarrow$}};

    \node[cbadd] (uea_cb_0) at (\xshft+4.50+0.00, 0.00+0.00) {};
    \node[cbadd] (uea_cb_1) at (\xshft+4.50+0.00, 0.00+0.25) {};
    \node[cbadd] (uea_cb_2) at (\xshft+4.50+0.00, 0.00+0.50) {};
    \node[cbadd] (uea_cb_3) at (\xshft+4.50+0.00, 0.00+0.75) {};

    \draw[->,>=latex] (ue1_cb_1_0.south east) -- (ue1_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_1$}};
    \draw[->,>=latex] (ue2_cb_1_0.south east) -- (ue2_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_2$}};
    \draw[->,>=latex] (ue3_cb_1_0.south east) -- (ue3_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_3$}};
    \draw[->,>=latex] (ue4_cb_1_0.south east) -- (ue4_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_4$}};
    \draw[->,>=latex] (ue5_cb_1_0.south east) -- (ue5_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_5$}};
    \draw[->,>=latex] (ue6_cb_1_0.south east) -- (ue6_cb_3.north) node [midway, fill=white] {\tiny{$\bm{h}_6$}};

    \node[text width=1.5cm, align=center] (t1) at (\xshft-1.0, 0.00+0.50) {\tiny{Users chosen}};
    \node[text width=1.5cm, align=center] (t2) at (\xshft-1.0, 0.00+0.25) {\tiny{codewords}};

    \node[text width=1.5cm, align=center] (t3) at (0.00+0.375, \yshft+1.00) {\tiny{$UE1$ CB}};
    \node[text width=1.5cm, align=center] (t4) at (1.50+0.375, \yshft+1.00) {\tiny{$UE2$ CB}};
    \node[text width=1.5cm, align=center] (t5) at (3.00+0.375, \yshft+1.00) {\tiny{$UE3$ CB}};
    \node[text width=1.5cm, align=center] (t6) at (4.50+0.375, \yshft+1.00) {\tiny{$UE4$ CB}};
    \node[text width=1.5cm, align=center] (t7) at (6.00+0.375, \yshft+1.00) {\tiny{$UE5$ CB}};
    \node[text width=1.5cm, align=center] (t8) at (7.50+0.375, \yshft+1.00) {\tiny{$UE6$ CB}};

    \node[draw=Paired-1, rounded corners=2pt, minimum height=2cm, dashed, fit=(t3) (ue1_cb_0_3) (t8) (ue6_cb_3_3) (ue1_cb_0)] {};

  \end{tikzpicture}
  }
  \\
  \subfloat[][SCMA uplink chain with channel coding.]{
  \label{fig:scma_codec}
  \begin{tikzpicture}[baseline]
    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (in_bs_uej) at (0.0, 0.225+0.000+0.0) {\tiny{$UEJ$ bit stream}};
    % \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (in_bs_uex) at (0.0, 0.225+0.450+0.2) {\tiny{$UEX$ bit stream}};
    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (in_bs_ue2) at (0.0, 0.225+0.900+0.4) {\tiny{$UE2$ bit stream}};
    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (in_bs_ue1) at (0.0, 0.225+1.350+0.6) {\tiny{$UE1$ bit stream}};

    \node[draw=black, minimum width=1.8cm, minimum height=0.45cm, align=center] (cc_uej) at (1.95+0.3, 0.225+0.000+0.0) {\tiny{Channel enc.}};
    % \node[draw=black, minimum width=1.8cm, minimum height=0.45cm, align=center] (cc_uex) at (1.95+0.3, 0.225+0.450+0.2) {\tiny{Channel enc.}};
    \node[draw=black, minimum width=1.8cm, minimum height=0.45cm, align=center] (cc_ue2) at (1.95+0.3, 0.225+0.900+0.4) {\tiny{Channel enc.}};
    \node[draw=black, minimum width=1.8cm, minimum height=0.45cm, align=center] (cc_ue1) at (1.95+0.3, 0.225+1.350+0.6) {\tiny{Channel enc.}};

    \node[draw=black, minimum width=1.7cm, minimum height=0.45cm, align=center] (scma_uej) at (4.0+0.3, 0.225+0.000+0.0) {\tiny{SCMA enc.}};
    % \node[draw=black, minimum width=1.7cm, minimum height=0.45cm, align=center] (scma_uex) at (4.0+0.3, 0.225+0.450+0.2) {\tiny{SCMA enc.}};
    \node[draw=black, minimum width=1.7cm, minimum height=0.45cm, align=center] (scma_ue2) at (4.0+0.3, 0.225+0.900+0.4) {\tiny{SCMA enc.}};
    \node[draw=black, minimum width=1.7cm, minimum height=0.45cm, align=center] (scma_ue1) at (4.0+0.3, 0.225+1.350+0.6) {\tiny{SCMA enc.}};

    \node[minimum width=0.5cm, align=center] (xj) at (5.420+0.3, 0.225+0.000+0.0) {\tiny{$\bm{x}_j$}};
    % \node[minimum width=0.5cm, align=center] (xx) at (5.420+0.3, 0.225+0.450+0.2) {\tiny{$\bm{x}_x$}};
    \node[minimum width=0.5cm, align=center] (x2) at (5.420+0.3, 0.225+0.900+0.4) {\tiny{$\bm{x}_2$}};
    \node[minimum width=0.5cm, align=center] (x1) at (5.420+0.3, 0.225+1.350+0.6) {\tiny{$\bm{x}_1$}};

    \node[draw=black, circle, align=center, minimum width=0.60cm] (plus) at (6.7, 1.20) {\tiny{$+$}};
    \node[align=center, minimum width=0.60cm] (n0) at (6.7, 2.20) {\tiny{$\bm{n$}}};

    \node[draw=black, align=center, minimum width=1.00cm, minimum height=2.40cm, label={[black, rotate=-90]center:\tiny{PRE demapping}}] (dmap)  at (7.8,     1.20) {\tiny{}};
    \node[draw=black, align=center, minimum width=1.00cm, minimum height=2.40cm, label={[black, rotate=-90]center:\tiny{SCMA decoder}} ] (dscma) at (8.8+0.3, 1.20) {\tiny{}};

    \node[draw=black, minimum width=2.1cm, minimum height=0.45cm, align=center] (cd_uej) at (10.65+0.3, 0.225+0.000+0.0) {\tiny{Channel dec.}};
    % \node[draw=black, minimum width=2.1cm, minimum height=0.45cm, align=center] (cd_uex) at (10.65+0.3, 0.225+0.450+0.2) {\tiny{Channel dec.}};
    \node[draw=black, minimum width=2.1cm, minimum height=0.45cm, align=center] (cd_ue2) at (10.65+0.3, 0.225+0.900+0.4) {\tiny{Channel dec.}};
    \node[draw=black, minimum width=2.1cm, minimum height=0.45cm, align=center] (cd_ue1) at (10.65+0.3, 0.225+1.350+0.6) {\tiny{Channel dec.}};

    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (out_bs_uej) at (13.05+0.3, 0.225+0.000+0.0) {\tiny{$UEJ$ bit stream}};
    % \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (out_bs_uex) at (13.05+0.3, 0.225+0.450+0.2) {\tiny{$UEX$ bit stream}};
    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (out_bs_ue2) at (13.05+0.3, 0.225+0.900+0.4) {\tiny{$UE2$ bit stream}};
    \node[minimum width=2.1cm, minimum height=0.45cm, align=center] (out_bs_ue1) at (13.05+0.3, 0.225+1.350+0.6) {\tiny{$UE1$ bit stream}};

    \node[draw=Paired-1, rounded corners=2pt, label={[Paired-1]below:\tiny{SCMA encoding}}, minimum height=2cm, dashed, fit=(scma_uej) (scma_ue1) (plus)] {};
    \node[draw=Paired-3, rounded corners=2pt, label={[Paired-3]below:\tiny{SCMA decoding}}, minimum height=2cm, dashed, fit=(dscma)] {};

    \draw[->,>=latex] (n0) -- (plus);

    \draw[->,>=latex] (in_bs_uej) -- (cc_uej);
    \draw[->,>=latex] (in_bs_ue2) -- (cc_ue2);
    \draw[->,>=latex] (in_bs_ue1) -- (cc_ue1);

    \draw[->,>=latex] (cc_uej) -- (scma_uej);
    \draw[->,>=latex] (cc_ue2) -- (scma_ue2);
    \draw[->,>=latex] (cc_ue1) -- (scma_ue1);

    \draw[->,>=latex] (scma_uej) -- (xj);
    \draw[->,>=latex] (scma_ue2) -- (x2);
    \draw[->,>=latex] (scma_ue1) -- (x1);

    \draw[->,>=latex] (xj) -- (plus) node [midway, below] {\tiny{$\bm{h}_j$}};
    \draw[->,>=latex] (x2) -- (plus) node [midway, below] {\tiny{$\bm{h}_2$}};
    \draw[->,>=latex] (x1) -- (plus) node [midway, above] {\tiny{$\bm{h}_1$}};

    \draw[->,>=latex] (plus) -- (dmap);

    \draw[->,>=latex] (dmap) -- (dscma);

    \draw[->,>=latex] (9.6, 0.225+0.000+0.0) -- (cd_uej);
    \draw[->,>=latex] (9.6, 0.225+0.900+0.4) -- (cd_ue2);
    \draw[->,>=latex] (9.6, 0.225+1.350+0.6) -- (cd_ue1);

    \draw[->,>=latex] (cd_uej) -- (out_bs_uej);
    \draw[->,>=latex] (cd_ue2) -- (out_bs_ue2);
    \draw[->,>=latex] (cd_ue1) -- (out_bs_ue1);

    \draw[dotted] (in_bs_ue2)  -- (in_bs_uej);
    \draw[dotted] (cc_ue2)     -- (cc_uej);
    \draw[dotted] (scma_ue2)   -- (scma_uej);
    \draw[dotted] (cd_ue2)     -- (cd_uej);
    \draw[dotted] (out_bs_ue2) -- (out_bs_uej);
  \end{tikzpicture}
  }
  \\
  \subfloat[][Factor graph representation of a decoder.]{
  \label{fig:scma_dec_graph}
  \begin{tikzpicture}[baseline]
    \tikzset{ vn/.style={draw=black, circle, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-3!40 } }
    \tikzset{ cn/.style={draw=black        , minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-1!40 } }

    \node[vn, label={[black]left:\tiny{$UE1$}}] (v1) at (0.00, 0.00+0.00) {};
    \node[vn, label={[black]left:\tiny{$UE2$}}] (v2) at (0.00, 0.25+0.40) {};
    \node[vn, label={[black]left:\tiny{$UE3$}}] (v3) at (0.00, 0.50+0.80) {};
    \node[vn, label={[black]left:\tiny{$UE4$}}] (v4) at (0.00, 0.75+1.20) {};
    \node[vn, label={[black]left:\tiny{$UE5$}}] (v5) at (0.00, 1.00+1.60) {};
    \node[vn, label={[black]left:\tiny{$UE6$}}] (v6) at (0.00, 1.25+2.00) {};

    \node[cn, label={[black]right:\tiny{$RES1$}}] (ca) at (1.00, 0.25+0.40) {};
    \node[cn, label={[black]right:\tiny{$RES2$}}] (cb) at (1.00, 0.50+0.80) {};
    \node[cn, label={[black]right:\tiny{$RES3$}}] (cc) at (1.00, 0.75+1.20) {};
    \node[cn, label={[black]right:\tiny{$RES4$}}] (cd) at (1.00, 1.00+1.60) {};

    \draw[] (v1) -- (ca);
    \draw[] (v1) -- (cb);
    \draw[] (v2) -- (cc);
    \draw[] (v2) -- (cd);
    \draw[] (v3) -- (ca);
    \draw[] (v3) -- (cc);
    \draw[] (v4) -- (cb);
    \draw[] (v4) -- (cd);
    \draw[] (v5) -- (ca);
    \draw[] (v5) -- (cd);
    \draw[] (v6) -- (cb);
    \draw[] (v6) -- (cc);

    \node[draw=Paired-3, rounded corners=2pt, minimum width=2.9cm, dashed, fit=(v1) (v6) (ca) (cd)] {};
  \end{tikzpicture}
  }
  \quad
  \subfloat[][Message Passing Algorithm based on Bayesian factor graph:\linebreak
              (I)   Resource to user message,
              (II)  Guess swap at each user and user to resource message,
              (III) Final guess at each user.]{
  \label{fig:scma_dec_alg}
  \begin{tikzpicture}[baseline]
    \tikzset{ vn/.style ={draw=black   , circle, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-3!40 } }
    \tikzset{ cn/.style ={draw=black           , minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-1!40 } }
    \tikzset{ vns/.style={draw=black!40, circle, minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-4!40 } }
    \tikzset{ cns/.style={draw=black!40        , minimum width=0.25cm, minimum height=0.25cm, text=black, fill=Paired-2!40 } }

    \node[vns, label={[black!40]left:\tiny{$UE1$}}] (v1_i) at (0.00, 0.00+0.00) {};
    \node[vn , label={[black   ]left:\tiny{$UE2$}}] (v2_i) at (0.00, 0.25+0.40) {};
    \node[vns, label={[black!40]left:\tiny{$UE3$}}] (v3_i) at (0.00, 0.50+0.80) {};
    \node[vn , label={[black   ]left:\tiny{$UE4$}}] (v4_i) at (0.00, 0.75+1.20) {};
    \node[vn , label={[black   ]left:\tiny{$UE5$}}] (v5_i) at (0.00, 1.00+1.60) {};
    \node[vns, label={[black!40]left:\tiny{$UE6$}}] (v6_i) at (0.00, 1.25+2.00) {};

    \node[cns, label={[black!40]right:\tiny{$RES1$}}] (ca_i) at (1.00, 0.25+0.40) {};
    \node[cns, label={[black!40]right:\tiny{$RES2$}}] (cb_i) at (1.00, 0.50+0.80) {};
    \node[cns, label={[black!40]right:\tiny{$RES3$}}] (cc_i) at (1.00, 0.75+1.20) {};
    \node[cn , label={[black   ]right:\tiny{$RES4$}}] (cd_i) at (1.00, 1.00+1.60) {};

    \draw[dotted, black!40                      ] (v1_i) -- (ca_i);
    \draw[dotted, black!40                      ] (v1_i) -- (cb_i);
    \draw[dotted, black!40                      ] (v2_i) -- (cc_i);
    \draw[<-,>=latex, Paired-5, line width=0.7pt] (v2_i) -- (cd_i);
    \draw[dotted, black!40                      ] (v3_i) -- (ca_i);
    \draw[dotted, black!40                      ] (v3_i) -- (cc_i);
    \draw[dotted, black!40                      ] (v4_i) -- (cb_i);
    \draw[->,>=latex, Paired-3, line width=0.7pt] (v4_i) -- (cd_i);
    \draw[dotted, black!40                      ] (v5_i) -- (ca_i);
    \draw[->,>=latex, Paired-3, line width=0.7pt] (v5_i) -- (cd_i);
    \draw[dotted, black!40                      ] (v6_i) -- (cb_i);
    \draw[dotted, black!40                      ] (v6_i) -- (cc_i);

    \node (i) at (1.65, 0.00+0.00) {\small{(I)}};

    \node[draw=Paired-3, rounded corners=2pt, minimum width=2.9cm, dashed, fit=(v1_i) (v6_i) (ca_i) (cd_i)] {};

    \newcommand\xshft{3.5}

    \node[vns, label={[black!40]left:\tiny{$UE1$}}] (v1_ii) at (\xshft+0.00, 0.00+0.00) {};
    \node[vns, label={[black!40]left:\tiny{$UE2$}}] (v2_ii) at (\xshft+0.00, 0.25+0.40) {};
    \node[vn , label={[black   ]left:\tiny{$UE3$}}] (v3_ii) at (\xshft+0.00, 0.50+0.80) {};
    \node[vns, label={[black!40]left:\tiny{$UE4$}}] (v4_ii) at (\xshft+0.00, 0.75+1.20) {};
    \node[vns, label={[black!40]left:\tiny{$UE5$}}] (v5_ii) at (\xshft+0.00, 1.00+1.60) {};
    \node[vns, label={[black!40]left:\tiny{$UE6$}}] (v6_ii) at (\xshft+0.00, 1.25+2.00) {};

    \node[cn , label={[black   ]right:\tiny{$RES1$}}] (ca_ii) at (\xshft+1.00, 0.25+0.40) {};
    \node[cns, label={[black!40]right:\tiny{$RES2$}}] (cb_ii) at (\xshft+1.00, 0.50+0.80) {};
    \node[cn , label={[black   ]right:\tiny{$RES3$}}] (cc_ii) at (\xshft+1.00, 0.75+1.20) {};
    \node[cns, label={[black!40]right:\tiny{$RES4$}}] (cd_ii) at (\xshft+1.00, 1.00+1.60) {};

    \draw[dotted, black!40                      ] (v1_ii) -- (ca_ii);
    \draw[dotted, black!40                      ] (v1_ii) -- (cb_ii);
    \draw[dotted, black!40                      ] (v2_ii) -- (cc_ii);
    \draw[dotted, black!40                      ] (v2_ii) -- (cd_ii);
    \draw[->,>=latex, Paired-5, line width=0.7pt] (v3_ii) -- (ca_ii);
    \draw[<-,>=latex, Paired-3, line width=0.7pt] (v3_ii) -- (cc_ii);
    \draw[dotted, black!40                      ] (v4_ii) -- (cb_ii);
    \draw[dotted, black!40                      ] (v4_ii) -- (cd_ii);
    \draw[dotted, black!40                      ] (v5_ii) -- (ca_ii);
    \draw[dotted, black!40                      ] (v5_ii) -- (cd_ii);
    \draw[dotted, black!40                      ] (v6_ii) -- (cb_ii);
    \draw[dotted, black!40                      ] (v6_ii) -- (cc_ii);

    \node (ii) at (\xshft+1.60, 0.00+0.00) {\small{(II)}};

    \node[draw=Paired-3, rounded corners=2pt, minimum width=2.9cm, dashed, fit=(v1_ii) (v6_ii) (ca_ii) (cd_ii)] {};

    \node[vns, label={[black!40]left:\tiny{$UE1$}}] (v1_iii) at (\xshft+\xshft+0.00, 0.00+0.00) {};
    \node[vns, label={[black!40]left:\tiny{$UE2$}}] (v2_iii) at (\xshft+\xshft+0.00, 0.25+0.40) {};
    \node[vn , label={[black   ]left:\tiny{$UE3$}}] (v3_iii) at (\xshft+\xshft+0.00, 0.50+0.80) {};
    \node[vns, label={[black!40]left:\tiny{$UE4$}}] (v4_iii) at (\xshft+\xshft+0.00, 0.75+1.20) {};
    \node[vns, label={[black!40]left:\tiny{$UE5$}}] (v5_iii) at (\xshft+\xshft+0.00, 1.00+1.60) {};
    \node[vns, label={[black!40]left:\tiny{$UE6$}}] (v6_iii) at (\xshft+\xshft+0.00, 1.25+2.00) {};

    \node[cn , label={[black   ]right:\tiny{$RES1$}}] (ca_iii) at (\xshft+\xshft+1.00, 0.25+0.40) {};
    \node[cns, label={[black!40]right:\tiny{$RES2$}}] (cb_iii) at (\xshft+\xshft+1.00, 0.50+0.80) {};
    \node[cn , label={[black   ]right:\tiny{$RES3$}}] (cc_iii) at (\xshft+\xshft+1.00, 0.75+1.20) {};
    \node[cns, label={[black!40]right:\tiny{$RES4$}}] (cd_iii) at (\xshft+\xshft+1.00, 1.00+1.60) {};

    \draw[dotted, black!40                      ] (v1_iii) -- (ca_iii);
    \draw[dotted, black!40                      ] (v1_iii) -- (cb_iii);
    \draw[dotted, black!40                      ] (v2_iii) -- (cc_iii);
    \draw[dotted, black!40                      ] (v2_iii) -- (cd_iii);
    \draw[<-,>=latex, Paired-5, line width=0.7pt] (v3_iii) -- (ca_iii);
    \draw[<-,>=latex, Paired-5, line width=0.7pt] (v3_iii) -- (cc_iii);
    \draw[dotted, black!40                      ] (v4_iii) -- (cb_iii);
    \draw[dotted, black!40                      ] (v4_iii) -- (cd_iii);
    \draw[dotted, black!40                      ] (v5_iii) -- (ca_iii);
    \draw[dotted, black!40                      ] (v5_iii) -- (cd_iii);
    \draw[dotted, black!40                      ] (v6_iii) -- (cb_iii);
    \draw[dotted, black!40                      ] (v6_iii) -- (cc_iii);

    \node (iii) at (\xshft+\xshft+1.55, 0.00+0.00) {\small{(III)}};

    \node[draw=Paired-3, rounded corners=2pt, minimum width=2.9cm, dashed, fit=(v1_iii) (v6_iii) (ca_iii) (cd_iii)] {};
  \end{tikzpicture}
  }
  \caption
    [SCMA encoding and decoding schemes.]
    {SCMA encoding and decoding schemes.}
  \label{fig:scma}
\end{figure}

An SCMA encoder with $J$ users (layers) and $K$ physical resources is a function
that maps a binary stream of data to $K$-dimensional complex constellations
$f : \mathbb{B}^{log_{2}(M)} \rightarrow \mathbb{X}, x = f(\bm{b})$ where
$\bm{X} \subset \mathbb{C}^k$. The $K$-dimensional complex codeword $x$ is a
sparse vector with $N < K$ non-zero entries. Each layer $j=1, ..., J$ has its
own codebook to generate the desired codeword according to the binary input
stream. Fig.~\ref{fig:scma} shows SCMA uplink chain with $J = 6$, $K = 4$ and
$N = 2$. SCMA codewords are spread over $K$ physical resources, such as OFDM
tones. Fig.~\ref{fig:scma_enc} shows that in the multiplexed scheme of SCMA, all
chosen codewords of the $J$ layers are added together after being multiplied by
the channel coefficient $\bm{h}_j$. Then, the entire uplink chain is shown in
Fig.~\ref{fig:scma_codec}. The output of the SCMA encoder is affected by the
white additive noise $\bm{n}$.
\begin{equation}
  \label{eq:scma_1}
  \bm{y} = \sum\limits_{j=1}^J \diag(\bm{h}_j)\bm{x}_j+\bm{n},
\end{equation}
where $\bm{x}_j=(x_1,...,x_{Kj})^T$ and $\bm{h}_j=(h_1,...,h_{Kj})^T$ are
respectively codeword and channel coefficients of layer $j$.

\subsection{SCMA Detection Schemes}
\label{sec:scma_detection}

\subsubsection{Maximum Likelihood}
\label{sec:scma_ml}

For an arbitrary codeword, the optimum decision, i.e. the one that minimizes the
likelihood of transmission errors after decoding, is the one resulting from the
Maximum Likelihood (ML) estimation, which can be described as:
\begin{equation}
  \label{eq:scma_2}
  \bm{\hat{x}_{ML}} = \argmin_{c \in \bm{X}}||\bm{y - c}||^2,
\end{equation}
given the received codeword. In \eqref{eq:scma_2}, the soft outputs
$\hat{\bm{x}}$ are also called Log-Likelihood Ratios (LLRs) that can be
calculated with the following equation:
\begin{equation}
  \label{eq:scma_3}
  LLR_x = \ln\Bigg(\frac{\sum_{C\in\mathcal{L}_x^0} \prob(\bm{y}|\bm{c})}
  {\sum_{C\in\mathcal{L}_x^1} \prob(\bm{y}|\bm{c})}\Bigg),
\end{equation}
where $LLR_x$ is the log likelihood ratio of bit $x$ obtained from codeword
$\hat{\bm{x}}$. This codeword comes from $\mathcal{L}_x^1$ the set of codewords
in which bit $x$ is 1 and $\mathcal{L}_x^0$ the set of codewords in which bit
$x$ is 0. The probability function $\prob(\bm{y}|\bm{c})$ can be expressed as
in \eqref{eq:scma_4} when a signal is transmitted over an additive white
Gaussian channel with $\sigma^2$ variance:
\begin{equation}
  \label{eq:scma_4}
  \prob(\bm{y}|\bm{c}) = \frac{1}{\sqrt{2\pi}\sigma}\exp
  \Bigg(-\frac{||\bm{y}-\bm{c}||^2}{2\sigma^2}\Bigg).
\end{equation}
Although the ML method provides the best guess for $\bm{\hat{x}_{ML}}$,
performing the computation with this method requires unacceptable complexity in
real applications. In the case of six users and codebooks matrices size
$4\times4$ as in Fig.~\ref{fig:scma_enc}, the calculation of the soft output
for each bit in \eqref{eq:scma_4} needs 4096 exponential function computations,
which is unacceptable. Nevertheless, in this article the result of this method
is used to compare with practical methods to characterize the BER performance
degradation of MPA and log-MPA.

\subsubsection{Message Passing Algorithm (MPA)}
\label{sec:scma_mpa}

Fig.~\ref{fig:scma_dec_graph} shows a Bayesian factor graph representation of an
MPA decoder with six users and four physical resources. Thanks to sparsity of
the codebooks, exactly three users collide in each physical resource. There are
four possible codewords for each of the three connected user's codebooks which
gives 64 possible combined codewords in each physical resource. In the first
step of the MPA, the 64~distances between each possible combined codewords and
the actual received codeword are calculated.
\begin{equation}
  \label{eq:scma_5}
  d_{RES  \beta}(\bm{m}, \bm{H}) =
  \underset{l \subset \zeta, m_u\in\{1,...,K\}}{||\bm{y}_\beta -
  \sum \bm{h}_{l,m_u} \bm{x}_{l,m_u} ||},
\end{equation}
$\zeta$ is the set of users connected to resource $\beta$ and the
considered codeword is denoted as $m$. For instance, \eqref{eq:scma_5} can be
re-written for resource 4 as:
\begin{equation}
  \label{eq:scma_6}
  \begin{split}
  d_{RES 4}(m_2,m_4,m_6,\bm{h}_2, \bm{h}_4, \bm{h}_5) =
  \underset{m_{2,4,6}=1,2,3,4}{|| \bm{y_4} - \Big(\bm{h}_2\bm{x}_2(m_2) +
  (\bm{h}_4\bm{x}_4(m_4) + (\bm{h}_5\bm{x}_5(m_5) \Big) ||}.
  \end{split}
\end{equation}
In which $m_2$, $m_4$, $m_5$ indicate the different codewords for users 5, 4,
and 2 in \eqref{eq:scma_6}. Assuming perfect channel estimation and Gaussian
noise, these Euclidean distances can be expressed as probabilities using
\eqref{eq:scma_7}:
\begin{equation}
  \label{eq:scma_7}
  \Psi(d_{RES \beta}) = \exp \Bigg(-\frac{d_{RES \beta}^2}{2\sigma^2} \Bigg).
\end{equation}
After calculating the residual probability of each codeword with
\eqref{eq:scma_7}, iterative MPA starts exchanging beliefs (probabilities) on
possible received codewords among the users and resources nodes of the
factor-graph. According to Fig.~\ref{fig:scma_dec_alg}(I), a message from
resources to users has been defined to contain extrinsic information of two
other connected users. For instance, a message from resource 4 to user 2
containing the probability information of codeword $i$ can be expressed as:
\begin{equation}
  \label{eq:scma_8}
  \begin{split}
  \mu_{RES4 \rightarrow UE2}(i) = \sum\limits_{j=1}^4 \sum\limits_{i=1}^4 \Psi
  \Big(d_{RES4}(i,j,k,\bm{H}) \Big)
  \times \mu_{UE4 \rightarrow RES4}(j) \times \mu_{UE5 \rightarrow RES4}(k).
  \end{split}
\end{equation}
As shown in Fig.~\ref{fig:scma_dec_alg}(II) there are only two resources
connected to each user. A message from a user to a resource is a normalized
guess swap at the user node:
\begin{equation}
  \label{eq:scma_9}
  \mu_{UE3 \rightarrow RES1}(i) = \frac{\mu_{RES3 \rightarrow UE3}(i)}
  {\sum_i\mu_{RES3 \rightarrow UE3}(i)},
\end{equation}
message passing between users and resources (see \eqref{eq:scma_8} and
\eqref{eq:scma_9}) will be repeated three to eight times to reach the desired
decoding performance. The final belief at each user B (i) is the multiplication
of all incoming messages as illustrated in Fig.~{\ref{fig:scma_dec_alg}}(III)
and \eqref{eq:scma_10} for UE4 and codeword $i$. Finally, \eqref{eq:scma_11} is
used to calculate soft outputs for bit $b_x$:
\begin{equation}
  \label{eq:scma_10}
  B_3(i) = \mu_{RES1 \rightarrow UE3}(i) \times \mu_{RES3 \rightarrow UE3}(i),
\end{equation}
\begin{equation}
  \label{eq:scma_11}
  LLR_x = \ln \Bigg( \frac{\prob(\bm{y}|b_x=0)}{\prob(\bm{y}|b_x=1)} \Bigg) =
  \ln \Bigg( \frac{\sum_m B_m(i)_{~when~b_x=0}}{\sum_m B_m(i)_{~when~b_x=1}}
  \Bigg).
\end{equation}

\subsubsection{Log-MAP}
\label{sec:scma_log-map}

Since calculation of exponentials in \eqref{eq:scma_7} requires relatively high
computational effort, changing the algorithm to log domain using the Jacobi
formula \eqref{eq:scma_12} is a classical improvement of MPA:
\begin{equation}
  \label{eq:scma_12}
  \ln \Bigg( \sum\limits_{i-1}^N\exp(f_i) \Bigg) \approx \max\{f_1,f_2,...,f_N\}
\end{equation}
using Jacobi formula, \eqref{eq:scma_8} can be reduced to:
\begin{equation}
  \label{eq:scma_13}
  \mu_{RES1 \rightarrow UE5}(i) = \underset{j,k=1,...,4}
  {\max \Bigg(-\frac{d_{RES1}^2(i,j,k,\bm{H})}{2\sigma^2} \Bigg)} +
  \mu_{UE2 \rightarrow RES1}(j) + \mu_{UE3 \rightarrow RES1}(k),
\end{equation}
due to elimination of exponential's high dynamic ranges, there is no need to
normalize the guess swap and \eqref{eq:scma_9} will be:
\begin{equation}
  \label{eq:scma_14}
  \mu_{UE3 \rightarrow RES1}(i) = \mu_{RES3 \rightarrow UE3}(i).
\end{equation}
The rest of the algorithm can be expressed as follows:
\begin{equation}
  \label{eq:scma_15}
  B_3(i) = \mu_{RES3 \rightarrow UE3}(i) + \mu_{RES1 \rightarrow UE3}(i),
\end{equation}
\begin{equation}
  \label{eq:scma_16}
  LLR_x = \max_i(B_m(i))_{~when~b_x=0} - \max_i(B_m(i))_{~when~b_x=1}.
\end{equation}

\subsubsection{Estimated-MPA (E-MPA)}

Computation of the exponentials in \eqref{eq:scma_7} is one of the most
important bottlenecks of the MPA algorithm. It is possible to further accelerate
the computation by using proper estimations. The exact exponential computation
is not essential to produce a satisfying estimation in the MPA algorithms.
Considering that \eqref{eq:scma_7} represents a Gaussian PDF, it can be replaced
by sub-optimal bell-shaped polynomial distributions to model the noise. It will
be shown in Section~\ref{sec:eval_scma_throughput} that using a
polynomial estimation can increase the throughput while leading to marginal bit
error rate degradation after the MPA decoding. However, these estimated
probabilities cause small degradations of the block error rate (BLER)
performance after the channel decoding (cf. Section~\ref{sec:scma_fec}). The
proposed PDF must satisfy two conditions to be valid: 1) it must be positive and
lower bounded at zero, 2) its integral over $(-\infty, \infty)$ must be equal to
1. The following function is suggested to estimate the exponentials:
\begin{equation}
  \label{eq:scma_18}
  \Psi^{'}_{d_{RES \beta}} = \frac{2 / \pi}{2\sigma^2 + 4d^4_{RES \beta}}.
\end{equation}
The computation of $\Psi^{'}$ is faster than the original
$\Psi$~\cite{Ghaffari2017}. The probabilities produced using \eqref{eq:scma_7}
and \eqref{eq:scma_18} are normalized according to \eqref{eq:scma_9}.
Furthermore, the numerator $2/\pi$ does not play an important role in MPA and
can be uniformly eliminated from all calculations to reduce the computational
effort. Thus,
\begin{equation}
  \label{eq:scma_19}
  \Psi^{'}_{d_{RES \beta}} \approx \frac{1}{2\sigma^2 + 4d^4_{RES \beta}},
\end{equation}
can be used as a systematic replacement to the exponential calculations.

% \subsection{Evaluation of Error Performance and Convergence Rate}
\subsection{Evaluation of Error Performance}
\label{sec:scma_perf}

\begin{figure}[htp]
  \centering
  \subfloat[][BER performance comparison of ML, MPA and E-MPA for 5 iterations.]
  {
    \includegraphics[width=0.70\linewidth]{scma/ber_uncoded/ber_uncoded}
    \label{plot:scma_ber_uncoded_a}
  }
  \\
  % \qquad
  \centering
  \subfloat[][Convergence behavior of E-MPA and MPA.]
  {
    \includegraphics[width=0.70\linewidth]{scma/ber_uncoded_iter/ber_uncoded_iter}
    \label{plot:scma_ber_uncoded_b}
  }
  \caption{Performance of MPA compared with E-MPA.}
  \label{plot:scma_ber_uncoded}
\end{figure}

Fig.~\ref{plot:scma_ber_uncoded_a} shows the performance comparison of a maximum
likelihood (ML) decoder, an MPA decoder performing 5 iterations and an
estimated-MPA (E-MPA) decoder as explained in Section~\ref{sec:opt_scma} also
performing 5 iterations. There are very small differences in the bit error rate
performance of the three decoders (less than 0.10 dB). Although both MPA and
E-MPA show their optimum behavior with 5 iterations, the convergence behavior of
the two methods are different as illustrated in
Fig.~\ref{plot:scma_ber_uncoded_b}. E-MPA has a slower convergence rate for less
than three iterations. This phenomenon is expected as the probability functions
produced by bell-shaped polynomial PDF do not have the quality of probabilities
produced by exponentials. However, the convergence behavior is almost identical
for more than 4 iterations. The other optimizations like loops unrolling, fast
math libraries and vectorization were not found to degrade the BER performance
or the convergence rates.

\subsection{Channel Coding}
\label{sec:scma_fec}

\subsubsection{Complete Simulation Chain}
\label{sec:scma_fec_chain}

In the previous sections of this article, algorithmic improvements and
implementation techniques have been proposed. These optimizations lead to
drastic reductions of the processing time and to an increase of the processing
power efficiency. This is done with approximately no degradation of the BER
performance after SCMA decoding. Nevertheless, in a full communication chain,
multiple access algorithms are closely linked to the Forward Error Correction
(FEC) modules. Indeed, the input of the FEC decoder consists in the outputs of
the SCMA decoder.

In order to claim that the proposed improvements do not degrade the overall
error performance, it is necessary to embed the SCMA encoder and decoder in a
full communication chain. To this purpose, we used the \AFFECT\footnote{\AFFECT
is an Open-source software (MIT license) for fast forward error correction
simulations, see \url{http://aff3ct.github.io}} software which is an ideal
tool that provides the necessary simulation models and allows performing the
desired verifications.

\AFFECT is Open-source and specifically designed to offer an efficient
environment to the communication systems designers. Monte-Carlo simulations can
be run to measure various metrics such as the BER and BLER performance, or the
throughputs and latencies of each module, e.g. FEC encoders and decoders,
modulation and demodulation blocks, or different channel models.

\begin{figure}[htp]
  \centering
  \subfloat[][Code rate $R=1/3$.]
  {
  \includegraphics[width=0.70\linewidth]{scma/fec/fec_1_3}
    \label{plot:scma_fec_a}
  }
  \\
  % \qquad
  \centering
  \subfloat[][Code rate $R=1/2$.]
  {
  \includegraphics[width=0.70\linewidth]{scma/fec/fec_1_2}
    \label{plot:scma_fec_b}
  }
  \caption
    [BLER evaluation of MPA and E-MPA decoders combined with channel coding.]
    {BLER evaluation of SCMA MPA and E-MPA decoders combined with LDPC, polar
    and turbo codes.}
  \label{plot:scma_fec}
\end{figure}

According to the latest 3GPP report~\cite{ETSI2018}, in the 5G standard, the
two selected code families are the LDPC and polar codes. Being implemented in
the \AFFECT software, it is possible to test our SCMA decoders in a complete
communication chain, in conjunction with state-of-the art LDPC, polar and even
turbo decoders that were used in the LTE standard~\cite{ETSI2013}.
Fig.~\ref{plot:scma_fec} shows the BLER performances of MPA and E-MPA decoders
when combined with different channel codes. For a matter of reproducibility, the
full parameters of the FEC used are reported in the next section. This research
does not claim any novelty in channel coding, however, we found crucial to
validate our proposed SCMA optimizations in a sufficiently complete
communication chain.

\subsubsection{Channel Coding Configurations}
% \subsection{Forward Error Correction Characteristics}
\label{sec:scma_fec_characteristics}

\paragraph{Turbo codes}

In a first validation, the turbo code from the LTE standard is used. In the
decoder, 6 iterations are done. The two sub-decoders implement the max-log
Maximum A Posteriori algorithm (max-log-MAP)~\cite{Robertson1995} with a 0.75
scaling factor~\cite{Vogt2000}. In Fig.~\ref{plot:scma_fec_a}, the rate is
$R \approx 1/3$, no puncturing is performed, the number of information bits
$K$ is 1024 and the codeword length $N$ is 3084. In Fig.~\ref{plot:scma_fec_b},
$R \approx 1/2$ with the puncturing of half of the parity bits,
$K=2048$, and $N=4108$.

\paragraph{LDPC codes}

In a second set of validations, the LDPC codes used in this paper are based on
MacKay's matrices that have been taken from its personal
webpage\footnote{MacKay's webpage: \url{http://www.inference.org.uk/mackay/codes/data.html}}.
In Fig.~\ref{plot:scma_fec_a}, the matrix used is ($K=272$, $N=816$), and in
Fig.~\ref{plot:scma_fec_b} the matrix is ($K=2000$, $N=4000$). In both figures,
the decoder used is a Belief Propagation (BP) decoder with an Horizontal Layered
scheduling~\cite{Yeo2001}. For the update rules, the Sum-Product Algorithm (SPA)
has been used~\cite{MacKay1999}. The number of iterations is 100.

\paragraph{Polar codes}

In the final validation, polar codes are built by suitably selecting the frozen
bits. We used the Gaussian Approximation (GA) technique of~\cite{Trifonov2012}.
The input SNR for the code construction with the GA is 1 dB, which apparently is
very low considering that the SNR are 4 to 5 dB in the convergence zone. This is
motivated by the fact that the GA algorithm is designed to work with the BPSK
modulation. Using SCMA completely modifies the histogram of the LLR values for a
given SNR. Therefore, a shift on the input SNR of the GA algorithm must be
applied in order to efficiently select the frozen bits. If this shift is
not applied, the decoding performances of the polar code degrades drastically.
The number of information bits and the codeword length are ($K=682$, $N=2048$)
in Fig.~\ref{plot:scma_fec_a} and ($K=2048$, $N=4096$) in
Fig.~\ref{plot:scma_fec_b}. The decoder is a Successive Cancellation List (SCL)
decoder with $L=32$ and a 32-bit GZIP CRC that was proposed
in~\cite{Leonardon2019}.

\subsubsection{Effects of E-MPA on Error Correction}
\label{sec:scma_fec_e-mpa}

In Fig.~\ref{plot:scma_fec}, the number of iterations of the SCMA demodulator is
5. The objective of simulating multiple channel codes is not to compare them
with each other. A fair comparison of the different channel codes would indeed
impose using the same code lengths and more importantly their computational
complexity should be compared, which is not the case here. Our goal here is to
study the impact of using E-MPA on the BER and FER performances when the channel
codes are included in the communication chain. For each channel code, two curves
are plotted: one for the E-MPA and the other for the MPA. Only 0.2 to 0.4 dB
separate the two versions of the algorithm for all the considered channel codes.
These results show the extent to which uncertainty of estimations affects
channel coding. The decoding speed improvement brought by the E-MPA algorithm
has a cost in terms of decoding performance. This trade-off should be considered
in order to meet the system constraints.

\section{Discussion}

\begin{itemize}
  \item \xmark~mettre en avant le challenge d'avoir des algorithmes assez
    différents avec des structures de données variées
  \item \xmark~indiquer quelques utilisations typiques des algos les plus
    utilisés (5G, satellite, etc.)
  \item \xmark~dire que les décodeurs/démodulateurs présentés ne sont pas
    exhausitifs par famille de code, ils sont juste représentatifs des plus
    utilisés et discutés dans la littérature today (en citer quelques autres qui
    semblent prometteurs)
\end{itemize}