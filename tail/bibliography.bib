% Encoding: UTF-8

@TechReport{Ericsson2015,
  author      = {Ericsson},
  title       = {Cloud {RAN} - The Benefits of Virtualization, Centralisation and Coordination},
  institution = {Ericsson},
  year        = {2015},
  file        = {:pdf/Ericsson2015 - Cloud RAN - The Benefits of Virtualization, Centralisation and Coordination.pdf:PDF},
  groups      = {Cloud-RAN},
  url         = {https://www.ericsson.com/assets/local/publications/white-papers/wp-cloud-ran.pdf},
}

@TechReport{Huawei2013,
  author      = {Huawei},
  title       = {{5G}: A Technology Vision},
  institution = {Huawei},
  year        = {2013},
  file        = {:pdf/Huawei2013 - 5G\: A Technology Vision.pdf:PDF},
  groups      = {5G},
  url         = {https://www.huawei.com/ilink/en/download/HW_314849},
}

@InProceedings{Nikaein2015,
  author    = {N. Nikaein},
  title     = {Processing Radio Access Network Functions in the Cloud: Critical Issues and Modeling},
  booktitle = {International Workshop on Mobile Cloud Computing and Services (MCS)},
  year      = {2015},
  pages     = {36--43},
  publisher = {ACM},
  doi       = {10.1145/2802130.2802136},
  file      = {:pdf/Nikaein2015 - Processing Radio Access Network Functions in the Cloud\: Critical Issues and Modeling.pdf:PDF},
  groups    = {Cloud-RAN},
}

@InProceedings{Rowshan2019,
  author    = {M. {Rowshan} and E. {Viterbo}},
  title     = {How to Modify Polar Codes for List Decoding},
  booktitle = {International Symposium on Information Theory (ISIT)},
  year      = {2019},
  pages     = {1772--1776},
  publisher = {IEEE},
  doi       = {10.1109/ISIT.2019.8849539},
  file      = {:pdf/Rowshan2019 - How to Modify Polar Codes for List Decoding.pdf:PDF},
  groups    = {Polar Codes},
}

@Book{Dahlman2013,
  title     = {{4G}: {LTE/LTE}-Advanced for Mobile Broadband},
  doi       = {10.1016/C2013-0-06829-6},
  publisher = {Academic press},
  year      = {2013},
  author    = {Dahlman, E. and Parkvall, S. and Skold, J.},
  file      = {:pdf/Dahlman2013 - 4G\: LTE LTE-Advanced for Mobile Broadband.pdf:PDF},
  groups    = {4G},
}

@Book{Knuth1973,
  title     = {The Art of Computer Programming},
  publisher = {Addison-Wesley},
  year      = {1973},
  author    = {Knuth, D.E.},
  number    = {3},
  file      = {:pdf/Knuth1973 - The Art of Computer Programming.pdf:PDF},
  pages     = {207--209},
}

@Article{Schreier1932,
  author  = {J. Schreier},
  title   = {On Tournament Elimination Systems},
  journal = {Mathesis Polska},
  year    = {1932},
  volume  = {7},
  pages   = {154--160},
  url     = {https://ci.nii.ac.jp/naid/10027928626/en/},
}

@InProceedings{Furtak2007,
  author    = {T. Furtak and J. N. Amaral and R. Niewiadomski},
  title     = {Using {SIMD} Registers and Instructions to Enable Instruction-Level Parallelism in Sorting Algorithms},
  booktitle = {Symposium on Parallel Algorithms and Architectures},
  year      = {2007},
  pages     = {348--357},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Most contemporary processors offer some version of Single Instruction Multiple Data (SIMD) machinery - vector registers and instructions to manipulate data stored in such registers. The central idea of this paper is to use these SIMD resources to improve the performance of the tail of recursive sorting algorithms. When the number of elements to be sorted reaches a set threshold, data is loaded into the vector registers, manipulated in-register, and the result stored back to memory. Three implementations of sorting with two different SIMD machineries - x86-64's SSE2 and G5's AltiVec - demonstrate that this idea delivers significant speed improvements. The improvements provided are orthogonal to the gains obtained through empirical search for a suitable sorting algorithm [11]. When integrated with the Dynamically Tuned Sorting Library (DTSL) this new code generation strategy reduces the time spent by DTSL up to 22\% for moderately-sized arrays, with greater relative reductions for small arrays. Wall-clock performance of d-heaps is improved by up to 39\% using a similar technique.},
  acmid     = {1248436},
  doi       = {10.1145/1248377.1248436},
  file      = {:pdf/Furtak2007 - Using SIMD Registers and Instructions to Enable Instruction-Level Parallelism in Sorting Algorithms.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), Sort},
  isbn      = {978-1-59593-667-7},
  keywords  = {SIMD, instruction-level parallelism, quicksort, sorting, sorting networks, vectorization},
  location  = {San Diego, California, USA},
  numpages  = {10},
}

@Article{Matsumoto1998,
  author  = {M. Matsumoto and T. Nishimura},
  title   = {Mersenne Twister: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator},
  journal = {ACM Transactions on Modeling and Computer Simulation (TOMACS)},
  year    = {1998},
  volume  = {8},
  number  = {1},
  pages   = {3--30},
  doi     = {10.1145/272991.272995},
  file    = {:pdf/Matsumoto1998 - Mersenne Twister\: A 623-Dimensionally Equidistributed Uniform Pseudo-Random Number Generator.pdf:PDF},
  groups  = {Pseudo-Random Number Generator (PRNG)},
}

@Online{Walter,
  author   = {J. Walter and M. Koch},
  title    = {{uBLAS}},
  abstract = {uBLAS is a C++ template class library that provides BLAS level 1, 2, 3 functionality for dense, packed and sparse matrices. The design and implementation unify mathematical notation via operator overloading and efficient code generation via expression templates.

uBLAS provides templated C++ classes for dense, unit and sparse vectors, dense, identity, triangular, banded, symmetric, hermitian and sparse matrices. Views into vectors and matrices can be constructed via ranges, slices, adaptor classes and indirect arrays. The library covers the usual basic linear algebra operations on vectors and matrices: reductions like different norms, addition and subtraction of vectors and matrices and multiplication with a scalar, inner and outer products of vectors, matrix vector and matrix matrix products and triangular solver. The glue between containers, views and expression templated operations is a mostly STL conforming iterator interface.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {http://www.boost.org/libs/numeric},
}

@InProceedings{Henretty2011,
  author    = {T. Henretty and K. Stock and L-N. Pouchet and F. Franchetti and J. Ramanujam and P. Sadayappan},
  title     = {Data Layout Transformation for Stencil Computations on Short-Vector {SIMD} Architectures},
  booktitle = {International Conference on Compiler Construction (CC)},
  year      = {2011},
  file      = {:pdf/Henretty2011 - Data Layout Transformation for Stencil Computations on Short-Vector SIMD Architectures.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  keywords  = {stencil, data, layout, transformation, vectorization,simd},
}

@InProceedings{Videau2013,
  author    = {B. Videau and V. Marangozova-Martin and L. Genovese and T. Deutsch},
  title     = {Optimizing {3D} Convolutions for Wavelet Transforms on {CPUs} with {SSE} Units and {GPUs}},
  booktitle = {International European Conference on Parallel and Distributed Computing (Euro-Par)},
  year      = {2013},
  publisher = {Springer},
  doi       = {10.1007/978-3-642-40047-6_82},
  file      = {:pdf/Videau2013 - Optimizing 3D Convolutions for Wavelet Transforms on CPUs with SSE Units and GPUs.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  isbn      = {978-3-642-40047-6},
  keywords  = {optimization},
}

@Article{Igual2011,
  author   = {F. D. Igual and E. Chan and E. S. Quintana-Ortí and G. Quintana-Ortí and R. A. {van de Geijn} and F. G. {Van Zee}},
  title    = {The {FLAME} Approach: From Dense Linear Algebra Algorithms to High-Performance Multi-Accelerator Implementations},
  journal  = {Elsevier Journal of Parallel and Distributed Computing (JPDC)},
  year     = {2012},
  volume   = {72},
  number   = {9},
  pages    = {1134--1143},
  month    = {Sept},
  issn     = {0098-3500},
  doi      = {10.1016/j.jpdc.2011.10.014},
  file     = {:pdf/Igual2011 - The FLAME Approach\: From Dense Linear Algebra Algorithms to High-Performance Multi-Accelerator Implementations.pdf:PDF},
  keywords = {flame, libflame, supermatrix, dense, linear algebra, parallel,runtime,scheduling,multi accelerator},
}

@InProceedings{Meng2010,
  author    = {L. Meng and Y. Voronenko and J. R. Johnson and M. Moreno Maza and F. Franchetti and Y. Xie},
  title     = {Spiral-Generated Modular {FFT} Algorithms},
  booktitle = {International Workshop on Parallel and Symbolic Computation (PASCO)},
  year      = {2010},
  series    = {PASCO '10},
  pages     = {169--170},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {This paper presents an extension of the Spiral system to automatically generate and optimize FFT algorithms for the discrete Fourier transform over finite fields. The generated code is intended to support modular algorithms for multivariate polynomial computations in the modpn library used by Maple. The resulting code provides an order of magnitude speedup over the original implementations in the modpn library, and the Spiral system provides the ability to automatically tune the FFT code to different computing platforms.},
  acmid     = {1837235},
  doi       = {10.1145/1837210.1837235},
  file      = {:pdf/Meng2010 - Spiral-Generated Modular FFT Algorithms.pdf:PDF},
  isbn      = {978-1-4503-0067-4},
  keywords  = {FFT, autotuning, code generation, high performance computing, modular arithmetic, vectorization, FFT, autotuning, code generation, high performance computing, modular arithmetic, vectorization},
  location  = {Grenoble, France},
  numpages  = {2},
}

@Misc{Gallager1963,
  author = {R. G. Gallager},
  title  = {Low-Density Parity-Check Codes},
  year   = {1963},
  file   = {:pdf/Gallager1963 - Low-Density Parity-Check Code.pdf:PDF},
  groups = {LDPC Codes},
  url    = {http://www.inference.org.uk/mackay/gallager/papers/ldpc.pdf},
}

@Book{Wyglinski2009,
  title     = {Cognitive Radio Communications and Networks: Principles and Practice},
  publisher = {Academic Press},
  year      = {2009},
  author    = {A. M. Wyglinski and M. Nekovee and T. Hou},
  editor    = {Elsevier},
  file      = {:pdf/Wyglinski2009 - Cognitive Radio Communications and Networks\: Principles and Practice.pdf:PDF},
}

@InProceedings{Dutta2010,
  author    = {P. Dutta and Y. Kuo and A. Ledeczi and T. Schmid and P. Volgyesi},
  title     = {Putting the Software Radio on a Low-calorie Diet},
  booktitle = {Workshop on Hot Topics in Networks (HotNets)},
  year      = {2010},
  publisher = {ACM},
  abstract  = {Modern software-defined radios are large, expensive, and power-hungry devices and this, we argue, hampers their more widespread deployment and use, particularly in low-power, size-constrained application settings like mobile phones and sensor networks. To rectify this problem, we propose to put the software-defined radio on a diet by redesigning it around just two core chips -- an integrated RF transceiver and a Flash-based, mixed-signal FPGA. Modern transceivers integrate almost all RF front-end functions while emerging FPGAs integrate nearly all of required signal conditioning and processing functions. And, unlike conventional FPGAs, Flash-based FPGAs offer sleep mode power draws measured in the microamps and startup times measured in the microseconds, both of which are critical for low-power operation. If our platform architecture vision is realized, it will be possible to hold a software-defined radio in the palm of one's hand, build it for $100, and power it for days using the energy in a typical mobile phone battery. This will make software radios deployable in high densities and broadly accessible for research and education.},
  acmid     = {1868467},
  doi       = {10.1145/1868447.1868467},
  file      = {:pdf/Dutta2010 - Putting the Software Radio on a Low-calorie Diet.pdf:PDF},
  groups    = {FPGA},
  isbn      = {978-1-4503-0409-2},
  keywords  = {energy-efficiency, low-power communications, software defined radios, wireless sensor networks},
}

@Article{Shaik2013,
  author  = {S. Shaik and S. Angadi},
  title   = {Architecture and Component Selection for {SDR} Applications},
  journal = {International Journal of Engineering Trends and Technology (IJETT)},
  year    = {2013},
  volume  = {4},
  number  = {4},
  pages   = {691--694},
  file    = {:pdf/Shaik2013 - Architecture and Component Selection for SDR Applications.pdf:PDF},
  groups  = {FPGA, DSP},
  url     = {http://www.ijettjournal.org/volume-4/issue-4/IJETT-V4I4P236.pdf},
}

@Misc{ETSI2013,
  author = {ETSI},
  title  = {{3GPP} - {TS} 36.212 - {Multiplexing} and Channel Coding ({R.} 11)},
  month  = aug,
  year   = {2013},
  file   = {:pdf/ETSI2013 - 3GPP - TS 36.212 - Multiplexing and Channel Coding (R. 11).pdf:PDF},
  groups = {4G},
  url    = {https://www.etsi.org/deliver/etsi_ts/136200_136299/136212/11.03.00_60/ts_136212v110300p.pdf},
}

@Article{Benkeser2009,
  author   = {Benkeser, C. and Burg, A. and Cupaiuolo, T. and Qiuting Huang},
  title    = {Design and Optimization of an {HSDPA} Turbo Decoder {ASIC}},
  journal  = {IEEE Journal of Solid-State Circuits (JSSC)},
  year     = {2009},
  volume   = {44},
  number   = {1},
  pages    = {98--106},
  abstract = {The turbo decoder is the most challenging component in a digital HSDPA receiver in terms of computation requirement and power consumption, where large block size and recursive algorithm prevent pipelining or parallelism to be effectively deployed. This paper addresses the complexity and power consumption issues at algorithmic, arithmetic and gate levels of ASIC design, in order to bring power consumption and die area of turbo decoders to a level commensurate with wireless application. Realized in 0.13 &nbsp;mum CMOS technology, the turbo decoder ASIC measures 1.2&nbsp;mm2 excluding pads, and can achieve 10.8 Mb/s throughput while consuming only 32 mW.},
  doi      = {10.1109/JSSC.2008.2007166},
  file     = {:pdf/Benkeser2009 - Design and Optimization of an HSDPA Turbo Decoder ASIC.pdf:PDF},
  groups   = {Turbo Codes, Hardware Decoders},
}

@Article{Sun2011,
  author  = {Y. Sun and J. R. Cavallaro},
  title   = {Efficient Hardware Implementation of a Highly-Parallel {3GPP} {LTE/LTE}-Advance Turbo Decoder},
  journal = {Elsevier Integration, the VLSI Journal},
  year    = {2011},
  volume  = {44},
  number  = {4},
  pages   = {305--315},
  file    = {:pdf/Sun2011 - Efficient Hardware Implementation of a Highly-Parallel 3GPP LTE LTE-Advance Turbo Decoder.pdf:PDF},
  groups  = {Turbo Codes, Hardware Decoders},
}

@Book{Stroustrup2013,
  title     = {The {C++} Programming Language},
  publisher = {Addison-Wesley Professional},
  year      = {2013},
  author    = {Stroustrup, Bjarne},
  edition   = {4th},
  isbn      = {0321563840, 9780321563842},
  file      = {:pdf/Stroustrup2013 - The C++ Programming Language.pdf:PDF},
  groups    = {C++},
}

@Article{Gottschling2009,
  author   = {P. Gottschling and David S. Wise and Adwait Joshi},
  title    = {Generic Support of Algorithmic and Structural Recursion for Scientific Computing},
  journal  = {The International Journal of Parallel, Emergent and Distributed Systems ({IJPEDS})},
  year     = {2009},
  volume   = {24},
  number   = {6},
  pages    = {479--503},
  month    = {12/2009},
  note     = {Accepted},
  abstract = {Recursive algorithms, like quick-sort and recursive data structures, like trees, play a central role in programming. In the context of scientific computing, recursive algorithms and memory layouts are shown here to provide excellent cache and Translation Lookaside Buffer (TLB) locality independently of the platform. We show how, for the first time, generic programming and object-oriented programming allow us to abstract a multitude of dense-matrix memory layouts: from conventional row-major and column-major layouts over Z- and И-Morton orders to block-wise combinations of them. All are provided by a single class that is based on our new matrix abstraction.

The algorithmic recursion is supported in generic fashion by classes modelling the new recursator, an analogue of the Standard Template Library iterator. Although this concept supports recursion in general, we focus again on matrix operations. Results are presented for matrix multiplication, on both conventional and tiled representations using both homogeneous and heterogeneous matrix representations. Reaching about 60\% peak performance in portable C++ code establishes competitive performance without explicit prefetching and other platform-specific tuning. Comparisons with the manufacturers' libraries show superior locality. These new techniques are embedded in the Matrix Template Library, Version 4 (MTL4).},
  doi      = {10.1080/17445760902758560},
  file     = {:pdf/Gottschling2009 - Generic Support of Algorithmic and Structural Recursion for Scientific Computing.pdf:PDF},
  keywords = {dilated integers},
}

@InProceedings{Spampinato2014,
  author    = {D. G. Spampinato and M. P\"{u}schel},
  title     = {A Basic Linear Algebra Compiler},
  booktitle = {International Symposium on Code Generation and Optimization (CGO)},
  year      = {2014},
  series    = {CGO '14},
  pages     = {23:23--23:32},
  address   = {New York, NY, USA},
  publisher = {ACM, IEEE},
  acmid     = {2544155},
  articleno = {23},
  doi       = {10.1145/2544137.2544155},
  file      = {:pdf/Spampinato2014 - A Basic Linear Algebra Compiler.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), Kernel},
  isbn      = {978-1-4503-2670-4},
  keywords  = {Basic linear algebra, DSL, Program synthesis, SIMD vectorization, Small matrices, Tiling},
  location  = {Orlando, FL, USA},
  numpages  = {10},
}

@Article{Williams2009,
  author     = {S. Williams and A. Waterman and D. Patterson},
  title      = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  journal    = {Communications of the ACM},
  year       = {2009},
  volume     = {52},
  number     = {4},
  pages      = {65--76},
  month      = apr,
  issn       = {0001-0782},
  acmid      = {1498785},
  address    = {New York, NY, USA},
  doi        = {10.1145/1498765.1498785},
  file       = {:pdf/Williams2009 - Roofline\: An Insightful Visual Performance Model for Multicore Architectures.pdf:PDF},
  issue_date = {April 2009},
  numpages   = {12},
  publisher  = {ACM},
}

@InProceedings{Treibig2009,
  author    = {J. Treibig and G. Hager},
  title     = {Introducing a Performance Model for Bandwidth-Limited Loop Kernels},
  booktitle = {International Conference on Parallel Processing and Applied Mathematics: Part I (PPAM)},
  year      = {2009},
  pages     = {615--624},
  publisher = {ACM},
  file      = {:pdf/Treibig2009 - Introducing a Performance Model for Bandwidth-Limited Loop Kernels.pdf:PDF},
  isbn      = {3-642-14389-X, 978-3-642-14389-2},
  doi       = {10.1007/978-3-642-14390-8_64},
}

@InProceedings{Cassagne2016b,
  author    = {A. Cassagne and O. Aumage and C. Leroux and D. Barthou and B. {Le Gal}},
  title     = {Energy Consumption Analysis of Software Polar Decoders on Low Power Processors},
  booktitle = {European Signal Processing Conference (EUSIPCO)},
  year      = {2016},
  pages     = {642--646},
  month     = aug,
  publisher = {IEEE},
  abstract  = {This paper presents a new dynamic and fully generic implementation of a Successive Cancellation (SC) decoder (multi-precision support and intra-/inter-frame strategy support). This fully generic SC decoder is used to perform comparisons of the different configurations in terms of throughput, latency and energy consumption. A special emphasis is given on the energy consumption on low power embedded processors for software defined radio (SDR) systems. A N=4096 code length, rate 1/2 software SC decoder consumes only 14 nJ per bit on an ARM Cortex-A57 core, while achieving 65 Mbps. Some design guidelines are given in order to adapt the configuration to the application context.},
  doi       = {10.1109/EUSIPCO.2016.7760327},
  file      = {:pdf/Cassagne2016b - Energy Consumption Analysis of Software Polar Decoders on Low Power Processors.pdf:PDF;:pdf/Cassagne2016b - Energy Consumption Analysis of Software Polar Decoders on Low Power Processors [poster].pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  keywords  = {decoding, energy consumption, software radio, telecommunication power management, ARM Cortex-A57, SC decoder implementation, SDR system, low power embedded processor, software defined radio system, software polar decoder energy consumption analysis, successive cancellation decoder implementation, Bit error rate, Decoding, Encoding, Energy consumption, Program processors, Throughput, Cassagne},
}

@InProceedings{Cassagne2016a,
  author    = {A. Cassagne and T. Tonnellier and C. Leroux and B. {Le Gal} and O. Aumage and D. Barthou},
  title     = {Beyond {G}bps Turbo decoder on multi-core {CPUs}},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2016},
  pages     = {136--140},
  month     = sep,
  publisher = {IEEE},
  abstract  = {This paper presents a high-throughput implementation of a portable software turbo decoder. The code is optimized for traditional multi-core CPUs (like x86) and it is based on the Enhanced max-log-MAP turbo decoding variant. The code follows the LTE-Advanced specification. The key of the high performance comes from an inter-frame SIMD strategy combined with a fixed-point representation. Our results show that proposed multi-core CPU implementation of turbo-decoders is a challenging alternative to GPU implementation in terms of throughput and energy efficiency. On a high-end processor, our software turbo-decoder exceeds 1 Gbps information throughput for all rate-1/3 LTE codes with K $<$; 4096.},
  doi       = {10.1109/ISTC.2016.7593092},
  file      = {:pdf/Cassagne2016a - Beyond Gbps Turbo Decoder on Multi-Core CPUs.pdf:PDF;:pdf/Cassagne2016a - Beyond Gbps Turbo Decoder on Multi-Core CPUs [poster].pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP, AFF3CT},
  keywords  = {codecs, maximum likelihood decoding, microprocessor chips, turbo codes, Gbps turbo decoder, energy efficiency, enhanced max-log-MAP turbo decoding variant, inter-frame SIMD strategy, multicore CPU, portable software turbo decoder, rate-l/3 LTE codes, Instruction sets, Measurement, Cassagne},
}

@Online{AFF3CT2016,
  author   = {AFF3CT},
  title    = {{AFF3CT}: The first software release},
  year     = {2016},
  abstract = {This release contains Polar, Turbo, RSC (Recursive Systematic Convolutional), Repetition and RA (Repeat and Accumulate) codes. The simulation includes a BPSK modulation and an AWGN channel. Some Polar, RSC and Turbo decoders are optimized with SIMD instructions.},
  doi      = {10.5281/zenodo.55668},
}

@InProceedings{Xianjun2013,
  author    = {J. Xianjun and C. Canfeng and P. Jääskeläinen and V. Guzma and H. Berg},
  title     = {A 122{Mb}/s Turbo decoder using a mid-range {GPU}},
  booktitle = {International Wireless Communications and Mobile Computing Conference (IWCMC)},
  year      = {2013},
  pages     = {1090--1094},
  month     = jul,
  publisher = {IEEE},
  abstract  = {Parallel implementations of Turbo decoding has been studied extensively. Traditionally, the number of parallel sub-decoders is limited to maintain acceptable code block error rate performance loss caused by the edge effect of code block division. In addition, the sub-decoders require synchronization to exchange information in the iterative process. In this paper, we propose loosening the synchronization between the sub-decoders to achieve higher utilization of parallel processor resources. Our method allows high degree of parallel processor utilization in decoding of a single code block providing a scalable software-based implementation. The proposed implementation is demonstrated using a graphics processing unit. We achieve 122.8Mbps decoding throughput using a medium range GPU, the Nvidia GTX480. This is, to the best of our knowledge, the fastest Turbo decoding throughput achieved with a GPU-based implementation.},
  doi       = {10.1109/IWCMC.2013.6583709},
  file      = {:pdf/Xianjun2013 - A 122Mbs Turbo decoder using a mid-range GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2376-6492},
  keywords  = {graphics processing units, parallel processing, turbo codes, GPU based implementation, Nvidia GTX480, Turbo decoder, Turbo decoding throughput, code block division, code block error rate performance loss, edge effect, graphics processing unit, iterative process, medium range GPU, mid range GPU, parallel processor resources, parallel processor utilization, scalable software based implementation, single code block, subdecoders, synchronization, Decoding, Graphics processing units, Iterative decoding, Synchronization, Throughput, Turbo codes, GPGPU, Loose synchronization, Massively parallel computation, OpenCL, Turbo decoder},
}

@InProceedings{Belfanti2013,
  author    = {S. Belfanti and C. Roth and M. Gautschi and C. Benkeser and Q. Huang},
  title     = {A 1{Gb}ps {LTE}-Advanced Turbo-Decoder {ASIC} in 65{nm} {CMOS}},
  booktitle = {Symposium on VLSI Circuits},
  year      = {2013},
  pages     = {C284--C285},
  month     = jun,
  publisher = {IEEE},
  abstract  = {This paper presents a turbo-decoder ASIC for 3GPP LTE-Advanced supporting all specified code rates and block sizes. The highly parallelized architecture employs 16 SISO decoders with an optimized state-metric initialization scheme that reduces SISO-decoder latency, which is key for achieving very-high throughput. A novel CRC implementation for parallel turbo decoding prevents the decoder from performing redundant turbo iterations. The 65nm ASIC achieves a record data throughput of 1.013Gbps at 5.5 iterations with unprecedented energy efficiency of 0.17nJ/bit/iter.},
  file      = {:pdf/Belfanti2013 - A 1Gbps LTE-Advanced Turbo-Decoder ASIC in 65nm CMOS.pdf:PDF},
  groups    = {Turbo Codes, Hardware Decoders},
  issn      = {2158-5601},
  keywords  = {3G mobile communication, Long Term Evolution, decoding, 3GPP LTE advanced, CMOS, LTE advanced turbo decoder ASIC, SISO decoder latency, optimized state metric initialization scheme, parallel turbo decoding, redundant turbo iterations, size 65 nm, unprecedented energy efficiency, Application specific integrated circuits, Bit error rate, Decoding, Iterative decoding, Long Term Evolution, Throughput, ASIC implementation, CRC, LTE-Advanced, early termination, mobile communications, turbo decoder},
}

@Article{Giard2015,
  author    = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {237~{Gb}it/s Unrolled Hardware Polar Decoder},
  journal   = {IET Electronics Letters},
  year      = {2015},
  volume    = {51},
  number    = {10},
  pages     = {762--763},
  issn      = {0013-5194},
  abstract  = {A new architecture for a polar decoder using a reduced complexity successive-cancellation (SC) decoding algorithm is presented. This novel fully unrolled, deeply pipelined architecture is capable of achieving a coded throughput of over 237 Gbit/s for a (1024, 512) polar code implemented using a field-programmable gate array. This decoder is two orders of magnitude faster than state-of-the-art polar decoders.},
  doi       = {10.1049/el.2014.4432},
  file      = {:pdf/Giard2015 - 237 Gbps Unrolled Hardware Polar Decoder.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {decoding, field programmable gate arrays, FPGA, complexity successive cancellation, decoding algorithm, field programmable gate array, pipelined architecture, polar code, unrolled hardware polar decoder},
  publisher = {IEEE},
}

@InProceedings{Robertson1995,
  author    = {P. Robertson and E. Villebrun and P. Hoeher},
  title     = {A Comparison of Optimal and Sub-Optimal {MAP} Decoding Algorithms Operating in the Log Domain},
  booktitle = {International Conference on Communications (CC)},
  year      = {1995},
  volume    = {2},
  pages     = {1009--1013},
  month     = jun,
  publisher = {IEEE},
  abstract  = {For estimating the states or outputs of a Markov process, the symbol-by-symbol MAP algorithm is optimal. However, this algorithm, even in its recursive form, poses technical difficulties because of numerical representation problems, the necessity of nonlinear functions and a high number of additions and multiplications. MAP like algorithms operating in the logarithmic domain presented in the past solve the numerical problem and reduce the computational complexity, but are suboptimal especially at low SNR (a common example is the max-log-MAP because of its use of the max function). A further simplification yields the soft-output Viterbi algorithm (SOVA). We present a log-MAP algorithm that avoids the approximations in the max-log-MAP algorithm and hence is equivalent to the true MAP, but without its major disadvantages. We compare the (log-)MAP, max-log-MAP and SOVA from a theoretical point of view to illuminate their commonalities and differences. As a practical example forming the basis for simulations, we consider Turbo decoding, where recursive systematic convolutional component codes are decoded with the three algorithms, and we also demonstrate the practical suitability of the log-MAP by including quantization effects. The SOVA is, at 10\textsuperscript{-4}, approximately 0.7 dB inferior to the (log-)MAP, the max-log-MAP lying roughly in between. We also present some complexity comparisons and conclude that the three algorithms increase in complexity in the order of their optimality},
  doi       = {10.1109/ICC.1995.524253},
  file      = {:pdf/Robertson1995 - A Comparison of Optimal and Sub-Optimal MAP Decoding Algorithms Operating in the Log Domain.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {Markov processes, Viterbi decoding, computational complexity, convolutional codes, maximum likelihood decoding, maximum likelihood estimation, quantisation (signal), Markov process, SOVA, Turbo decoding, computational complexity reduction, log domain, log-MAP algorithm, low SNR, max function, max-log-MAP, optimal MAP decoding algorithms, quantization effects, recursive systematic convolutional component codes, simulations, soft-output Viterbi algorithm, suboptimal MAP decoding algorithms, Communications technology, Computational complexity, Convolution, Convolutional codes, Decoding, Markov processes, Quantization, State estimation, Turbo codes, Viterbi algorithm},
}

@InProceedings{Huang2011,
  author    = {L. Huang and Y. Luo and H. Wang and F. Yang and Z. Shi and D. Gu},
  title     = {A High Speed Turbo Decoder Implementation for {CPU}-Based {SDR} System},
  booktitle = {International Conference on Communication Technology and Applications (ICCTA)},
  year      = {2011},
  pages     = {19--23},
  month     = oct,
  publisher = {IEEE},
  abstract  = {More and more CPU-based SDR systems appear in recent two years. Such system requires high speed real-time signal processing. In this paper, we present our effort on the speed optimization of Turbo decoder, the most computation-demanding module in all baseband modules. We jointly consider the algorithm parallelism and the processor architecture. Single Instruction Multiple Data (SIMD) instruction is used for software implementation. The evaluation results show that this proposed design can achieve a maximum of 124 Mbps throughput for single Soft Input Soft Output (SISO) module with Max-Log-MAP algorithm.},
  doi       = {10.1049/cp.2011.0622},
  file      = {:pdf/Huang2011 - A High Speed Turbo Decoder Implementation for CPU-Based SDR System.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {MIMO communication, maximum likelihood estimation, signal processing, software radio, telecommunication computing, turbo codes, CPU-based SDR system, SIMD instruction, baseband module, bit rate 124 Mbit/s, computation-demanding module, high speed real-time signal processing, high speed turbo decoder implementation, max-log-MAP algorithm, processor architecture, single SISO module, single instruction multiple data instruction, single soft input soft output module, software-defined radio, speed optimization, CPU-based SDR, MAP algorithm, SIMD optimization, Turbo decoder},
}

@Article{Shannon1948,
  author   = {C. E. Shannon},
  title    = {A Mathematical Theory of Communication},
  journal  = {The Bell System Technical Journal},
  year     = {1948},
  volume   = {27},
  number   = {4},
  pages    = {623--656},
  month    = oct,
  issn     = {0005-8580},
  abstract = {In this final installment of the paper we consider the case where the signals or the messages or both are continuously variable, in contrast with the discrete nature assumed until now. To a considerable extent the continuous case can be obtained through a limiting process from the discrete case by dividing the continuum of messages and signals into a large but finite number of small regions and calculating the various parameters involved on a discrete basis. As the size of the regions is decreased these parameters in general approach as limits the proper values for the continuous case. There are, however, a few new effects that appear and also a general change of emphasis in the direction of specialization of the general results to particular cases.},
  doi      = {10.1002/j.1538-7305.1948.tb00917.x},
  file     = {:pdf/Shannon1948 - A Mathematical Theory of Communication.pdf:PDF},
}

@InProceedings{Giard2016,
  author    = {P. Giard and A. Balatsoukas-Stimming and T. C. Müller and A. Burg and C. Thibeault and W. J. Gross},
  title     = {A Multi-Gbps Unrolled Hardware List Decoder for a Systematic Polar Code},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2016},
  pages     = {1194--1198},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Polar codes are a new class of block codes with an explicit construction that provably achieve the capacity of various communications channels, even with the low-complexity successive-cancellation (SC) decoding algorithm. Yet, the more complex successive-cancellation list (SCL) decoding algorithm is gathering more attention lately as it significantly improves the error-correction performance of short-to moderate-length polar codes, especially when they are concatenated with a cyclic redundancy check code. However, as SCL decoding explores several decoding paths, existing hardware implementations tend to be significantly slower than SC-based decoders. In this paper, we show how the unrolling technique, which has already been used in the context of SC decoding, can be adapted to SCL decoding yielding a multi-Gbps SCL-based polar decoder with an error-correction performance that is competitive when compared to an LDPC code of similar length and rate. Post-place-and-route ASIC results for 28 nm CMOS are provided showing that this decoder can sustain a throughput greater than 10 Gbps at 468 MHz with an energy efficiency of 7.25 pJ/bit.},
  doi       = {10.1109/ACSSC.2016.7869561},
  file      = {:pdf/Giard2016 - A Multi-Gbps Unrolled Hardware List Decoder for a Systematic Polar Code.pdf:PDF;:pdf/Giard2016a - Multi-Mode Unrolled Architectures for Polar Decoders.pdf:PDF;:pdf/Giard2016b - Low-Latency Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {block codes, channel capacity, channel coding, cyclic redundancy check codes, decoding, error correction codes, CMOS, block codes, communication channel capacity, complex successive-cancellation list decoding algorithm, concatenated code, cyclic redundancy check code, energy efficiency, error-correction performance, frequency 468 MHz, low-complexity successive-cancellation decoding algorithm, multi-Gbps unrolled hardware list decoder, post-place-and-route ASIC, short-moderate-length polar codes, size 28 nm, systematic polar code, unrolling technique, Decoding, Hardware, Measurement, Parity check codes, Standards, Systematics, Throughput},
}

@Article{Box1958,
  author    = {G. E. P. {Box} and M. E. {Muller}},
  title     = {A Note on the Generation of Random Normal Deviates},
  journal   = {The Annals of Mathematical Statistics},
  year      = {1958},
  volume    = {29},
  number    = {2},
  pages     = {610--611},
  doi       = {10.1214/aoms/1177706645},
  file      = {:pdf/Box1958 - A Note on the Generation of Random Normal Deviates.pdf:PDF},
  groups    = {Pseudo-Random Number Generator (PRNG)},
  keywords  = {Box-Muller, Box, Muller},
  publisher = {Institute of Mathematical Statistics},
}

@Article{Wang2014,
  author   = {R. Wang and R. Liu},
  title    = {A Novel Puncturing Scheme for Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2014},
  volume   = {18},
  number   = {12},
  pages    = {2081--2084},
  month    = dec,
  issn     = {1089-7798},
  abstract = {A novel kind of punctured polar codes is proposed in this paper. The codes are constructed by certain constraints on both puncturing patterns and frozen sets such that the values of the punctured bits are known to the decoder. As a result, the initialized log-likelihood ratios of the punctured bits can be set to infinity (or minus infinity), which guarantees a better decoding performance. Furthermore, explicit puncturing patterns are designed for arbitrary code lengths. The simulation results show that the proposed codes outperform the conventional punctured polar codes.},
  doi      = {10.1109/LCOMM.2014.2364845},
  file     = {:pdf/Wang2014 - A Novel Puncturing Scheme for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {codes, arbitrary code lengths, log-likelihood ratios, polar codes, puncturing patterns, puncturing scheme, Bit error rate, Decoding, Error probability, Generators, Channel codes, channel codes, polar codes, length-compatible, polar code construction, puncturing, length-compatible, polar code construction, polar codes, puncturing},
}

@InProceedings{Liu2013,
  author    = {C. Liu and Z. Bie and C. Chen and X. Jiao},
  title     = {A Parallel {LTE} Turbo Decoder on {GPU}},
  booktitle = {International Conference on Communication Technology (ICCT)},
  year      = {2013},
  pages     = {609--614},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Turbo codes were developed by Claude Berrou in 1993, which were based on convolutional codes and Viterbi algorithm and could closely approach the channel capacity. Software Defined Radio (SDR) promises to revolutionize the communication industry by delivering low-cost, flexible software solutions. Graphics Processing Units (GPUs) offer unprecedented application performance since they have many cores. GPUs have been widely used to accelerate a wide range of applications. In this paper, a 3GPP LTE Turbo decoder on GPU is designed, which can offer high throughput. Sub-frame-level parallelism and trellis state-level parallelism have been implemented by some researchers. In this paper, we propose a new method to accelerate our decoder: sub-decoder-level parallelism. In addition, shared memory is utilized to keep memory access fast. Finally, the bit error rate (BER) performance and throughput of the decoder are presented.},
  doi       = {10.1109/ICCT.2013.6820447},
  file      = {:pdf/Liu2013 - A Parallel LTE Turbo Decoder on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {Long Term Evolution, channel capacity, convolutional codes, error statistics, graphics processing units, turbo codes, 3GPP LTE turbo decoder, BER, Claude Berrou, GPU, Long Term Evolution, SDR, Viterbi algorithm, bit error rate, channel capacity, convolutional codes, graphics processing units, memory access, software defined radio, sub-frame-level parallelism, trellis state-level parallelism, turbo codes, Bit error rate, Decoding, Graphics processing units, Instruction sets, Iterative decoding, Parallel processing, Throughput, GPU, Parallel computing, Turbo decoding},
}

@InProceedings{Ying2012,
  author    = {Y. Ying and K. You and L. Zhou and H. Quan and M. Jing and Z. Yu and X. Zeng},
  title     = {A Pure Software {LDPC} Decoder on a Multi-Core Processor Platform with Reduced Inter-Processor Communication Cost},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2012},
  pages     = {2609--2612},
  month     = may,
  publisher = {IEEE},
  abstract  = {As an error correction code, Low Density Parity Check (LDPC) code has been widely used in various communication standards such as WiMAX and DVB-S2. But these continuously-evolving communication standards and the high development cost and low-flexibility of hardwired ASIC solutions have pushed LDPC researchers to turn to more cost-efficient and flexible implementation, and thus the multi-core processor based implementation of LDPC decoder is gaining increasing attention in the last few years. However, the performance of the multi-core processor based implementation is far below the hardwired ASICs, with one of the key reasons that the cost of communication between processors is very high. Three approaches are proposed in this paper to reduce the communication cost, including: optimized algorithm partitioning to reduce communication traffic, utilizing imbalanced communication between tasks to optimize mapping and reduce overall communication distance, and simplified data sending-receiving mechanism to reduce the cost of identifying received data. By using these approaches, the communication time of the proposed implementation of LDPC decoder only accounts for 12.2\% of total decoding time, which generally occupies 50\% decoding time in the previously reported LDPC decoders on multi-core processors. And our work can achieve better throughput performance under the same hardware condition compared with other state-of-the-art works.},
  doi       = {10.1109/ISCAS.2012.6271839},
  file      = {:pdf/Ying2012 - A Pure Software LDPC Decoder on a Multi-Core Processor Platform with Reduced Inter-Processor Communication Cost.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, DVB},
  issn      = {0271-4302},
  keywords  = {WiMax, application specific integrated circuits, digital video broadcasting, error correction codes, multiprocessing systems, parity check codes, telecommunication standards, telecommunication traffic, DVB-S2, WiMAX, communication standards, communication traffic, data sending-receiving mechanism, error correction code, hardwired ASIC, inter-processor communication cost, low density parity check code, multicore processor platform, optimized algorithm partitioning, software LDPC decoder, Algorithm design and analysis, Decoding, Message passing, Multicore processing, Parity check codes, Partitioning algorithms, Program processors},
}

@InProceedings{Lin2014,
  author    = {J. Lin and C. Xiong and Z. Yan},
  title     = {A Reduced Latency List Decoding Algorithm for Polar Codes},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the successive cancelation (SC) decoding algorithm for short or moderate polar codes. However, the CRC aided SCL (CA-SCL) decoding algorithm still suffer from long decoding latency. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. For the proposed RLLD algorithm, all rate-0 nodes and part of rate-1 nodes are decoded instantly without traversing the corresponding subtree. A list maximum-likelihood decoding (LMLD) algorithm is proposed to decode the maximum likelihood (ML) nodes and the remaining rate-1 nodes. Moreover, a simplified LMLD (SLMLD) algorithm is also proposed to reduce the computational complexity of the LMLD algorithm. Suppose a partial parallel list decoder architecture with list size L = 4 is used, for an (8192, 4096) polar code, the proposed RLLD algorithm can reduce the number of decoding clock cycles and decoding latency by 6.97 and 6.77 times, respectively.},
  doi       = {10.1109/SiPS.2014.6986062},
  file      = {:pdf/Lin2014 - A Reduced Latency List Decoding Algorithm for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2162-3562},
  keywords  = {cyclic codes, cyclic redundancy check codes, decoding, error correction codes, CRC, LMLD algorithm, RLLD algorithm, SCL decoding algorithm, computational complexity, cyclic redundancy check, decoding clock cycles, decoding latency, list maximum-likelihood decoding, polar codes, reduced latency list decoding algorithm, successive cancellation list, Government},
}

@Article{Raymond2014,
  author   = {A. J. Raymond and W. J. Gross},
  title    = {A Scalable Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2014},
  volume   = {62},
  number   = {20},
  pages    = {5339--5347},
  month    = oct,
  issn     = {1053-587X},
  abstract = {Polar codes are the first error-correcting codes to provably achieve channel capacity, asymptotically in code length, with an explicit construction. However, under successive-cancellation decoding, polar codes require very long code lengths to compete with existing modern codes. Nonetheless, the successive cancellation algorithm enables very-low-complexity implementations in hardware, due to the regular structure exhibited by polar codes. In this paper, we present an improved architecture for successive-cancellation decoding of polar codes, making use of a novel semi-parallel, encoder-based partial-sum computation module. We also provide quantization results for realistic code length N=2\textsuperscript{15}, and explore various optimization techniques such as a chained processing element and a variable quantization scheme. This design is shown to scale to code lengths of up to N=2\textsuperscript{21}, enabled by its low logic use, low register use and simple datapaths, limited almost exclusively by the amount of available SRAM. It also supports an overlapped loading of frames, allowing full-throughput decoding with a single set of input buffers.},
  doi      = {10.1109/TSP.2014.2347262},
  file     = {:pdf/Raymond2014 - A Scalable Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {channel capacity, decoding, error correction codes, optimisation, Polar codes, SRAM, channel capacity, error-correcting codes, full-throughput decoding, novel semi-parallel encoder-based partial-sum computation module, optimization techniques, scalable successive-cancellation decoder algorithm, variable quantization scheme, very-low-complexity implementations, Clocks, Computer architecture, Decoding, Indexes, Quantization (signal), Random access memory, Vectors, Error-correcting codes, hardware implementation, polar codes, successive-cancellation decoding},
}

@Article{Leroux2013,
  author   = {C. Leroux and A. J. Raymond and G. Sarkis and W. J. Gross},
  title    = {A Semi-Parallel Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2013},
  volume   = {61},
  number   = {2},
  pages    = {289--299},
  month    = jan,
  issn     = {1053-587X},
  abstract = {Polar codes are a recently discovered family of capacity-achieving codes that are seen as a major breakthrough in coding theory. Motivated by the recent rapid progress in the theory of polar codes, we propose a semi-parallel architecture for the implementation of successive cancellation decoding. We take advantage of the recursive structure of polar codes to make efficient use of processing resources. The derived architecture has a very low processing complexity while the memory complexity remains similar to that of previous architectures. This drastic reduction in processing complexity allows very large polar code decoders to be implemented in hardware. An N=2\textsuperscript{17} polar code successive cancellation decoder is implemented in an FPGA. We also report synthesis results for ASIC.},
  doi      = {10.1109/TSP.2012.2223693},
  file     = {:pdf/Leroux2013 - A Semi-Parallel Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {application specific integrated circuits, codes, decoding, field programmable gate arrays, ASIC, FPGA, capacity-achieving codes, coding theory, polar codes, semi-parallel architecture, semi-parallel successive-cancellation decoder, successive cancellation decoding, very low processing complexity, Application specific integrated circuits, Complexity theory, Computer architecture, Decoding, Field programmable gate arrays, Hardware, Vectors, Codes, FPGA, VLSI, decoding, polar codes, successive cancellation},
}

@Article{Alamdar-Yazdi2011,
  author   = {A. Alamdar-Yazdi and F. R. Kschischang},
  title    = {A Simplified Successive-Cancellation Decoder for Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2011},
  volume   = {15},
  number   = {12},
  pages    = {1378--1380},
  month    = dec,
  issn     = {1089-7798},
  abstract = {A modification is introduced of the successive-cancellation decoder for polar codes, in which local decoders for rate-one constituent codes are simplified. This modification reduces the decoding latency and algorithmic complexity of the conventional decoder, while preserving the bit and block error rate. Significant latency and complexity reductions are achieved over a wide range of code rates.},
  doi      = {10.1109/LCOMM.2011.101811.111480},
  file     = {:pdf/Alamdar-Yazdi2011 - A Simplified Successive-Cancellation Decoder for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {channel coding, error statistics, parity check codes, source coding, bit error rate, block error rate, constituent codes, local decoders, polar codes, successive cancellation decoder, Complexity theory, Decoding, Echo cancellers, Generators, Source coding, Vectors, Polar codes, successive-cancellation decoding, nodes, node, rate 1, rate one, rate 0, rate zero},
}

@InProceedings{Mishra2012,
  author    = {A. Mishra and A. J. Raymond and L. G. Amaru and G. Sarkis and C. Leroux and P. Meinerzhagen and A. Burg and W. J. Gross},
  title     = {A Successive Cancellation Decoder {ASIC} for a 1024\mbox{-}bit Polar Code in 180{nm} {CMOS}},
  booktitle = {Asian Solid-State Circuits Conference (A-SSCC)},
  year      = {2012},
  pages     = {205--208},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper presents the first ASIC implementation of a successive cancellation (SC) decoder for polar codes. The implemented ASIC relies on a semi-parallel architecture where processing resources are reused to achieve good hardware efficiency. A speculative decoding technique is employed to increase the throughput by 25\% at the cost of very limited added complexity. The resulting architecture is implemented in a 180nm technology. The fabricated chip can be clocked at 150 MHz and uses 183k gates. It was verified using an FPGA testing setup and provides reference for the true silicon complexity of SC decoders for polar codes.},
  doi       = {10.1109/IPEC.2012.6522661},
  file      = {:pdf/Mishra2012 - A Successive Cancellation Decoder ASIC for a 1024-bit Polar Code in 180nm CMOS.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {CMOS integrated circuits, application specific integrated circuits, codecs, decoding, field programmable gate arrays, ASIC implementation, CMOS, FPGA testing setup, polar code, silicon complexity, size 180 nm, speculative decoding technique, successive cancellation decoder},
}

@Article{Whaley2008,
  author    = {R. C. Whaley and A. M. Castaldo},
  title     = {Achieving Accurate and Context-Sensitive Timing for Code Optimization},
  journal   = {Software: Practice and Experience},
  year      = {2008},
  volume    = {38},
  number    = {15},
  pages     = {1621--1642},
  file      = {:pdf/Whaley2008 - Achieving Accurate and Context-Sensitive Timing for Code Optimization.pdf:PDF},
  keywords  = {atlas, methodology, accurate, optimization, timing, autotuning},
  publisher = {Wiley Online Library},
}

@Article{Li2012,
  author   = {B. Li and H. Shen and D. Tse},
  title    = {An Adaptive Successive Cancellation List Decoder for Polar Codes with Cyclic Redundancy Check},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2012},
  volume   = {16},
  number   = {12},
  pages    = {2044--2047},
  month    = dec,
  issn     = {1089-7798},
  abstract = {In this letter, we propose an adaptive SC (Successive Cancellation)-List decoder for polar codes with CRC. This adaptive SC-List decoder iteratively increases the list size until at least one survival path can pass CRC. Simulation shows that the adaptive SC-List decoder provides significant complexity reduction. We also demonstrate that polar code (2048, 1024) with 24-bit CRC decoded by our proposed adaptive SC-List decoder with very large maximum list size can achieve a frame error rate FER $\leq$ 10\textsuperscript{-3}{-3} at E\textsubscript{b}/N\textsubscript{o} = 1.1dB, which is about 0.25dB from the information theoretic limit at this block length.},
  doi      = {10.1109/LCOMM.2012.111612.121898},
  file     = {:pdf/Li2012 - An Adaptive Successive Cancellation List Decoder for Polar Codes with Cyclic Redundancy Check.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {adaptive codes, cyclic redundancy check codes, decoding, CRC code, FER, adaptive SC list decoder, adaptive successive cancellation list decoder, cyclic redundancy check codes, frame error rate, information theoretic limit, polar codes, word length 24 bit, Complexity theory, Cyclic redundancy check, Error analysis, Iterative decoding, Maximum likelihood decoding, Signal to noise ratio, Polar codes, list decoder},
}

@Article{Li2014,
  author    = {R. Li and Y. Dou and J. Xu and X. Niu and S. Ni},
  title     = {An Efficient Parallel {SOVA}-based Turbo Decoder for Software Defined Radio on {GPU}},
  journal   = {IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
  year      = {2014},
  volume    = {97},
  number    = {5},
  pages     = {1027--1036},
  doi       = {10.1587/transfun.E97.A.1027},
  file      = {:pdf/Li2014 - An Efficient Parallel SOVA-based Turbo Decoder for Software Defined Radio on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  publisher = {The Institute of Electronics, Information and Communication Engineers},
}

@InProceedings{Cassagne2015c,
  author    = {A. Cassagne and B. {Le Gal} and C. Leroux and O. Aumage and D. Barthou},
  title     = {An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes},
  booktitle = {International Workshop on Languages and Compilers for Parallel Computing (LCPC)},
  year      = {2015},
  month     = sep,
  publisher = {Springer},
  abstract  = {Error Correction Code decoding algorithms for consumer products such as Internet of Things (IoT) devices are usually implemented as dedicated hardware circuits. As processors are becoming increasingly powerful and energy efficient, there is now a strong desire to perform this processing in software to reduce production costs and time to market. The recently introduced family of Successive Cancellation decoders for Polar codes has been shown in several research works to efficiently leverage the ubiquitous SIMD units in modern CPUs, while offering strong potentials for a wide range of optimizations. The P-EDGE environment introduced in this paper, combines a specialized skeleton generator and a building blocks library routines to provide a generic, extensible Polar code exploration workbench. It enables ECC code designers to easily experiments with combinations of existing and new optimizations, while delivering performance close to state-of-art decoders.},
  date      = {2015-11-01},
  doi       = {10.1007/978-3-319-29778-1_19},
  file      = {:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes.pdf:PDF;:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes [slides].pdf:PDF;:pdf/Cassagne2015c - An Efficient, Portable and Generic Library for Successive Cancellation Decoding of Polar Codes [poster].pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  journal   = {Languages and Compilers for Parallel Computing},
  keywords  = {Cassagne},
}

@InProceedings{Sarkis2014,
  author    = {G. Sarkis and P. Giard and C. Thibeault and W. J. Gross},
  title     = {Autogenerating Software Polar Decoders},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2014},
  pages     = {6--10},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Polar decoders are well suited for high-speed software implementations. In this work, we present a framework for generating fully-unrolled software polar decoders with branchless data flow. We discuss the memory layout of data in these decoders and show the optimization techniques used. At 335 Mbps, when decoding a (2048, 1707) polar code, the resulting decoder has more than twice the speed of the state of the art floating-point software polar decoder.},
  doi       = {10.1109/GlobalSIP.2014.7032067},
  file      = {:pdf/Sarkis2014 - Autogenerating Software Polar Decoders.pdf:PDF;:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation.pdf:PDF;:pdf/Sarkis2014b - Increasing the Speed of Polar List Decoders.pdf:PDF;:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation2.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  keywords  = {data flow computing, decoding, program compilers, source code (software), branchless data flow, data memory layout, floating-point software polar decoder, fully-unrolled software polar decoders, optimization techniques, software polar decoder autogeneration, Computer architecture, Hardware, Maximum likelihood decoding, Signal processing algorithms, Software, Throughput, decoder, polar codes, software},
}

@Article{Whaley2001,
  author    = {R. C. Whaley and A. Petitet and J. J. Dongarra},
  title     = {Automated Empirical Optimizations of Software and the {ATLAS} Project},
  journal   = {Elsevier Parallel Computing Journal},
  year      = {2001},
  volume    = {27},
  number    = {1},
  pages     = {3--35},
  doi       = {10.1016/S0167-8191(00)00087-9},
  file      = {:pdf/Whaley2001 - Automated Empirical Optimizations of Software and the ATLAS Project.pdf:PDF},
  publisher = {Elsevier},
}

@InProceedings{Jaeger2012,
  author    = {J. Jaeger and D. Barthou},
  title     = {Automatic Efficient Data Layout for Multithreaded Stencil Codes on {CPUs} and {GPUs}},
  booktitle = {International Conference on High Performance Computing (HiPC)},
  year      = {2012},
  pages     = {1--10},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Stencil based computation on structured grids is a kernel at the heart of a large number of scientific applications. The variety of stencil kernels used in practice make this computation pattern difficult to assemble into a high performance computing library. With the multiplication of cores on a single chip, answering architectural alignment requirements became an even more important key to high performance. Along with vector accesses, data layout optimization must also consider concurrent parallel accesses. In this paper, we develop a strategy to automatically generate stencil codes for multicore vector architectures, searching for the best data layout possible to answer architectural alignment problems. We introduce a new method for aligning multidimensional data structures, called multipadding, that can be adapted to specificities of multicores and GPUs architectures. We present multiple methods with different level of complexity. We show on different stencil patterns that generated codes with multipadding display better performance than existing optimizations.},
  doi       = {10.1109/HiPC.2012.6507504},
  file      = {:pdf/Jaeger2012 - Automatic Efficient Data Layout for Multithreaded Stencil Codes on CPUs and GPUs.pdf:PDF},
  keywords  = {data structures, graphics processing units, grid computing, multi-threading, parallel processing, software libraries, CPU, GPU architectures, architectural alignment problems, architectural alignment requirements, automatic efficient data layout, computation pattern, concurrent parallel accesses, core multiplication, data layout optimization, high performance computing library, multicore vector architectures, multicores, multidimensional data structures, multipadding, multithreaded stencil codes, scientific applications, single chip, stencil based computation, stencil kernels, structured grids},
}

@Article{Wubben2014,
  author   = {D. Wubben and P. Rost and J. S. Bartelt and M. Lalam and V. Savin and M. Gorgoglione and A. Dekorsy and G. Fettweis},
  title    = {Benefits and Impact of Cloud Computing on {5G} Signal Processing: Flexible Centralization Through Cloud-{RAN}},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2014},
  volume   = {31},
  number   = {6},
  pages    = {35--44},
  month    = nov,
  issn     = {1053-5888},
  abstract = {Cloud computing draws significant attention in the information technology (IT) community as it provides ubiquitous on-demand access to a shared pool of configurable computing resources with minimum management effort. It gains also more impact on the communication technology (CT) community and is currently discussed as an enabler for flexible, cost-efficient and more powerful mobile network implementations. Although centralized baseband pools are already investigated for the radio access network (RAN) to allow for efficient resource usage and advanced multicell algorithms, these technologies still require dedicated hardware and do not offer the same characteristics as cloud-computing platforms, i.e., on-demand provisioning, virtualization, resource pooling, elasticity, service metering, and multitenancy. However, these properties of cloud computing are key enablers for future mobile communication systems characterized by an ultradense deployment of radio access points (RAPs) leading to severe multicell interference in combination with a significant increase of the number of access nodes and huge fluctuations of the rate requirements over time. In this article, we will explore the benefits that cloud computing offers for fifth-generation (5G) mobile networks and investigate the implications on the signal processing algorithms.},
  doi      = {10.1109/MSP.2014.2334952},
  file     = {:pdf/Wubben2014 - Benefits and Impact of Cloud Computing on 5G Signal Processing\: Flexible Centralization Through Cloud-RAN.pdf:PDF},
  groups   = {5G},
  keywords = {4G mobile communication, cloud computing, information technology, mobile computing, radio access networks, radiofrequency interference, signal processing, 5G signal processing, RAP, access nodes, cloud computing, cloud-RAN, communication technology, information technology, multicell interference, radio access network, radio access points, resource pooling, service metering, 5G mobile communication, Cellular networks, Cloud computing, Decoding, Mobile communication, Mobile computing, Next generation networking, Radio access networks, Signal processing algorithms, Wireless cellular networks},
}

@InProceedings{Chen2013,
  author    = {Xiang Chen and Ji Zhu and Ziyu Wen and Yu Wang and Huazhong Yang},
  title     = {{BER} Guaranteed Optimization and Implementation of Parallel Turbo Decoding on {GPU}},
  booktitle = {International Conference on Communications and Networking in China (CHINACOM)},
  year      = {2013},
  pages     = {183--188},
  month     = aug,
  publisher = {IEEE},
  abstract  = {In this this paper, we present an optimized parallel implementation of a Bit Error Rate (BER) guaranteed turbo decoder on a General Purpose Graphic Process Unit (GPGPU). Actually, it is a critical task to implement complex communication signal processing over GPGPUs, since the parallelism over GPGPUs in general requires independent data streams for processing. So we explore both the inherent parallelisms and the extended sub-frame level parallelisms in turbo decoding and map them onto the recent GPU architecture. A guarding mechanism called Previous Iteration Value Initialization with Double Sided Training Window (PIVIDSTW) is used to minimize the loss of BER performance caused by sub-frame level parallelism, while the high throughput is still maintained. In addition, to explore the potential of parallelization in Turbo decoding on GPUs, the theoretical occupancy and scalability are analyzed with the consideration of the number of sub-frames per frame. Compared with previous work in [5] and [7], we achieve a better trade-off between BER performance and throughput concerns.},
  doi       = {10.1109/ChinaCom.2013.6694588},
  file      = {:pdf/Chen2013 - BER Guaranteed Optimization and Implementation of Parallel Turbo Decoding on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {error statistics, graphics processing units, turbo codes, BER guaranteed optimization, BER performance, GPGPU, GPU, GPU architecture, PIVIDSTW, bit error rate, complex communication signal processing, data streams, extended subframe level parallelisms, general purpose graphic process unit, parallel turbo decoding, parallelization, previous iteration value initialization with double sided training window, sub-frame level parallelism, theoretical occupancy, theoretical scalability, Bit error rate, Decoding, Graphics processing units, Instruction sets, Measurement, Parallel processing, Throughput},
}

@InProceedings{Niu2013,
  author    = {K. Niu and K. Chen and J. R. Lin},
  title     = {Beyond Turbo Codes: Rate-Compatible Punctured Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2013},
  pages     = {3423--3427},
  month     = jun,
  publisher = {IEEE},
  abstract  = {CRC (cyclic redundancy check) concatenated polar codes are superior to the turbo codes under the successive cancellation list (SCL) or successive cancellation stack (SCS) decoding algorithms. But the code length of polar codes is limited to the power of two. In this paper, a family of rate-compatible punctured polar (RCPP) codes is proposed to satisfy the construction with arbitrary code length. We propose a simple quasi-uniform puncturing algorithm to generate the puncturing table. And we prove that this method has better row-weight property than that of the random puncturing. Simulation results under the binary input additive white Gaussian noise channels (BI-AWGNs) show that these RCPP codes outperform the performance of turbo codes in WCDMA (Wideband Code Division Multiple Access) or LTE (Long Term Evolution) wireless communication systems in the large range of code lengths. Especially, the RCPP code with CRC-aided SCL/SCS algorithm can provide over 0.7dB performance gain at the block error rate (BLER) of 10\textsuperscript{-4} with short code length M = 512 and code rate R = 0.5.},
  doi       = {10.1109/ICC.2013.6655078},
  file      = {:pdf/Niu2013 - Beyond Turbo Codes\: Rate-Compatible Punctured Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1550-3607},
  keywords  = {AWGN channels, concatenated codes, decoding, error statistics, turbo codes, BI-AWGN, BLER, CRC concatenated codes, LTE, Long Term Evolution, RCPP codes, SCL decoding algorithms, SCS decoding algorithms, WCDMA, arbitrary code length, binary input additive white Gaussian noise channels, block error rate, cyclic redundancy check codes, puncturing table, quasi-uniform puncturing algorithm, random puncturing, rate-compatible punctured polar codes, row-weight property, successive cancellation list decoding algorithms, successive cancellation stack decoding algorithms, turbo codes, wideband code division multiple access, wireless communication systems, Decoding, Generators, Multiaccess communication, Spread spectrum communication, Turbo codes, Vectors},
}

@Article{Arikan2009,
  author   = {E. Arikan},
  title    = {Channel Polarization: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2009},
  volume   = {55},
  number   = {7},
  pages    = {3051--3073},
  month    = jul,
  issn     = {0018-9448},
  abstract = {A method is proposed, called channel polarization, to construct code sequences that achieve the symmetric capacity I(W) of any given binary-input discrete memoryless channel (B-DMC) W. The symmetric capacity is the highest rate achievable subject to using the input letters of the channel with equal probability. Channel polarization refers to the fact that it is possible to synthesize, out of N independent copies of a given B-DMC W, a second set of N binary-input channels {WN(i)1 les i les N} such that, as N becomes large, the fraction of indices i for which I(WN(i)) is near 1 approaches I(W) and the fraction for which I(WN(i)) is near 0 approaches 1-I(W). The polarized channels {WN(i)} are well-conditioned for channel coding: one need only send data at rate 1 through those with capacity near 1 and at rate 0 through the remaining. Codes constructed on the basis of this idea are called polar codes. The paper proves that, given any B-DMC W with I(W) $>$ 0 and any target rate R$<$ I(W) there exists a sequence of polar codes {Cfrn;nges1} such that Cfrn has block-length N=2n , rate ges R, and probability of block error under successive cancellation decoding bounded as Pe(N,R) les O(N-1/4) independently of the code rate. This performance is achievable by encoders and decoders with complexity O(N logN) for each.},
  doi      = {10.1109/TIT.2009.2021379},
  file     = {:pdf/Arikan2009 - Channel Polarization\: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {binary codes, channel capacity, channel coding, decoding, memoryless systems, probability, channel capacity, channel coding, channel polarization, code sequence, polar codes, probability, successive cancellation decoding algorithm, symmetric binary-input memoryless channel, Capacity planning, Channel capacity, Channel coding, Codes, Councils, Decoding, Information theory, Memoryless systems, Noise cancellation, Polarization, Capacity-achieving codes, Plotkin construction, Reed-- Muller (RM) codes, channel capacity, channel polarization, polar codes, successive cancellation decoding},
}

@Article{Chase1972,
  author   = {D. Chase},
  title    = {Class of Algorithms for Decoding Block Codes with Channel Measurement Information},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1972},
  volume   = {18},
  number   = {1},
  pages    = {170--182},
  month    = jan,
  issn     = {0018-9448},
  abstract = {A class of decoding algorithms that utilizes channel measurement information, in addition to the conventional use of the algebraic properties of the code, is presented. The maximum number of errors that can, with high probability, be corrected is equal to one less thand, the minimum Hamming distance of the code. This two-fold increase over the error-correcting capability of a conventional binary decoder is achieved by using channel measurement (soft-decision) information to provide a measure of the relative reliability of each of the received binary digits. An upper bound on these decoding algorithms is derived, which is proportional to the probability of an error fordth order diversity, an expression that has been evaluated for a wide range of communication channels and modulation techniques. With the aid of a lower bound on these algorithms, which is also a lower bound on a correlation (maximum-likelihood) decoder, we show for both the Gaussian and Rayleigh fading channels, that as the signal-to-noise ratio (SNR) increases, the asymptotic behavior of these decoding algorithms cannot be improved. Computer simulations indicate that even for !ow SNR the performance of a correlation decoder can be approached by relatively simple decoding procedures. In addition, we study the effect on the performance of these decoding algorithms when a threshold is used to simplify the decoding process.},
  doi      = {10.1109/TIT.1972.1054746},
  file     = {:pdf/Chase1972 - Class of Algorithms for Decoding Block Codes with Channel Measurement Information.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {Block codes, Decoding},
}

@Article{Rost2014,
  author   = {P. Rost and C. J. Bernardos and A. D. Domenico and M. D. Girolamo and M. Lalam and A. Maeder and D. Sabella and D. Wübben},
  title    = {Cloud Technologies for Flexible {5G} Radio Access Networks},
  journal  = {IEEE Communications Magazine},
  year     = {2014},
  volume   = {52},
  number   = {5},
  pages    = {68--76},
  month    = may,
  issn     = {0163-6804},
  abstract = {The evolution toward 5G mobile networks will be characterized by an increasing number of wireless devices, increasing device and service complexity, and the requirement to access mobile services ubiquitously. Two key enablers will allow the realization of the vision of 5G: very dense deployments and centralized processing. This article discusses the challenges and requirements in the design of 5G mobile networks based on these two key enablers. It discusses how cloud technologies and flexible functionality assignment in radio access networks enable network densification and centralized operation of the radio access network over heterogeneous backhaul networks. The article describes the fundamental concepts, shows how to evolve the 3GPP LTE a},
  doi      = {10.1109/MCOM.2014.6898939},
  file     = {:pdf/Rost2014 - Cloud Technologies for Flexible 5G Radio Access Networks.pdf:PDF},
  groups   = {5G, Cloud-RAN},
  keywords = {Algorithm design and analysis, Computer architecture, Long Term Evolution, Mobile communication, Mobile computing, Protocols, Radio access networks},
}

@Article{Zhang2017,
  author   = {Q. Zhang and A. Liu and X. Pan and K. Pan},
  title    = {{CRC} Code Design for List Decoding of Polar Codes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {6},
  pages    = {1229--1232},
  month    = jun,
  issn     = {1089-7798},
  abstract = {Cyclic redundancy check (CRC) assisted list successive cancellation decoding (SCLD) makes polar codes competitive with the state-of-art codes. In this letter, we try to find the optimal CRC for polar codes to further improve its performance. We first analyze the undetected error probability of CRC-aided SCLD as well as the characteristics of Hamming weight distribution of polar codes. Based on these characteristics, a multilevel SCLD-based searching strategy with moderate list size is proposed to compute the minimum Hamming weight distribution (MHWD) of different CRC-concatenated polar codes. Using the searched MHWD, the optimal CRC for polar codes are presented in this letter. Simulation results show that the performance of optimal CRC-aided SCLD significantly outperforms the standard one, especially at high code rate.},
  doi      = {10.1109/LCOMM.2017.2672539},
  file     = {:pdf/Zhang2017 - CRC Code Design for List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Hamming codes, concatenated codes, cyclic redundancy check codes, decoding, probability, search problems, CRC code design, CRC-concatenated polar code, MHWD, cyclic redundancy check code design, list successive cancellation decoding, minimum Hamming weight distribution, multilevel SCLD-based searching strategy, undetected error probability, Cyclic redundancy check codes, Decoding, Error probability, Generators, Hamming weight, Standards, System performance, Polar codes, cyclic redundancy check, list successive cancellation decoder},
}

@Article{Studer2011,
  author   = {C. Studer and C. Benkeser and S. Belfanti and Q. Huang},
  title    = {Design and Implementation of a Parallel Turbo-Decoder {ASIC} for {3GPP}-{LTE}},
  journal  = {IEEE Journal of Solid-State Circuits (JSSC)},
  year     = {2011},
  volume   = {46},
  number   = {1},
  pages    = {8--17},
  month    = jan,
  issn     = {0018-9200},
  abstract = {Turbo-decoding for the 3GPP-LTE (Long Term Evolution) wireless communication standard is among the most challenging tasks in terms of computational complexity and power consumption of corresponding cellular devices. This paper addresses design and implementation aspects of parallel turbo-decoders that reach the 326.4 Mb/s LTE peak data-rate using multiple soft-input soft-output decoders that operate in parallel. To highlight the effectiveness of our design-approach, we realized a 3.57 mm\textsuperscript{2} radix-4based 8$\times$ parallel turbo-decoder ASIC in 0.13 $\mu$m CMOS technology achieving 390 Mb/s. At the more realistic 100 Mb/s LTE milestone targeted by industry today, the turbo-decoder consumes only 69 mW.},
  doi      = {10.1109/JSSC.2010.2075390},
  file     = {:pdf/Studer2011 - Design and Implementation of a Parallel Turbo-Decoder ASIC for 3GPP-LTE.pdf:PDF},
  groups   = {Turbo Codes, 4G},
  keywords = {3G mobile communication, CMOS integrated circuits, Long Term Evolution, application specific integrated circuits, cellular radio, communication complexity, parallel processing, turbo codes, 3GPP-LTE, CMOS technology, LTE wireless communication, Long Term Evolution, bit rate 390 Mbit/s, cellular device, computational complexity, parallel turbo-decoder ASIC, power 69 mW, power consumption, size 0.13 mum, soft-input soft-output decoder, turbo decoding, Decoding, Memory management, Multiaccess communication, Random access memory, Systematics, Throughput, 3G mobile communication, ASIC implementation, LTE, low-power, parallel turbo-decoder, radix-4},
}

@Article{Trifonov2012,
  author   = {P. Trifonov},
  title    = {Efficient Design and Decoding of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2012},
  volume   = {60},
  number   = {11},
  pages    = {3221--3227},
  month    = nov,
  issn     = {0090-6778},
  abstract = {Polar codes are shown to be instances of both generalized concatenated codes and multilevel codes. It is shown that the performance of a polar code can be improved by representing it as a multilevel code and applying the multistage decoding algorithm with maximum likelihood decoding of outer codes. Additional performance improvement is obtained by replacing polar outer codes with other ones with better error correction performance. In some cases this also results in complexity reduction. It is shown that Gaussian approximation for density evolution enables one to accurately predict the performance of polar codes and concatenated codes based on them.},
  doi      = {10.1109/TCOMM.2012.081512.110872},
  file     = {:pdf/Trifonov2012 - Efficient Design and Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Gaussian processes, concatenated codes, design, maximum likelihood decoding, Gaussian approximation, concatenated codes, density evolution, design, maximum likelihood decoding, multilevel codes, multistage decoding algorithm, polar codes, Approximation algorithms, Concatenated codes, Constellation diagram, Error probability, Maximum likelihood decoding, Vectors, Polar codes, concatenated codes, multilevel codes},
}

@Article{Hashemi2017,
  author   = {S. A. Hashemi and C. Condo and W. J. Gross},
  title    = {Fast and Flexible Successive-Cancellation List Decoders for Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2017},
  volume   = {65},
  number   = {21},
  pages    = {5756--5769},
  month    = nov,
  issn     = {1053-587X},
  abstract = {Polar codes have gained significant amount of attention during the past few years and have been selected as a coding scheme for the next generation of mobile broadband standard. Among decoding schemes, successive-cancellation list (SCL) decoding provides a reasonable tradeoff between the error-correction performance and hardware implementation complexity when used to decode polar codes, at the cost of limited throughput. The simplified SCL (SSCL) and its extension SSCL-SPC increase the speed of decoding by removing redundant calculations when encountering particular information and frozen bit patterns (rate one and single parity check codes), while keeping the error-correction performance unaltered. In this paper, we improve SSCL and SSCL-SPC by proving that the list size imposes a specific number of path splitting required to decode rate one and single parity check codes. Thus, the number of splitting can be limited while guaranteeing exactly the same error-correction performance as if the paths were forked at each bit estimation. We call the new decoding algorithms Fast-SSCL and Fast-SSCL-SPC. Moreover, we show that the number of path forks in a practical application can be tuned to achieve desirable speed, while keeping the error-correction performance almost unchanged. Hardware architectures implementing both algorithms are then described and implemented: It is shown that our design can achieve $\mathbf {1.86}$ Gb/s throughput, higher than the best state-of-the-art decoders.},
  doi      = {10.1109/TSP.2017.2740204},
  file     = {:pdf/Hashemi2017 - Fast and Flexible Successive-Cancellation List Decoders for Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {block codes, broadband networks, decoding, error correction codes, linear codes, mobile communication, next generation networks, parity check codes, SCL decoding schemes, bit estimation, bit rate 1.86 Gbit/s, coding scheme, error correction performance, fast successive-cancellation list decoder, fast-SSCL, fast-SSCL-SPC, flexible successive-cancellation list decoder, frozen bit patterns, hardware implementation complexity, next generation mobile broadband standard, path splitting, polar code, simplified SCL, single parity check codes, Algorithm design and analysis, Estimation, Hardware, Maximum likelihood decoding, Signal processing algorithms, Throughput, Polar codes, hardware implementation, list decoding, successive-cancellation decoding},
}

@Article{Sarkis2016,
  author   = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Fast List Decoders for Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2016},
  volume   = {34},
  number   = {2},
  pages    = {318--328},
  month    = feb,
  issn     = {0733-8716},
  abstract = {Polar codes asymptotically achieve the symmetric capacity of memoryless channels, yet their error-correcting performance under successive-cancellation (SC) decoding for short and moderate length codes is worse than that of other modern codes such as low-density parity-check (LDPC) codes. Of the many methods to improve the error-correction performance of polar codes, list decoding yields the best results, especially when the polar code is concatenated with a cyclic redundancy check (CRC). List decoding involves exploring several decoding paths with SC decoding, and therefore tends to be slower than SC decoding itself, by an order of magnitude in practical implementations. In this paper, we present a new algorithm based on unrolling the decoding tree of the code that improves the speed of list decoding by an order of magnitude when implemented in software. Furthermore, we show that for software-defined radio applications, our proposed algorithm is faster than the fastest software implementations of LDPC decoders in the literature while offering comparable error-correction performance at similar or shorter code lengths.},
  doi      = {10.1109/JSAC.2015.2504299},
  file     = {:pdf/Sarkis2016 - Fast List Decoders for Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, HoF Polar - SCL},
  keywords = {cyclic redundancy check codes, parity check codes, software radio, LDPC codes, cyclic redundancy check, fast list decoders, low-density parity-check codes, memoryless channels, polar codes, software-defined radio, successive-cancellation decoding, symmetric capacity, Complexity theory, Decoding, Hardware, Parity check codes, Reliability, Software, Software algorithms, LDPC, list decoding, polar codes, software decoders, software-defined radio},
}

@Article{Sarkis2014a,
  author   = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Fast Polar Decoders: Algorithm and Implementation},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2014},
  volume   = {32},
  number   = {5},
  pages    = {946--957},
  month    = may,
  issn     = {0733-8716},
  abstract = {Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. The adoption of polar codes however, has been hampered by the low throughput of their decoding algorithm. This work aims to increase the throughput of polar decoding hardware by an order of magnitude relative to successive-cancellation decoders and is more than 8 times faster than the current fastest polar decoder. We present an algorithm, architecture, and FPGA implementation of a flexible, gigabit-per-second polar decoder.},
  doi      = {10.1109/JSAC.2014.140514},
  file     = {:pdf/Sarkis2014a - Fast Polar Decoders\: Algorithm and Implementation.pdf:PDF;},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {block codes, decoding, error correction codes, field programmable gate arrays, linear codes, FPGA implementation, fast polar decoders, flexible polar decoder, gigabit-per-second polar decoder, polar codes, polar decoding hardware throughput, successive-cancellation decoders, symmetric memoryless channel capacity, Complexity theory, Maximum likelihood decoding, Parity check codes, Reliability, Systematics, Throughput, polar codes, storage systems, successive-cancellation decoding, node, nodes, spc, rep},
}

@InProceedings{Chinnici2012,
  author    = {S. Chinnici and P. Spallaccini},
  title     = {Fast Simulation of Turbo Codes on {GPU}s},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2012},
  pages     = {61--65},
  month     = aug,
  publisher = {IEEE},
  abstract  = {Simulation of turbo codes with moderately long block lengths down to bit error rates of the order of 10\textsuperscript{-9} requires long runtimes on conventional CPUs. The approach described in this paper is based on a CPU/GPU co-processing strategy which aims at effectively distributing the processing tasks between the two platforms. In this paper, the most computationally intensive parts of turbo codes simulation are analyzed and their implementation on the GPU parallel architecture is discussed. Results on a case study for a serial concatenated convolutional code are presented, showing a simulation speed-up in excess of 10$\times$. These initial results show that the CPU/GPU approach is a powerful tool that allows the characterization of the high SNR behavior of turbo codes with a short simulation runtime.},
  doi       = {10.1109/ISTC.2012.6325199},
  file      = {:pdf/Chinnici2012 - Fast Simulation of Turbo Codes on GPUs.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2165-4700},
  keywords  = {graphics processing units, parallel architectures, turbo codes, CPU-GPU coprocessing strategy, GPU parallel architecture, bit error rates, conventional CPU, turbo codes simulation, Bit error rate, Computational modeling, Decoding, Graphics processing unit, Iterative decoding, Quadrature amplitude modulation, Turbo codes, GPU, SCCC, turbo codes},
}

@InProceedings{Giard2014,
  author    = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {Fast Software Polar Decoders},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {7555--7559},
  month     = may,
  publisher = {IEEE},
  abstract  = {Among error-correcting codes, polar codes are the first to provably achieve channel capacity with an explicit construction. In this work, we present software implementations of a polar decoder that leverage the capabilities of modern general-purpose processors to achieve an information throughput in excess of 200 Mbps, a throughput well suited for software-defined-radio applications. We also show that, for a similar error-correction performance, the throughput of polar decoders both surpasses that of LDPC decoders targeting general-purpose processors and is competitive with that of state-of-the-art software LDPC decoders running on graphic processing units.},
  doi       = {10.1109/ICASSP.2014.6855069},
  file      = {:pdf/Giard2014 - Fast Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  issn      = {1520-6149},
  keywords  = {channel capacity, codecs, error correction codes, graphics processing units, parity check codes, software radio, channel capacity, error-correcting codes, error-correction performance, general-purpose processors, graphic processing units, information throughput, polar codes, software LDPC decoders, software implementations, software polar decoders, software-defined-radio applications, Decoding, Graphics processing units, Hardware, Parity check codes, Signal processing algorithms, Throughput, Decoding, Error-Correction, Polar Codes, Software-Defined-Radio},
}

@Article{Muller2009,
  author   = {O. Muller and A. Baghdadi and M. Jezequel},
  title    = {From Parallelism Levels to a Multi-{ASIP} Architecture for Turbo Decoding},
  journal  = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year     = {2009},
  volume   = {17},
  number   = {1},
  pages    = {92--102},
  month    = jan,
  issn     = {1063-8210},
  abstract = {Emerging digital communication applications and the underlying architectures encounter drastically increasing performance and flexibility requirements. In this paper, we present a novel flexible multiprocessor platform for high throughput turbo decoding. The proposed platform enables exploiting all parallelism levels of turbo decoding applications to fulfill performance requirements. In order to fulfill flexibility requirements, the platform is structured around configurable application-specific instruction-set processors (ASIP) combined with an efficient memory and communication interconnect scheme. The designed ASIP has an single instruction multiple data (SIMD) architecture with a specialized and extensible instruction-set and 6-stages pipeline control. The attached memories and communication interfaces enable its integration in multiprocessor architectures. These multiprocessor architectures benefit from the recent shuffled decoding technique introduced in the turbo-decoding field to achieve higher throughput. The major characteristics of the proposed platform are its flexibility and scalability which make it reusable for all simple and double binary turbo codes of existing and emerging standards. Results obtained for double binary WiMAX turbo codes demonstrate around 250 Mb/s throughput using 16-ASIP multiprocessor architecture.},
  doi      = {10.1109/TVLSI.2008.2003164},
  file     = {:pdf/Muller2009 - From Parallelism Levels to a Multi-ASIP Architecture for Turbo Decoding.pdf:PDF},
  groups   = {Turbo Codes, Hardware Decoders, ASIP},
  keywords = {binary codes, decoding, digital communication, instruction sets, multiprocessor interconnection networks, parallel architectures, turbo codes, application-specific instruction-set processors, binary turbo codes, communication interconnect scheme, digital communication, flexibility requirements, flexible multiprocessor platform, multi-ASIP architecture, multiprocessor architectures, parallelism levels, single instruction multiple data, turbo decoding, Application-specific instruction-set processor (ASIP), Bahl--Cocke--Jelinek--Raviv (BCJR), multiprocessor, parallel processing, turbo decoding},
}

@Article{MacKay1999,
  author   = {D. J. C. MacKay},
  title    = {Good Error-Correcting Codes based on Very Sparse Matrices},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1999},
  volume   = {45},
  number   = {2},
  pages    = {399--431},
  month    = mar,
  issn     = {0018-9448},
  abstract = {We study two families of error-correcting codes defined in terms of very sparse matrices. {\textquotedblleft}MN{\textquotedblright} (MacKay-Neal (1995)) codes are recently invented, and {\textquotedblleft}Gallager codes{\textquotedblright} were first investigated in 1962, but appear to have been largely forgotten, in spite of their excellent properties. The decoding of both codes can be tackled with a practical sum-product algorithm. We prove that these codes are {\textquotedblleft}very good{\textquotedblright}, in that sequences of codes exist which, when optimally decoded, achieve information rates up to the Shannon limit. This result holds not only for the binary-symmetric channel but also for any channel with symmetric stationary ergodic noise. We give experimental results for binary-symmetric channels and Gaussian channels demonstrating that practical performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed, the performance of Gallager codes is almost as close to the Shannon limit as that of turbo codes},
  doi      = {10.1109/18.748992},
  file     = {:pdf/MacKay1999 - Good Error-Correcting Codes based on Very Sparse Matrices.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {Gaussian channels, channel coding, decoding, error correction codes, noise, sparse matrices, Gallager codes, Gaussian channels, MacKay-Neal codes, Shannon limit, binary-symmetric channel, channel coding, code sequences, concatenated codes, convolutional codes, decoding, error-correcting codes, information rates, optimally decoded codes, performance, sum-product algorithm, symmetric stationary ergodic noise, turbo codes, very sparse matrices, Code standards, Concatenated codes, Convolutional codes, Decoding, Error correction codes, Gaussian channels, Information rates, Sparse matrices, Sum product algorithm, Turbo codes},
}

@InProceedings{Yoge2012,
  author    = {D. R. N. Yoge and N. Chandrachoodan},
  title     = {{GPU} Implementation of a Programmable Turbo Decoder for Software Defined Radio Applications},
  booktitle = {International Conference on VLSI Design},
  year      = {2012},
  pages     = {149--154},
  month     = jan,
  publisher = {IEEE},
  abstract  = {This paper presents the implementation of a 3GPP standards compliant configurable turbo decoder on a GPU. The challenge in implementing a turbo decoder on a GPU is in suitably parallelizing the Log-MAP decoding algorithm and doing an architecture aware mapping of it on to the GPU. The approximations in parallelizing the Log-MAP algorithm come at the cost of reduced BER performance. To mitigate this reduction, different guarding mechanisms of varying computational complexity have been presented. The limited shared memory and registers available on GPUs are carefully allocated to obtain a high real-time decoding rate without requiring several independent data streams in parallel.},
  doi       = {10.1109/VLSID.2012.62},
  file      = {:pdf/Yoge2012 - GPU Implementation of a Programmable Turbo Decoder for Software Defined Radio Applications.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP, GPU},
  issn      = {1063-9667},
  keywords  = {3G mobile communication, approximation theory, coprocessors, error statistics, graphics processing units, maximum likelihood decoding, software radio, turbo codes, 3GPP standards compliant configurable turbo decoder, GPU implementation, Log-MAP decoding algorithm, approximations, programmable turbo decoder, reduced BER performance, software defined radio applications, Bit error rate, Decoding, Graphics processing unit, Instruction sets, Measurement, Parallel processing, Throughput, CUDA, GPU implementation, Guarding Mechanisms, Parallel Log-MAP, Turbo Decoder},
}

@Article{Leroux2012,
  author    = {C. Leroux and A. J. Raymond and G. Sarkis and I. Tal and A. Vardy and W. J. Gross},
  title     = {Hardware Implementation of Successive-Cancellation Decoders for Polar Codes},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2012},
  volume    = {69},
  number    = {3},
  pages     = {305},
  month     = dec,
  abstract  = {The recently-discovered polar codes are seen as a major breakthrough in coding theory; they provably achieve the theoretical capacity of discrete memoryless channels using the low-complexity successive cancellation decoding algorithm. Motivated by recent developments in polar coding theory, we propose a family of efficient hardware implementations for successive cancellation (SC) polar decoders. We show that such decoders can be implemented with O(N) processing elements and O(N) memory elements. Furthermore, we show that SC decoding can be implemented in the logarithmic domain, thereby eliminating costly multiplication and division operations, and reducing the complexity of each processing element greatly. We also present a detailed architecture for an SC decoder and provide logic synthesis results confirming the linear complexity growth of the decoder as the code length increases.},
  date      = {2012-12-01},
  doi       = {10.1007/s11265-012-0685-3},
  file      = {:pdf/Leroux2012 - Hardware Implementation of Successive-Cancellation Decoders for Polar Codes.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  publisher = {Springer},
}

@InProceedings{Wang2013,
  author    = {G. Wang and M. Wu and B. Yin and J. R. Cavallaro},
  title     = {High Throughput Low Latency {LDPC} Decoding on {GPU} for {SDR} Systems},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2013},
  pages     = {1258--1261},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, we present a high throughput and low latency LDPC (low-density parity-check) decoder implementation on GPUs (graphics processing units). The existing GPU-based LDPC decoder implementations suffer from low throughput and long latency, which prevent them from being used in practical SDR (software-defined radio) systems. To overcome this problem, we present optimization techniques for a parallel LDPC decoder including algorithm optimization, fully coalesced memory access, asynchronous data transfer and multi-stream concurrent kernel execution for modern GPU architectures. Experimental results demonstrate that the proposed LDPC decoder achieves 316 Mbps (at 10 iterations) peak throughput on a single GPU. The decoding latency, which is much lower than that of the state of the art, varies from 0.207 ms to 1.266 ms for different throughput requirements from 62.5 Mbps to 304.16 Mbps. When using four GPUs concurrently, we achieve an aggregate peak throughput of 1.25 Gbps (at 10 iterations).},
  doi       = {10.1109/GlobalSIP.2013.6737137},
  file      = {:pdf/Wang2013 - High Throughput Low Latency LDPC Decoding on GPU for SDR Systems.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, HoF LDPC - BP},
  keywords  = {codecs, data communication, decoding, graphics processing units, parity check codes, software radio, GPU architectures, GPU systems, GPU-based LDPC decoder, LDPC decoder, SDR systems, algorithm optimization, asynchronous data transfer, bit rate 1.25 Gbit/s, bit rate 316 Mbit/s, bit rate 62.5 Mbit/s to 304.16 Mbit/s, coalesced memory access, decoding latency, graphics processing units, high throughput low latency LDPC decoding, multistream concurrent kernel execution, optimization techniques, parallel LDPC decoder, software-defined radio, Decoding, Graphics processing units, Kernel, Message systems, Parity check codes, Throughput, WiMAX, GPU, LDPC codes, high throughput, low latency, software-defined radio},
}

@InProceedings{Zhang2012,
  author    = {Suiping Zhang and Rongrong Qian and Tao Peng and Ran Duan and Kuilin Chen},
  title     = {High Throughput Turbo Decoder Design for {GPP} Platform},
  booktitle = {International Conference on Communications and Networking in China (CHINACOM)},
  year      = {2012},
  pages     = {817--821},
  month     = aug,
  publisher = {IEEE},
  abstract  = {The great development of general-purpose processor technology makes the GPP possess the efficiently massive data processing abilities. In the solution(s) of 3GPP LTE system based on GPP platform, Turbo decoding algorithms play a vital role for high decoding complexity. This paper presents the design and implementation of SIMD technique based parallel turbo decoder suitable for GPP architecture. We improve the throughput through a) adopting fix-point design, b) taking full advantage of SIMD technique, c) using multiple soft-input soft-output decoders that operate in parallel, d) introducing multi-core technique to parallelize workload across cores. With 4 threads, the throughput of our SIMD and multi-threading technology based turbo decoder for GPP platform achieves 150 Mbps which can meet the requirement of LTE system.},
  doi       = {10.1109/ChinaCom.2012.6417597},
  file      = {:pdf/Zhang2012 - High Throughput Turbo Decoder Design for GPP Platform.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {3G mobile communication, Long Term Evolution, codecs, multi-threading, parallel processing, turbo codes, 3GPP LTE system, GPP platform, SIMD technique, general-purpose processor technology, massive data processing, multi-threading technology, turbo decoder design, turbo decoding algorithms, Approximation algorithms, Complexity theory, Computer architecture, Decoding, Signal processing, Throughput, Wireless communication, GPP, LTE, SIMD, multi-thread, turbo decoder},
}

@Article{LeGal2015,
  author   = {B. {Le Gal} and C. J\'ego},
  title    = {High-Throughput {LDPC} Decoder on Low-Power Embedded Processors},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2015},
  volume   = {19},
  number   = {11},
  pages    = {1861--1864},
  month    = nov,
  issn     = {1089-7798},
  abstract = {Real-time efficient implementations of LDPC decoders have long been considered exclusively reachable using dedicated hardware architectures. Attempts to implement LDPC decoders on CPU and GPU devices have lead to high power consumptions as well as high processing latencies that are incompatible with most embedded and mobile transmission systems. In this letter, we propose ARM-based decoders that go from 50 to 100 Mbps while executing 10 layered-decoding iterations. We hereby demonstrate that efficient LDPC decoders can be implemented on a low-power programmable architecture. The proposed decoders are competitive with recent GPU related works. Therefore, software LDPC decoders constitute a response to software defined radio constraints.},
  doi      = {10.1109/LCOMM.2015.2477081},
  file     = {:pdf/LeGal2015 - High-Throughput LDPC Decoder on Low-Power Embedded Processors.pdf:PDF},
  groups   = {LDPC Codes, Software Decoders},
  keywords = {decoding, embedded systems, graphics processing units, low-power electronics, microcontrollers, parity check codes, 10-layered-decoding iterations, ARM-based decoders, CPU devices, GPU devices, hardware architectures, high-throughput LDPC decoder, latencies, low-density parity check code, low-power embedded processors, low-power programmable architecture, Decoding, Graphics processing units, Parity check codes, Power demand, Throughput, ARM processor, LDPC, SIMD, multi-core},
}

@Article{LeGal2016,
  author   = {B. {Le Gal} and C. J\'ego},
  title    = {High-Throughput Multi-Core {LDPC} Decoders Based on x86 Processor},
  journal  = {IEEE Transactions on Parallel and Distributed Systems (TPDS)},
  year     = {2016},
  volume   = {27},
  number   = {5},
  pages    = {1373--1386},
  month    = may,
  issn     = {1045-9219},
  abstract = {Low-Density Parity-Check (LDPC) codes are an efficient way to correct transmission errors in digital communication systems. Although initially targeting strictly to ASICs due to computation complexity, LDPC decoders have been recently ported to multicore and many-core systems. Most works focused on taking advantage of GPU devices. In this paper, we propose an alternative solution based on a layered OMS/NMS LDPC decoding algorithm that can be efficiently implemented on a multi-core device using Single Instruction Multiple Data (SIMD) and Single Program Multiple Data (SPMD) programming models. Several experimentations were performed on a x86 processor target. Throughputs up to 170 Mbps were achieved on a single core of an INTEL Core i7 processor when executing 20 layered-based decoding iterations. Throughputs reaches up to 560 Mbps on four INTEL Core-i7 cores. Experimentation results show that the proposed implementations achieved similar BER correction performance than previous works. Moreover, much higher throughputs have been achieved by comparison with all previous GPU and CPU works. They range from x1.4 to x8 by comparison with recent GPU works.},
  doi      = {10.1109/TPDS.2015.2435787},
  file     = {:pdf/LeGal2016 - High-Throughput Multi-Core LDPC Decoders Based on x86 Processor.pdf:PDF},
  groups   = {LDPC Codes, Software Decoders, HoF LDPC - BP},
  keywords = {application specific integrated circuits, decoding, error statistics, microprocessor chips, multiprocessing systems, parity check codes, 20 layered-based decoding iterations, ASIC, BER correction, INTEL Core i7 processor, SIMD, SPMD, computation complexity, digital communication systems, high-throughput multicore LDPC decoders, layered OMS-NMS LDPC decoding, low-density parity-check codes, many-core systems, multicore device, normalized min-sum, offset min-sum, single instruction multiple data, single program multiple data, transmission errors, x86 processor, Decoding, Graphics processing units, Iterative decoding, Processor scheduling, Throughput, GPU, LDPC decoding, SIMD architecture, Software Define Radio (SDR), software decoders, software define radio (SDR), throughput, x86 processor},
}

@InProceedings{Debbabi2016,
  author    = {I. Debbabi and N. Khouja and F. Tlili and B. {Le Gal} and C. J\'ego},
  title     = {Multicore Implementation of {LDPC} Decoders based on {ADMM} Algorithm},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2016},
  pages     = {971--975},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Alternate direction method of multipliers (ADMM) technique has recently been proposed for LDPC decoding. Even though it improves the error rate performance compared with traditional message passing (MP) techniques, it shows a higher computation complexity. In this article, the ADMM decoding algorithm is first described. Then, its computation complexity is analyzed. Finally, an optimized version which benefits from the multi-core processors architecture as well as the ADMM algorithm's parallelism is presented. The optimized version of the ADMM decoder can achieve up to 30 Mbps for standardized LDPC codes on a laptop x86 processor. Therefore, it could guide an efficient GPU implementation for real-time and high-throughput decoding systems requiring correction performances beyond MP-Sum Product Algorithm (SPA) capabilities.},
  doi       = {10.1109/ICASSP.2016.7471820},
  file      = {:pdf/Debbabi2016a - Real Time LP Decoding of LDPC Codes for High Correction Performance Applications.pdf:PDF;:pdf/Debbabi2016a - Real Time LP Decoding of {LDPC} Codes for High Correction Performance Applications.pdf:PDF},
  groups    = {LDPC Codes, Software Decoders, HoF LDPC - LP},
  keywords  = {computational complexity, decoding, graphics processing units, multiprocessing systems, parity check codes, ADMM technique, GPU implementation, LDPC decoding, MP-SPA capabilities, MP-sum product algorithm capabilities, alternate direction method of multipliers technique, computation complexity, error rate performance, laptop x86 processor, message passing techniques, multicore processors architecture, standardized LDPC codes, Complexity theory, Decoding, Iterative decoding, Kernel, Signal processing algorithms, Software algorithms, ADMM algorithm, Forward Error Correction codes, LDPC decoding, muti-core, software optimization},
}

@Article{Tal2013,
  author   = {I. Tal and A. Vardy},
  title    = {How to Construct Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2013},
  volume   = {59},
  number   = {10},
  pages    = {6562--6582},
  month    = oct,
  issn     = {0018-9448},
  abstract = {A method for efficiently constructing polar codes is presented and analyzed. Although polar codes are explicitly defined, straightforward construction is intractable since the resulting polar bit-channels have an output alphabet that grows exponentially with the code length. Thus, the core problem that needs to be solved is that of faithfully approximating a bit-channel with an intractably large alphabet by another channel having a manageable alphabet size. We devise two approximation methods which {\textquotedblleft}sandwich{\textquotedblright} the original bit-channel between a degraded and an upgraded version thereof. Both approximations can be efficiently computed and turn out to be extremely close in practice. We also provide theoretical analysis of our construction algorithms, proving that for any fixed $\epsilon$ $>$ 0 and all sufficiently large code lengths n, polar codes whose rate is within $\epsilon$ of channel capacity can be constructed in time and space that are both linear in n.},
  doi      = {10.1109/TIT.2013.2272694},
  file     = {:pdf/Tal2013 - How to Construct Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {approximation theory, codes, alphabet size, approximation methods, channel capacity, construction algorithms, polar bit-channels, polar codes, Approximation algorithms, Approximation methods, Complexity theory, Convolutional codes, Decoding, Kernel, Quantization (signal), Channel degrading and upgrading, channel polarization, construction algorithms, polar codes},
}

@InProceedings{Wu2013,
  author    = {M. Wu and G. Wang and B. Yin and C. Studer and J. R. Cavallaro},
  title     = {{HSPA+/LTE-A} Turbo Decoder on {GPU} and Multicore {CPU}},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2013},
  pages     = {824--828},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper compares two implementations of reconfigurable and high-throughput turbo decoders. The first implementation is optimized for an NVIDIA Kepler graphics processing unit (GPU), whereas the second implementation is for an Intel Ivy Bridge processor. Both implementations support max-log-MAP and log-MAP turbo decoding algorithms, various code rates, different interleaver types, and all block-lengths, as specified by HSPA; and LTE-Advanced. In order to ensure a fair comparison between both implementations, we perform device-specific optimizations to improve the decoding throughput and error-rate performance. Our results show that the Intel Ivy Bridge processor implementation achieves up to 2$\times$ higher decoding throughput than our GPU implementation. In addition our CPU implementation requires roughly 4$\times$ fewer codewords to be processed in parallel to achieve its peak throughput.},
  doi       = {10.1109/ACSSC.2013.6810402},
  file      = {:pdf/Wu2013 - HSPA+ LTE-A Turbo Decoder on GPU and Multicore CPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {Long Term Evolution, graphics processing units, maximum likelihood decoding, multiprocessing systems, turbo codes, GPU, HSPA, Intel Ivy Bridge processor, LTE-Advanced, NVIDIA Kepler graphics processing unit, block-lengths, code rates, decoding throughput, device-specific optimizations, error-rate performance, high-throughput turbo decoders, interleaver types, log-MAP turbo decoding algorithms, max-log-MAP algorithms, multicore CPU, reconfigurable turbo decoders, Approximation methods, Decoding, Graphics processing units, Measurement, Throughput, Turbo codes, Vectors},
}

@InProceedings{Paul2015,
  author    = {H. Paul and D. Wübben and P. Rost},
  title     = {Implementation and Analysis of Forward Error Correction Decoding for Cloud-{RAN} Systems},
  booktitle = {International Conference on Communications Workshop (ICCW)},
  year      = {2015},
  pages     = {2708--2713},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In future 5G mobile networks, radio access network functions will be virtualized and implemented on centralized cloud platforms. In principle, this allows for more advanced algorithms of joint processing and offers the ability to balance the computational load. However, the shift of functionality on a cloud-platform also imposes challenges on the design of the applied algorithms. In this paper, we analyse the implementation of forward error correction decoding algorithms on general purpose hardware, which draws the main computational burden of signal processing in the uplink. Although the protocol stack introduces strict timing requirements we demonstrate by numerical results the feasibility of such implementation.},
  doi       = {10.1109/ICCW.2015.7247588},
  file      = {:pdf/Paul2015 - Implementation and Analysis of Forward Error Correction Decoding for Cloud-RAN Systems.pdf:PDF},
  groups    = {Cloud-RAN},
  issn      = {2164-7038},
  keywords  = {5G mobile communication, cloud computing, decoding, forward error correction, protocols, radio access networks, signal processing, virtualisation, 5G mobile networks, centralized cloud platforms, cloud-RAN systems, computational load, forward error correction decoding algorithms, protocol stack, radio access network functions, signal processing, Decoding, Forward error correction, Hardware, Mobile communication, Radio access networks, Signal processing algorithms, Signal to noise ratio},
}

@InProceedings{Wu2010,
  author    = {M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {Implementation of a {3GPP} {LTE} Turbo Decoder Accelerator on {GPU}},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2010},
  pages     = {192--197},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This paper presents a 3GPP LTE compliant turbo decoder accelerator on GPU. The challenge of implementing a turbo decoder is finding an efficient mapping of the decoder algorithm on GPU, e.g. finding a good way to parallelize workload across cores and allocate and use fast on-die memory to improve throughput. In our implementation, we increase throughput through 1) distributing the decoding workload for a codeword across multiple cores, 2) decoding multiple codewords simultaneously to increase concurrency and 3) employing memory optimization techniques to reduce memory bandwidth requirements. In addition, we analyze how different MAP algorithm approximations affect both throughput and bit error rate (BER) performance of this decoder.},
  doi       = {10.1109/SIPS.2010.5624788},
  file      = {:pdf/Wu2010 - Implementation of a 3GPP LTE Turbo Decoder Accelerator on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  issn      = {2162-3562},
  keywords  = {3G mobile communication, computer graphic equipment, coprocessors, error statistics, maximum likelihood decoding, turbo codes, 3 GPP, GPU, LTE, MAP algorithm, bit error rate, codeword, decoding, memory optimization techniques, turbo decoder accelerator, Bit error rate, Decoding, Graphics processing unit, Instruction sets, Iterative decoding, Kernel, Throughput},
}

@Article{Bang2014,
  author    = {S. Bang and C. Ahn and Y. Jin and S. Choi and J. Glossner and S. Ahn},
  title     = {Implementation of {LTE} System on an {SDR} Platform using {CUDA} and {UHD}},
  journal   = {Springer Journal of Analog Integrated Circuits and Signal Processing (AICSP)},
  year      = {2014},
  volume    = {78},
  number    = {3},
  pages     = {599},
  month     = mar,
  abstract  = {In this paper, we present an implementation of a long term evolution (LTE) system on a software defined radio (SDR) platform using a conventional personal computer that adopts a graphic processing unit (GPU) and a universal software radio peripheral2 (USRP2) with a URSP hardware driver (UHD) to implement an SDR software modem and a radio frequency transceiver, respectively. The central processing unit executes C++ control code that can access the USRP2 via the UHD. We have adopted the Ettus Research UHD due to its high degree of flexibility in the design of the transceiver chain. By taking advantage of this benefit, a simple cognitive radio engine has been implemented using libraries provided by the UHD. We have implemented the software modem on a GPU that is suitable for parallel computing due to its powerful arithmetic and logic units. A parallel programming method is proposed that exploits the single instruction multiple data architecture of the GPU. We focus on the implementation of the Turbo decoder due to its high computational requirements and difficulty in parallelizing the algorithm. The implemented system is analyzed primarily in terms of computation time using the compute unified device architecture profiler. From our experimental tests using the implemented system, we have measured the total processing time for a single frame of both transmit and receive LTE data. We find that it takes 5.00 and 8.58 ms for transmit and receive, respectively. This confirms that the implemented system is capable of real-time processing of all the baseband signal processing algorithms required for LTE systems.},
  date      = {2014-03-01},
  doi       = {10.1007/s10470-013-0229-1},
  file      = {:pdf/Bang2014 - Implementation of LTE System on an SDR Platform using CUDA and UHD.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, GPU},
  publisher = {Springer},
}

@Article{Wu2011,
  author    = {M. Wu and Y. Sun and G. Wang and J. R. Cavallaro},
  title     = {Implementation of a High Throughput {3GPP} Turbo Decoder on {GPU}},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2011},
  volume    = {65},
  number    = {2},
  pages     = {171},
  month     = sep,
  abstract  = {Turbo code is a computationally intensive channel code that is widely used in current and upcoming wireless standards. General-purpose graphics processor unit (GPGPU) is a programmable commodity processor that achieves high performance computation power by using many simple cores. In this paper, we present a 3GPP LTE compliant Turbo decoder accelerator that takes advantage of the processing power of GPU to offer fast Turbo decoding throughput. Several techniques are used to improve the performance of the decoder. To fully utilize the computational resources on GPU, our decoder can decode multiple codewords simultaneously, divide the workload for a single codeword across multiple cores, and pack multiple codewords to fit the single instruction multiple data (SIMD) instruction width. In addition, we use shared memory judiciously to enable hundreds of concurrent multiple threads while keeping frequently used data local to keep memory access fast. To improve efficiency of the decoder in the high SNR regime, we also present a low complexity early termination scheme based on average extrinsic LLR statistics. Finally, we examine how different workload partitioning choices affect the error correction performance and the decoder throughput.},
  date      = {2011-09-10},
  doi       = {10.1007/s11265-011-0617-7},
  file      = {:pdf/Wu2011 - Implementation of a High Throughput 3GPP Turbo Decoder on GPU.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  publisher = {Springer},
}

@Article{Vogt2000,
  author   = {J. Vogt and A. Finger},
  title    = {Improving the max-log-{MAP} Turbo Decoder},
  journal  = {IET Electronics Letters},
  year     = {2000},
  volume   = {36},
  number   = {23},
  pages    = {1937--1939},
  month    = nov,
  issn     = {0013-5194},
  abstract = {Decoding turbo codes with the max-log-MAP algorithm is a good compromise between performance and complexity. The decoding quality of the max-log-MAP decoder is improved by using a scaling factor within the extrinsic calculation. Simulations using the 1MT-2000/3GPP parameters demonstrate that this method gives ~0.2 to 0.4 dB performance gain compared to the standard max-log-MAP algorithm},
  doi      = {10.1049/el:20001357},
  file     = {:pdf/Vogt2000 - Improving the max-log-MAP Turbo Decoder.pdf:PDF},
  groups   = {Turbo Codes},
  keywords = {decoding, turbo codes, 1MT-2000 parameters, 3GPP parameters, decoding quality improvement, max-log-MAP turbo decoder, scaling factor, turbo codes},
}

@InProceedings{Sarkis2014b,
  author    = {G. Sarkis and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title     = {Increasing the Speed of Polar List Decoders},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this work, we present a simplified successive cancellation list decoder that uses a Chase-like decoding process to achieve a six time improvement in speed compared to successive cancellation list decoding while maintaining the same error-correction performance advantage over standard successive-cancellation polar decoders. We discuss the algorithm and detail the data structures and methods used to obtain this speed-up. We also propose an adaptive decoding algorithm that significantly improves the throughput while retaining the error-correction performance. Simulation results over the additive white Gaussian noise channel are provided and show that the proposed system is up to 16 times faster than an LDPC decoder of the same frame size, code rate, and similar error-correction performance, making it more suitable for use as a software decoding solution.},
  doi       = {10.1109/SiPS.2014.6986089},
  file      = {:pdf/Sarkis2014b - Increasing the Speed of Polar List Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SCL},
  issn      = {2162-3562},
  keywords  = {AWGN channels, adaptive decoding, data structures, error correction, parity check codes, software engineering, Chase-like decoding process, LDPC decoder, adaptive decoding algorithm, additive white Gaussian noise channel, cancellation list decoding, data structures, error-correction performance advantage, polar list decoders, simplified successive cancellation list decoder, software decoding solution, standard successive-cancellation polar decoders, Maximum likelihood decoding, Parity check codes, Reliability, Simulation, Software, Throughput},
}

@Article{Tal2015,
  author   = {I. Tal and A. Vardy},
  title    = {List Decoding of Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2015},
  volume   = {61},
  number   = {5},
  pages    = {2213--2226},
  month    = may,
  issn     = {0018-9448},
  abstract = {We describe a successive-cancellation list decoder for polar codes, which is a generalization of the classic successive-cancellation decoder of Ar{\i}kan. In the proposed list decoder, L decoding paths are considered concurrently at each decoding stage, where L is an integer parameter. At the end of the decoding process, the most likely among the L paths is selected as the single codeword at the decoder output. Simulations show that the resulting performance is very close to that of maximum-likelihood decoding, even for moderate values of L. Alternatively, if a genie is allowed to pick the transmitted codeword from the list, the results are comparable with the performance of current state-of-the-art LDPC codes. We show that such a genie can be easily implemented using simple CRC precoding. The specific list-decoding algorithm that achieves this performance doubles the number of decoding paths for each information bit, and then uses a pruning procedure to discard all but the L most likely paths. However, straightforward implementation of this algorithm requires {{$\Omega$}}(Ln\textsuperscript{2}) time, which is in stark contrast with the O(n log n) complexity of the original successive-cancellation decoder. In this paper, we utilize the structure of polar codes along with certain algorithmic transformations in order to overcome this problem: we devise an efficient, numerically stable, implementation of the proposed list decoder that takes only O(Ln logn) time and O(Ln) space.},
  doi      = {10.1109/TIT.2015.2410251},
  file     = {:pdf/Tal2015 - List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {cyclic redundancy check codes, maximum likelihood decoding, parity check codes, precoding, CRC precoding, LDPC codes, algorithmic transformations, codeword, decoder output, decoding path, decoding process, decoding stage, information bit, integer parameter, list decoding, maximum-likelihood decoding, polar codes, pruning procedure, specific list-decoding algorithm, stark contrast, successive-cancellation list decoder, Arrays, Bit error rate, Complexity theory, Maximum likelihood decoding, Parity check codes, Vectors, List decoding, polar codes, successive cancellation decoding},
}

@InProceedings{Tal2011,
  author    = {I. Tal and A. Vardy},
  title     = {List Decoding of Polar Codes},
  booktitle = {International Symposium on Information Theory (ISIT)},
  year      = {2011},
  pages     = {1--5},
  month     = jul,
  publisher = {IEEE},
  abstract  = {We describe a successive-cancellation list decoder for polar codes, which is a generalization of the classic successive-cancellation decoder of Arikan. In the proposed list decoder, up to L decoding paths are considered concurrently at each decoding stage. Simulation results show that the resulting performance is very close to that of a maximum-likelihood decoder, even for moderate values of L. Thus it appears that the proposed list decoder bridges the gap between successive-cancellation and maximum-likelihood decoding of polar codes. The specific list-decoding algorithm that achieves this performance doubles the number of decoding paths at each decoding step, and then uses a pruning procedure to discard all but the L {\textquotedblleft}best{\textquotedblright} paths. In order to implement this algorithm, we introduce a natural pruning criterion that can be easily evaluated. Nevertheless, straightforward implementation still requires O(L $\cdot$ n\textsuperscript{2}) time, which is in stark contrast with the O(n log n) complexity of the original successive-cancellation decoder. We utilize the structure of polar codes to overcome this problem. Specifically, we devise an efficient, numerically stable, implementation taking only O(L $\cdot$ n log n) time and O(L $\cdot$ n) space.},
  doi       = {10.1109/ISIT.2011.6033904},
  file      = {:pdf/Tal2011 - List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2157-8095},
  keywords  = {maximum likelihood decoding, Arikan, decoding paths, maximum likelihood decoder, polar codes, successive-cancellation list decoder, Arrays, Complexity theory, Equations, Error analysis, Mathematical model, Maximum likelihood decoding},
}

@Article{Balatsoukas-Stimming2015,
  author   = {A. Balatsoukas-Stimming and M. B. Parizi and A. Burg},
  title    = {{LLR}-Based Successive Cancellation List Decoding of Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2015},
  volume   = {63},
  number   = {19},
  pages    = {5165--5179},
  month    = oct,
  issn     = {1053-587X},
  abstract = {We show that successive cancellation list decoding can be formulated exclusively using log-likelihood ratios. In addition to numerical stability, the log-likelihood ratio based formulation has useful properties that simplify the sorting step involved in successive cancellation list decoding. We propose a hardware architecture of the successive cancellation list decoder in the log-likelihood ratio domain which, compared with a log-likelihood domain implementation, requires less irregular and smaller memories. This simplification, together with the gains in the metric sorter, lead to 56\% to 137\% higher throughput per unit area than other recently proposed architectures. We then evaluate the empirical performance of the CRC-aided successive cancellation list decoder at different list sizes using different CRCs and conclude that it is important to adapt the CRC length to the list size in order to achieve the best error-rate performance of concatenated polar codes. Finally, we synthesize conventional successive cancellation decoders at large block-lengths with the same block-error probability as our proposed CRC-aided successive cancellation list decoders to demonstrate that, while our decoders have slightly lower throughput and larger area, they have a significantly smaller decoding latency.},
  doi      = {10.1109/TSP.2015.2439211},
  file     = {:pdf/Balatsoukas-Stimming2015a - On Metric Sorting for Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {concatenated codes, cyclic redundancy check codes, error statistics, statistical analysis, CRC-aided successive cancellation list decoder, LLR-based successive cancellation list decoding, block-error probability, concatenated polar codes, log-likelihood ratio based formulation, metric sorter, numerical stability, sorting step, Decoding, Hardware, Indexes, Receivers, Signal processing algorithms, Sorting, Throughput, CRC-aided successive cancellation list decoder, hardware implementation, polar codes, successive cancellation decoder, successive cancellation list decoder},
}

@InProceedings{Balatsoukas-Stimming2014,
  author    = {A. Balatsoukas-Stimming and M. Bastani Parizi and A. Burg},
  title     = {{LLR}-Based Successive Cancellation List Decoding of Polar Codes},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {3903--3907},
  month     = may,
  publisher = {IEEE},
  abstract  = {We present an LLR-based implementation of the successive cancellation list (SCL) decoder. To this end, we associate each decoding path with a metric which (i) is a monotone function of the path's likelihood and (ii) can be computed efficiently from the channel LLRs. The LLR-based formulation leads to a more efficient hardware implementation of the decoder compared to the known log-likelihood based implementation. Synthesis results for an SCL decoder with block-length of N = 1024 and list sizes of L = 2 and L = 4 confirm that the LLR-based decoder has considerable area and operating frequency advantages in the orders of 50\% and 30\%, respectively.},
  doi       = {10.1109/ICASSP.2014.6854333},
  file      = {:pdf/Balatsoukas-Stimming2014 - LLR-Based Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1520-6149},
  keywords  = {channel coding, decoding, LLR-based successive cancellation list decoding, SCL decoder, block-length, channel LLR, log-likelihood based implementation, log-likelihood ratios, monotone function, path likelihood, polar codes, Clocks, Decoding, Hardware, Memory management, Signal processing, Hardware Implementation, Polar Codes, Successive Cancellation List Decoder},
}

@InProceedings{Fan2015,
  author    = {Y. Fan and J. Chen and C. Xia and C. y. Tsui and J. Jin and H. Shen and B. Li},
  title     = {Low-latency List Decoding of Polar Codes with Double Thresholding},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2015},
  pages     = {1042--1046},
  month     = apr,
  publisher = {IEEE},
  abstract  = {For polar codes with short-to-medium code length, list successive cancellation decoding is used to achieve a good error-correcting performance. However, list pruning in the current list decoding is based on the sorting strategy and its timing complexity is high. This results in a long decoding latency for large list size. In this work, aiming at a low-latency list decoding implementation, a double thresholding algorithm is proposed for a fast list pruning. As a result, with a negligible performance degradation, the list pruning delay is greatly reduced. Based on the double thresholding, a low-latency list decoding architecture is proposed and implemented using a UMC 90nm CMOS technology. Synthesis results show that, even for a large list size of 16, the proposed low-latency architecture achieves a decoding throughput of 220 Mbps at a frequency of 641 MHz.},
  doi       = {10.1109/ICASSP.2015.7178128},
  file      = {:pdf/Fan2015 - Low-latency List Decoding of Polar Codes with Double Thresholding.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1520-6149},
  keywords  = {CMOS logic circuits, decoding, UMC CMOS technology, double thresholding, error-correcting performance, frequency 641 MHz, list pruning, list successive cancellation decoding, long decoding latency, low-latency list decoding, polar codes, short-to-medium code length, size 90 nm, sorting strategy, timing complexity, Clocks, Computer architecture, Decoding, Delays, Phasor measurement units, Sorting, Polar codes, VLSI implementation, list decoding, low latency, successive cancellation decoding},
}

@Article{Fan2016,
  author   = {Y. Fan and C. Xia and J. Chen and C. Y. Tsui and J. Jin and H. Shen and B. Li},
  title    = {A Low-Latency List Successive-Cancellation Decoding Implementation for Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2016},
  volume   = {34},
  number   = {2},
  pages    = {303--317},
  month    = feb,
  issn     = {0733-8716},
  abstract = {Due to their provably capacity-achieving performance, polar codes have attracted a lot of research interest recently. For a good error-correcting performance, list successive-cancellation decoding (LSCD) with large list size is used to decode polar codes. However, as the complexity and delay of the list management operation rapidly increase with the list size, the overall latency of LSCD becomes large and limits the applicability of polar codes in high-throughput and latency-sensitive applications. Therefore, in this work, the low-latency implementation for LSCD with large list size is studied. Specifically, at the system level, a selective expansion method is proposed such that some of the reliable bits are not expanded to reduce the computation and latency. At the algorithmic level, a double thresholding scheme is proposed as a fast approximate-sorting method for the list management operation to reduce the LSCD latency for large list size. A VLSI architecture of the LSCD implementing the selective expansion and double thresholding scheme is then developed, and implemented using a UMC 90 nm CMOS technology. Experimental results show that, even for a large list size of 16, the proposed LSCD achieves a decoding throughput of 460 Mbps at a clock frequency of 658 MHz.},
  doi      = {10.1109/JSAC.2015.2504318},
  file     = {:pdf/Fan2016 - A Low-Latency List Successive-Cancellation Decoding Implementation for Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {CMOS integrated circuits, VLSI, computational complexity, decoding, error correction codes, integrated circuit reliability, LSCD VLSI architecture, UMC CMOS technology, error-correcting performance, high-throughput applications, latency-sensitive applications, list management operation complexity, list management operation delay, low-latency list successive-cancellation decoding implementation, polar code reliability, selective expansion method, Complexity theory, Computer architecture, Delays, Maximum likelihood decoding, Reliability, Polar codes, VLSI decoder architectures, double thresholding, list decoding, selective expansion, successive-cancellation decoding},
}

@InProceedings{Li2014a,
  author    = {B. Li and H. Shen and D. Tse and W. Tong},
  title     = {Low-Latency Polar Codes via Hybrid Decoding},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2014},
  pages     = {223--227},
  month     = aug,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a family of hybrid decoders for polar codes. By decomposing the overall polar code into an inner code and an outer code, a hybrid decoder in the family uses successive cancellation (SC) to decode the inner code and maximum-likelihood (ML) to decode the outer code. At one extreme in the family is the ML decoder, when the entire polar code is viewed as the outer code; at the other extreme is the SC decoder, when the entire polar code is viewed as the inner code. Since ML decoding has lower latency than SC decoding, a hybrid decoder can achieve lower latency than the conventional SC decoder, at the expense of higher complexity due to the ML decoding of the outer code. We propose a reduction in the complexity of ML decoding by exploiting the structure of polar codes.},
  doi       = {10.1109/ISTC.2014.6955118},
  file      = {:pdf/Li2014a - Low-Latency Polar Codes via Hybrid Decoding.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2165-4700},
  keywords  = {codes, computational complexity, maximum likelihood decoding, ML decoder, SC decoder, hybrid decoding, inner code, low-latency polar codes, maximum-likelihood decoder, outer code, successive cancellation decoder, Complexity theory, Indexes, Information processing, Iterative decoding, Maximum likelihood decoding, Turbo codes},
}

@Article{Giard2016b,
  author    = {P. Giard and G. Sarkis and C. Leroux and C. Thibeault and W. J. Gross},
  title     = {Low-Latency Software Polar Decoders},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2016},
  volume    = {90},
  pages     = {761--775},
  month     = jul,
  abstract  = {Polar codes are a new class of capacity-achieving error-correcting codes with low encoding and decoding complexity. Their low-complexity decoding algorithms rendering them attractive for use in software-defined radio applications where computational resources are limited. In this work, we present low-latency software polar decoders that exploit modern processor capabilities. We show how adapting the algorithm at various levels can lead to significant improvements in latency and throughput, yielding polar decoders that are suitable for high-performance software-defined radio applications on modern desktop processors and embedded-platform processors. These proposed decoders have an order of magnitude lower latency and memory footprint compared to state-of-the-art decoders, while maintaining comparable throughput. In addition, we present strategies and results for implementing polar decoders on graphical processing units. Finally, we show that the energy efficiency of the proposed decoders is comparable to state-of-the-art software polar decoders.},
  date      = {2016-07-11},
  doi       = {10.1007/s11265-016-1157-y},
  file      = {:pdf/Giard2016b - Low-Latency Software Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  publisher = {Springer},
}

@InProceedings{Shen2016,
  author    = {Y. Shen and C. Zhang and J. Yang and S. Zhang and X. You},
  title     = {Low-Latency Software Successive Cancellation List Polar Decoder using Stage-Located Copy},
  booktitle = {International Conference on Digital Signal Processing (DSP)},
  year      = {2016},
  pages     = {84--88},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Successive cancellation list (SCL) decoding for polar codes is promising in data communication. However, in addition to L times complexity of conventional SC, both path selecting and updating result in extra complexity. In detail, the copy of intermediate values suffers from a long latency, especially when list size L is large. In this paper, a stage-located copy algorithm is proposed to avoid copying the same contents in candidate paths, which significantly reduces the processing latency. Furthermore, the resulting data processing speedup increases with code length. For (2048, 1723) polar codes, experimental results have shown that by employing the proposed stage-located copy, throughput of software-based SCL decoder with L = 32 achieves up to 1.1 Mbps throughput with 45\% increase compared to the state-of-the-art software SCL decoders.},
  doi       = {10.1109/ICDSP.2016.7868521},
  file      = {:pdf/Shen2016 - Low-Latency Software Successive Cancellation List Polar Decoder using Stage-Located Copy.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SCL},
  keywords  = {channel coding, computational complexity, data communication, software radio, L times complexity, data communication, low-latency software successive cancellation list polar decoder, polar codes, software defined radio, software-based SCL decoder, stage-located copy algorithm, Complexity theory, Decoding, Indexes, Software, Software algorithms, Sorting, Upper bound, Polar codes, reference matrix, stage-located copy, successive cancellation list (SCL) decoder},
}

@Article{Yuan2014,
  author   = {B. Yuan and K. K. Parhi},
  title    = {Low-Latency Successive-Cancellation Polar Decoder Architectures Using 2\mbox{-}Bit Decoding},
  journal  = {IEEE Transactions on Circuits and Systems (TCS)},
  year     = {2014},
  volume   = {61},
  number   = {4},
  pages    = {1241--1254},
  month    = apr,
  issn     = {1549-8328},
  abstract = {Polar codes have emerged as important error correction codes due to their capacity-achieving property. Successive cancellation (SC) algorithm is viewed as a good candidate for hardware design of polar decoders due to its low complexity. However, for (n, k) polar codes, the long latency of SC algorithm of (2n-2) is a bottleneck for designing high-throughput polar decoder. In this paper, we present a novel reformulation for the last stage of SC decoding. The proposed reformulation leads to two benefits. First, critical path and hardware complexity in the last stage of SC algorithm is significantly reduced. Second, 2 bits can be decoded simultaneously instead of 1 bit. As a result, this new decoder, referred to as 2b-SC decoder, reduces latency from (2n-2) to (1.5n-2) without performance loss. Additionally, overlapped-scheduling, precomputation and look-ahead techniques are used to design two additional decoders referred to as 2b-SC-Overlapped-scheduling decoder and 2b-SC-Precomputation decoder, respectively. All three architectures offer significant advantages with respect to throughput and hardware efficiency. Compared to known prior least-latency SC decoder, the 2b-SC-Precomputation decoder has 25\% less latency. Synthesis results show that the proposed (1024, 512) 2b-SC-Precomputation decoder can achieve at least 4 times increase in throughput and 40\% increase in hardware efficiency.},
  doi      = {10.1109/TCSI.2013.2283779},
  file     = {:pdf/Yuan2014 - Low-Latency Successive-Cancellation Polar Decoder Architectures Using 2-Bit Decoding.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {decoding, 2 bit decoding, SC algorithm, hardware complexity, hardware design, hardware efficiency, least-latency SC decoder, low latency successive cancellation polar decoder architectures, successive cancellation, Algorithm design and analysis, Complexity theory, Computer architecture, Decoding, Encoding, Equations, Hardware, 2-bit decoder, Look-ahead, overlapped scheduling, polar codes, precomputation, successive cancellation},
}

@Article{LeGal2015a,
  author   = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title    = {Multi-{G}b/s Software Decoding of Polar Codes},
  journal  = {IEEE Transactions on Signal Processing (TSP)},
  year     = {2015},
  volume   = {63},
  number   = {2},
  pages    = {349--359},
  month    = jan,
  issn     = {1053-587X},
  abstract = {This paper presents an optimized software implementation of a Successive Cancellation (SC) decoder for polar codes. Despite the strong data dependencies in SC decoding, a highly parallel software polar decoder is devised for x86 processor target. A high level of performance is achieved by exploiting the parallelism inherent in today's processor architectures (SIMD, multicore, etc.). Some optimizations that were originally thought for hardware implementation (memory reduction techniques and algorithmic simplifications) were also applied to enhance the throughput of the software implementation. Finally, some low level optimizations such as explicit assembly description or data packing are used to improve the throughput even more. The resulting decoder description is implemented on different x86 processor targets. An analysis of the decoder in terms of latency and throughput is proposed. The influence of several parameters on the throughput and the latency is investigated: the selected target, the code rate, the code length, the SIMD mode (SSE/AVX), the multithreading mode, etc. The energy per decoded bit is also estimated. The proposed software decoder compares favorably with state of the art software polar decoders. Extensive experimentations demonstrate that the proposed software polar decoder exceeds 1 Gb/s for code lengths N $\leq$ 2\textsuperscript{17} on a single core and reaches multi-Gb/s throughputs when using four cores in parallel in AVX mode.},
  doi      = {10.1109/TSP.2014.2371781},
  file     = {:pdf/LeGal2015a - Multi-Gbps Software Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, HoF Polar - SC, AFF3CT},
  keywords = {decoding, optimisation, AVX mode, SC decoder, SIMD mode, algorithmic simplifications, code length, code rate, data packing, energy per decoded bit, explicit assembly description, low level optimizations, memory reduction techniques, multiGb/s software decoding, multithreading mode, parallel software polar decoder, polar codes, processor architectures, selected target, software polar decoders, successive cancellation decoder, x86 processor target, Decoding, Optimization, Signal processing algorithms, Software, Systematics, Throughput, Vectors, Polar codes, SIMD, software optimizations, successive cancellation decoding, x86 processor},
}

@Article{Giard2016a,
  author   = {P. Giard and G. Sarkis and C. Thibeault and W. J. Gross},
  title    = {Multi-Mode Unrolled Architectures for Polar Decoders},
  journal  = {IEEE Transactions on Circuits and Systems (TCS)},
  year     = {2016},
  volume   = {63},
  number   = {9},
  pages    = {1443--1453},
  month    = sep,
  issn     = {1549-8328},
  abstract = {In this work, we present a family of architectures for polar decoders using a reduced-complexity successive-cancellation decoding algorithm that employs unrolling to achieve extremely high throughput values while retaining moderate implementation complexity. The resulting fully-unrolled, deeply-pipelined architecture is capable of achieving a coded throughput in excess of 1 Tbps on a 65 nm ASIC at 500 MHz-three orders of magnitude greater than current state-of-the-art polar decoders. However, unrolled decoders are built for a specific, fixed code. Therefore we also present a new method to enable the use of multiple code lengths and rates in a fully-unrolled polar decoder architecture. This method leads to a length- and rate-flexible decoder while retaining the very high speed typical to unrolled decoders. The resulting decoders can decode a master polar code of a given rate and length, and several shorter codes of different rates and lengths. We present results for two versions of a multi-mode decoder supporting eight and ten different polar codes, respectively. Both are capable of a peak throughput of 25.6 Gbps. For each decoder, the energy efficiency for the longest supported polar code is shown to be of 14.8 pJ/bit at 250 MHz and of 8.8 pJ/bit at 500 MHz.},
  doi      = {10.1109/TCSI.2016.2586218},
  file     = {:pdf/Giard2016a - Multi-Mode Unrolled Architectures for Polar Decoders.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {application specific integrated circuits, decoding, energy conservation, low-power electronics, ASIC, code lengths, coded throughput, energy efficiency, frequency 250 MHz, frequency 500 MHz, fully-unrolled polar decoder architecture, length-flexible decoder, master polar code, multimode decoder, polar decoders, rate-flexible decoder, reduced-complexity successive-cancellation decoding algorithm, size 65 nm, unrolled decoders, Computer architecture, Encoding, Hardware, Maximum likelihood decoding, Systematics, Throughput, ASIC, high throughput, multi-mode, polar codes, unrolled architecture},
}

@InProceedings{Berrou1993,
  author    = {C. Berrou and A. Glavieux and P. Thitimajshima},
  title     = {Near {Shannon} Limit Error-Correcting Coding and Decoding: Turbo-Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {1993},
  volume    = {2},
  pages     = {1064--1070},
  month     = may,
  publisher = {IEEE},
  abstract  = {A new class of convolutional codes called turbo-codes, whose performances in terms of bit error rate (BER) are close to the Shannon limit, is discussed. The turbo-code encoder is built using a parallel concatenation of two recursive systematic convolutional codes, and the associated decoder, using a feedback decoding rule, is implemented as P pipelined identical elementary decoders},
  doi       = {10.1109/ICC.1993.397441},
  file      = {:pdf/Berrou1993 - Near Shannon Limit Error-Correcting Coding and Decoding\: Turbo-Codes.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {codecs, concatenated codes, convolutional codes, decoding, error correction codes, error statistics, feedback, pipeline processing, Shannon limit, bit error rate, decoder, encoder, feedback decoding rule, parallel concatenation, pipelined identical elementary decoders, recursive systematic convolutional codes, turbo-codes, Bit error rate, Convolutional codes, Decoding, Digital communication, Digital integrated circuits, Equations, Europe, Feedback, Laboratories, Turbo codes},
}

@InProceedings{Balatsoukas-Stimming2015a,
  author    = {A. Balatsoukas-Stimming and M. Bastani Parizi and A. Burg},
  title     = {On Metric Sorting for Successive Cancellation List Decoding of Polar Codes},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2015},
  pages     = {1993--1996},
  month     = may,
  publisher = {IEEE},
  abstract  = {We focus on the metric sorter unit of successive cancellation list decoders for polar codes, which lies on the critical path in all current hardware implementations of the decoder. We review existing metric sorter architectures and we propose two new architectures that exploit the structure of the path metrics in a log-likelihood ratio based formulation of successive cancellation list decoding. Our synthesis results show that, for the list size of L = 32, our first proposed sorter is 14\% faster and 45\% smaller than existing sorters, while for smaller list sizes, our second sorter has a higher delay in return for up to 36\% reduction in the area.},
  doi       = {10.1109/ISCAS.2015.7169066},
  file      = {:pdf/Balatsoukas-Stimming2015a - On Metric Sorting for Successive Cancellation List Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {0271-4302},
  keywords  = {channel coding, maximum likelihood decoding, log-likelihood ratio, metric sorter unit architecture, path metrics, polar codes, successive cancellation list decoder, successive cancellation list decoding, Computer architecture, Decoding, Delays, Hardware, Indexes, Sorting},
}

@Article{Bahl1974,
  author   = {L. Bahl and J. Cocke and F. Jelinek and J. Raviv},
  title    = {Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate (Corresp.)},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1974},
  volume   = {20},
  number   = {2},
  pages    = {284--287},
  month    = mar,
  issn     = {0018-9448},
  abstract = {The general problem of estimating the a posteriori probabilities of the states and transitions of a Markov source observed through a discrete memoryless channel is considered. The decoding of linear block and convolutional codes to minimize symbol error probability is shown to be a special case of this problem. An optimal decoding algorithm is derived.},
  doi      = {10.1109/TIT.1974.1055186},
  file     = {:pdf/Bahl1974 - Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate (Corresp.).pdf:PDF},
  keywords = {Convolutional codes, Decoding, Estimation, Linear codes, Markov processes, Convolutional codes, Error analysis, Error probability, Feedback, Linear code, Maximum likelihood decoding, Memoryless systems, Reflective binary codes, State estimation, Viterbi algorithm},
}

@Article{Miloslavskaya2015,
  author   = {V. Miloslavskaya},
  title    = {Shortened Polar Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {2015},
  volume   = {61},
  number   = {9},
  pages    = {4852--4865},
  month    = sep,
  issn     = {0018-9448},
  abstract = {An optimization algorithm for finding a shortening pattern and a set of frozen symbols for polar codes is proposed. The structure of polar codes is exploited to eliminate many equivalent shortening patterns, thus reducing the search space. A reduced-complexity suboptimal algorithm is proposed for finding shortening patterns for long polar codes. Shortened codes obtained with the proposed method are shown to outperform low-density parity-check (LDPC) codes.},
  doi      = {10.1109/TIT.2015.2453312},
  file     = {:pdf/Miloslavskaya2015 - Shortened Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {decoding, optimisation, low-density parity-check codes, optimization algorithm, polar codes, reduced-complexity suboptimal algorithm, sequential decoding, successive cancellation decoding, Arrays, Decoding, Error probability, Heuristic algorithms, Iron, Linear codes, Optimization, Polar codes, sequential decoding, shortening, successive cancellation decoding},
}

@InProceedings{LeGal2014,
  author    = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title     = {Software Polar Decoder on an Embedded Processor},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2014},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This paper presents the software implementation of a Polar Codes decoder on an embedded processor. An efficient use of computation and memory resource is made in order to devise a fast polar decoder on an embedded ARM processor. Memory footprint reduction and algorithmic simplifications are applied in order to increase the throughput of the decoder. The NEON instruction set of ARM processors is used to exploit the parallelism of the algorithm. The resulting decoder description is implemented on a Cortex A9 ARM processor. The throughput of the resulting decoder is reported and discussed for several parameters : the code rate, the code length and the multithreading mode. To the best of our knowledge, this is the first reported implementation of a polar decoder on an embedded processor core. The proposed software decoder reaches $>$100Mbps for a codelength of 16K. Moreover, it compares favorably with state of the art LDPC decoders implemented on embedded processors.},
  doi       = {10.1109/SiPS.2014.6986083},
  file      = {:pdf/LeGal2014 - Software Polar Decoder on an Embedded Processor.pdf:PDF},
  groups    = {Polar Codes, Software Decoders, HoF Polar - SC},
  issn      = {2162-3562},
  keywords  = {codecs, embedded systems, information theory, instruction sets, parity check codes, Cortex A9 ARM processor, LDPC decoders, NEON instruction set, algorithmic simplifications, code length, code rate, embedded ARM processor, memory footprint reduction, multithreading mode, polar codes decoder, software implementation, software polar decoder, Decoding, Multicore processing, Parallel processing, Parity check codes, Software, Software algorithms, Throughput},
}

@Article{Puschel2005,
  author   = {M. Puschel and J. M. F. Moura and J. R. Johnson and D. Padua and M. M. Veloso and B. W. Singer and Jianxin Xiong and F. Franchetti and A. Gacic and Y. Voronenko and K. Chen and R. W. Johnson and N. Rizzolo},
  title    = {{SPIRAL}: Code Generation for {DSP} Transforms},
  journal  = {Proceedings of the IEEE},
  year     = {2005},
  volume   = {93},
  number   = {2},
  pages    = {232--275},
  month    = feb,
  issn     = {0018-9219},
  abstract = {Fast changing, increasingly complex, and diverse computing platforms pose central problems in scientific computing: How to achieve, with reasonable effort, portable optimal performance? We present SPIRAL, which considers this problem for the performance-critical domain of linear digital signal processing (DSP) transforms. For a specified transform, SPIRAL automatically generates high-performance code that is tuned to the given platform. SPIRAL formulates the tuning as an optimization problem and exploits the domain-specific mathematical structure of transform algorithms to implement a feedback-driven optimizer. Similar to a human expert, for a specified transform, SPIRAL "intelligently" generates and explores algorithmic and implementation choices to find the best match to the computer's microarchitecture. The "intelligence" is provided by search and learning techniques that exploit the structure of the algorithm and implementation space to guide the exploration and optimization. SPIRAL generates high-performance code for a broad set of DSP transforms, including the discrete Fourier transform, other trigonometric transforms, filter transforms, and discrete wavelet transforms. Experimental results show that the code generated by SPIRAL competes with, and sometimes outperforms, the best available human tuned transform library code.},
  doi      = {10.1109/JPROC.2004.840306},
  file     = {:pdf/Puschel2005 - SPIRAL\: Code Generation for DSP Transforms.pdf:PDF},
  keywords = {discrete Fourier transforms, discrete wavelet transforms, mathematics computing, optimising compilers, signal processing, DSP transforms, SPIRAL, code generation, computer microarchitecture, discrete Fourier transform, discrete wavelet transforms, diverse computing platforms, domain specific mathematical structure, feedback driven optimizer, filter transforms, human tuned transform library code, learning methods, linear digital signal processing transforms, optimization, scientific computing, search methods, transform algorithms, trigonometric transforms, Digital signal processing, Discrete Fourier transforms, Discrete wavelet transforms, Fourier transforms, Humans, Microarchitecture, Portable computers, Scientific computing, Signal processing algorithms, Spirals, Adaptation, Markov decision process, automatic performance tuning, code optimization, discrete Fourier transform (DFT), discrete cosine transform (DCT), fast Fourier transform (FFT), filter, genetic and evolutionary algorithm, high-performance computing, learning, library generation, linear signal transform, search, wavelet},
}

@Article{Arikan2011,
  author   = {E. Arikan},
  title    = {Systematic Polar Coding},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2011},
  volume   = {15},
  number   = {8},
  pages    = {860--862},
  month    = aug,
  issn     = {1089-7798},
  abstract = {Polar codes were originally introduced as a class of non-systematic linear block codes. This paper gives encoding and decoding methods for systematic polar coding that preserve the low-complexity nature of non-systematic polar coding while guaranteeing the same frame error rate. Simulation results are given to show that systematic polar coding offers significant advantages in terms of bit error rate performance.},
  doi      = {10.1109/LCOMM.2011.061611.110862},
  file     = {:pdf/Arikan2011 - Systematic Polar Coding.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {binary codes, decoding, error statistics, linear codes, bit error rate performance, decoding methods, encoding methods, frame error rate, nonsystematic linear block codes, nonsystematic polar coding, systematic polar coding, Binary phase shift keying, Bit error rate, Channel coding, Complexity theory, Decoding, Systematics, Polar codes, error propagation, successive cancellation decoding, systematic polar codes},
}

@InProceedings{Zhang2014,
  author    = {Y. Zhang and Z. Xing and L. Yuan and C. Liu and Q. Wang},
  title     = {The Acceleration of Turbo Decoder on the Newest {GPGPU} of {Kepler} Architecture},
  booktitle = {International Symposium on Communications and Information Technologies (ISCIT)},
  year      = {2014},
  pages     = {199--203},
  month     = sep,
  publisher = {IEEE},
  abstract  = {In the paper, a new implementation of a 3GPP LTE standards compliant turbo decoder based on GPGPU is proposed. It uses the newest GPU-Tesla K20c, which is based on the Kepler GK110 architecture. The new architecture has more powerful parallel computing capability and we use it to fully exploit the parallelism in the turbo decoding algorithm in novel ways. Meanwhile, we use various memory hierarchies to meet various kinds of data demands on speed and capacity. Simulation shows that our implementation is practical and it gets 76\% improvement on throughput over the latest GPU implementation. The result demonstrates that the newest Kepler architecture is suitable for turbo decoding and it can be a promising reconfigurable platform for the communication system.},
  doi       = {10.1109/ISCIT.2014.7011900},
  file      = {:pdf/Zhang2014 - The Acceleration of Turbo Decoder on the Newest GPGPU of Kepler Architecture.pdf:PDF},
  groups    = {Turbo Codes, Software Decoders, HoF Turbo - MAP},
  keywords  = {3G mobile communication, Long Term Evolution, codecs, decoding, graphics processing units, parallel architectures, turbo codes, 3GPP LTE standards, GPGPU, GPU-Tesla K20c, Kepler GK110 architecture, Kepler architecture, general purpose graphic processing units, turbo decoder acceleration, turbo decoding algorithm, Bit error rate, Computer architecture, Decoding, Graphics processing units, Kernel, Parallel processing, Throughput, GPGPU, Kepler, Max-Log-Map Algorithm, Parallel, Turbo Decoder},
}

@Article{Frigo2005,
  author   = {M. Frigo and S. G. Johnson},
  title    = {The Design and Implementation of {FFTW3}},
  journal  = {Proceedings of the IEEE},
  year     = {2005},
  volume   = {93},
  number   = {2},
  pages    = {216--231},
  month    = feb,
  issn     = {0018-9219},
  abstract = {FFTW is an implementation of the discrete Fourier transform (DFT) that adapts to the hardware in order to maximize performance. This paper shows that such an approach can yield an implementation that is competitive with hand-optimized libraries, and describes the software structure that makes our current FFTW3 version flexible and adaptive. We further discuss a new algorithm for real-data DFTs of prime size, a new way of implementing DFTs by means of machine-specific single-instruction, multiple-data (SIMD) instructions, and how a special-purpose compiler can derive optimized implementations of the discrete cosine and sine transforms automatically from a DFT algorithm.},
  doi      = {10.1109/JPROC.2004.840301},
  file     = {:pdf/Frigo2005 - The Design and Implementation of FFTW3.pdf:PDF},
  groups   = {Single Instruction Multiple Data (SIMD)},
  keywords = {discrete Fourier transforms, discrete cosine transforms, mathematics computing, optimising compilers, parallel programming, software libraries, DFT algorithm, FFTW3 design, FFTW3 version, cosine transforms, discrete Fourier transform, hand optimized libraries, machine specific single instruction, multiple data instructions, sine transforms, software structure, Data structures, Discrete Fourier transforms, Discrete cosine transforms, Discrete transforms, Fast Fourier transforms, Fourier transforms, Hardware, Multidimensional systems, Optimizing compilers, Software libraries, Adaptive software, Fourier transform, Hartley transform, I/O tensor, cosine transform, fast Fourier transform (FFT)},
}

@InProceedings{Rodriguez2017,
  author    = {V. Q. Rodriguez and F. Guillemin},
  title     = {Towards the Deployment of a Fully Centralized Cloud-{RAN} Architecture},
  booktitle = {International Wireless Communications and Mobile Computing Conference (IWCMC)},
  year      = {2017},
  pages     = {1055--1060},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In the framework of Network Function Virtualization (NFV), we address in this work the design and sizing of Cloud-RAN architectures. We concretely investigate the execution time of software-based Base Band Units (BBUs) on multi-core systems. Since Cloud-RAN requires a real-time behavior, we use parallel programming techniques in order to minimize the runtime of BBU functions. For an efficient utilization of computing resources, we investigate the relevance of resource pooling where a global scheduling algorithm allocates processing units to runnable BBU-jobs. We specifically examine the gain that can be obtained when applying data parallelism on the channel decoding BBU-function which is the most expensive one in terms of processing time. Performance results show a significant reduction in the runtime of PHY functions which enables the deployment of a fully centralized Cloud-RAN architecture.},
  doi       = {10.1109/IWCMC.2017.7986431},
  file      = {:pdf/Rodriguez2017 - Towards the Deployment of a Fully Centralized Cloud-RAN Architecture.pdf:PDF},
  groups    = {Cloud-RAN},
  keywords  = {channel coding, cloud computing, multiprocessing systems, parallel programming, processor scheduling, radio access networks, telecommunication computing, virtualisation, NFV, PHY functions, channel decoding BBU-function, cloud-RAN architecture, global scheduling algorithm, multicore systems, network function virtualization, parallel programming techniques, software-based base band units, Antennas, Cloud computing, Computer architecture, Decoding, Long Term Evolution, Parallel processing, Runtime, BBU, Cloud-RAN, NFV, VNF, channel decoding, global scheduling, multi-core},
}

@Article{LeGal2017a,
  author   = {B. {Le Gal} and C. Leroux and C. J\'ego},
  title    = {High-Performance Software Implementation of {SCAN} Decoders for Polar codes},
  journal  = {Springer Annals of Telecommunications},
  year     = {2018},
  volume   = {73},
  number   = {5},
  pages    = {401--412},
  month    = {Jun},
  issn     = {1958-9395},
  abstract = {This paper presents the first optimized software implementation of a SCAN decoder for Polar codes. Unlike SC and SC-List decoding algorithms, the SCAN decoding algorithm provides soft outputs (useful for, e.g., parallel concatenated decoders Zhang et al. IEEE Trans Commun 64(2):456--466 2016). Despite the strong data dependencies in the SCAN decoding, two highly parallel software implementations are devised for x86 processor target. Different parallelization strategies, algorithmic improvements, and source code optimizations were applied in order to enhance the throughput of the decoders. The impact of the parallelization approach, the code rate, and the code length on the throughput and the latency is investigated. Extensive experimentations demonstrate that the proposed software polar decoder can exceed 600 Mb/s on a single core and reaches multi-Gb/s when using four cores simultaneously. These decoders can then achieve real-time performance required in many applications such as software defined radio or cloud-RAN systems where network physical layer is implemented in software.},
  day      = {01},
  doi      = {10.1007/s12243-018-0634-7},
  file     = {:pdf/LeGal2018 - High-Performance Software Implementation of SCAN Decoders for Polar Codes.pdf:PDF},
  groups   = {HoF Polar - SCAN, Software Decoders, Polar Codes},
}

@Article{Falcao2008,
  author   = {G. Falcao and V. Silva and L. Sousa and J. Marinho},
  title    = {High Coded Data Rate and Multicodeword {WiMAX} {LDPC} Decoding on Cell/{BE}},
  journal  = {IET Electronics Letters},
  year     = {2008},
  volume   = {44},
  number   = {24},
  pages    = {1415--1416},
  month    = nov,
  issn     = {0013-5194},
  abstract = {A novel, flexible and scalable parallel LDPC decoding approach for the WiMAX wireless broadband standard (IEEE 802.16e) in the multicore Cell broadband engine architecture is proposed. A multicodeword LDPC decoder performing the simultaneous decoding of 96 codewords is presented. The coded data rate achieved a range of 72-80-Mbit/s, which compares well with VLSI-based decoders and is superior to the maximum coded data rate required by the WiMAX standard performing in worst case conditions. The 8-bit precision arithmetic adopted shows additional advantages over traditional 6-bit precision dedicated VLSI-based solutions, allowing better error floors and BER performance.},
  doi      = {10.1049/el:20081927},
  file     = {:pdf/Falcao2008 - High Coded Data Rate and Multicodeword WiMAX LDPC Decoding on Cell BE.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {WiMax, broadband networks, decoding, error statistics, microprocessor chips, parity check codes, 8-bit precision arithmetic, BER performance, WiMAX wireless broadband standard, multicodeword LDPC decoding, multicore Cell broadband engine architecture},
}

@InProceedings{Wang2008,
  author    = {S. Wang and S. Cheng and Q. Wu},
  title     = {A Parallel Decoding Algorithm of {LDPC} Codes Using {CUDA}},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2008},
  pages     = {171--175},
  month     = oct,
  publisher = {IEEE},
  abstract  = {A parallel belief propagation algorithm for decoding low-density parity-check (LDPC) codes is presented in this paper based on Compute Unified Device Architecture (CUDA). As a new hardware and software architecture for addressing and managing computations, CUDA offers parallel data computing using the highly multithreaded coprocessor driven by very high memory bandwidth GPU. The parallel decoding algorithm, based on CUDA, allows that all bit-nodes or check-nodes work simultaneously, thus provides an efficient and fast way for implementing the decoder.},
  doi       = {10.1109/ACSSC.2008.5074385},
  file      = {:pdf/Wang2008 - A Parallel Decoding Algorithm of LDPC Codes Using CUDA.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1058-6393},
  keywords  = {belief networks, coprocessors, decoding, multi-threading, parity check codes, telecommunication computing, CUDA, Compute Unified Device Architecture, LDPC codes, bit-nodes, check-nodes, graphics processor unit, high memory bandwidth GPU, low-density parity-check codes, multithreaded coprocessor, parallel belief propagation algorithm, parallel data computing, parallel decoding algorithm, Bandwidth, Belief propagation, Computer architecture, Concurrent computing, Coprocessors, Decoding, Hardware, Memory management, Parity check codes, Software architecture},
}

@Article{Falcao2009,
  author    = {G. Falcão and S. Yamagiwa and V. Silva and L. Sousa},
  title     = {Parallel {LDPC} Decoding on {GPUs} Using a Stream-Based Computing Approach},
  journal   = {Springer Journal of Computer Science and Technology (JCST)},
  year      = {2009},
  volume    = {24},
  number    = {5},
  pages     = {913},
  month     = sep,
  issn      = {1860-4749},
  abstract  = {Low-Density Parity-Check (LDPC) codes are powerful error correcting codes adopted by recent communication standards. LDPC decoders are based on belief propagation algorithms, which make use of a Tanner graph and very intensive message-passing computation, and usually require hardware-based dedicated solutions. With the exponential increase of the computational power of commodity graphics processing units (GPUs), new opportunities have arisen to develop general purpose processing on GPUs. This paper proposes the use of GPUs for implementing flexible and programmable LDPC decoders. A new stream-based approach is proposed, based on compact data structures to represent the Tanner graph. It is shown that such a challenging application for stream-based computing, because of irregular memory access patterns, memory bandwidth and recursive flow control constraints, can be efficiently implemented on GPUs. The proposal was experimentally evaluated by programming LDPC decoders on GPUs using the Caravela platform, a generic interface tool for managing the kernels' execution regardless of the GPU manufacturer and operating system. Moreover, to relatively assess the obtained results, we have also implemented LDPC decoders on general purpose processors with Streaming Single Instruction Multiple Data (SIMD) Extensions. Experimental results show that the solution proposed here efficiently decodes several codewords simultaneously, reducing the processing time by one order of magnitude.},
  date      = {2009-09-01},
  doi       = {10.1007/s11390-009-9266-8},
  file      = {:pdf/Falcao2009 - Parallel LDPC Decoding on GPUs Using a Stream-Based Computing Approach.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  publisher = {Springer},
}

@InProceedings{Zhao2011,
  author    = {J. Zhao and M. Zhao and H. Yang and J. Chen and X. Chen and J. Wang},
  title     = {High Performance {LDPC} Decoder on Cell {BE} for {WiMAX} System},
  booktitle = {International Conference on Communications and Mobile Computing (CMC)},
  year      = {2011},
  pages     = {278--281},
  month     = apr,
  publisher = {IEEE},
  abstract  = {Low-density parity-check (LDPC) codes are widely used in many radio systems due to its superior performance and Software Defined Radio (SDR) is an emerging paradigm of the wireless communication system design due to its good flexibility and adaptability. However, LDPC decoding is computationally intensive and the implementation of it on software in a parallel way is quite challenging due to the characteristics of LDPC decoding and structures of software. In this paper, we presented an efficient software implementation of the LDPC decoder using the Parallel Code Block Decoding mode for the WiMAX SDR base band system on IBM CELL Broadband Engine (BE). With a single Synergistic Processor Element (SPE) running at 3.2GHz, the implemented LDPC decoder can achieve a throughput up to 1.71 Mbps. With eight SPEs working in parallel, the decoder can obtain the throughput more than 13Mbps, which can meet the WiMAX system requirement at 5MHz bandwidth mode.},
  doi       = {10.1109/CMC.2011.117},
  file      = {:pdf/Zhao2011 - High Performance LDPC Decoder on CELL BE for WiMAX System.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {WiMax, block codes, codecs, microprocessor chips, parity check codes, software radio, CELL BE, IBM CELL, LDPC decoder, WiMAX System, bandwidth 5 MHz, broadband engine, frequency 3.2 GHz, low density parity check codes, parallel code block decoding mode, software defined radio, software implementation, synergistic processor element, Baseband, Computer architecture, Decoding, Microprocessors, Parity check codes, Throughput, WiMAX, CELL BE, LDPC decoder, Software Defined Radio, WiMAX},
}

@Article{Chang2011,
  author   = {C. C. Chang and Y. L. Chang and M. Y. Huang and B. Huang},
  title    = {Accelerating Regular {LDPC} Code Decoders on {GPUs}},
  journal  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (J-STARS)},
  year     = {2011},
  volume   = {4},
  number   = {3},
  pages    = {653--659},
  month    = sep,
  issn     = {1939-1404},
  abstract = {Modern active and passive satellite and airborne sensors with higher temporal, spectral and spatial resolutions for Earth remote sensing result in a significant increase in data volume. This poses a challenge for data transmission over error-prone wireless links to a ground receiving station. Low-density parity-check (LDPC) codes have been adopted in modern communication systems for robust error correction. Demands for LDPC decoders at a ground receiving station for efficient and flexible data communication links have inspired the usage of a cost-effective high-performance computing device. In this paper we propose a graphic-processing-unit (GPU)-based regular LDPC decoders with the log sum-product iterative decoding algorithm (log-SPA). The GPU code was written to run NVIDIA GPUs using the compute unified device architecture (CUDA) language with a novel implementation of asynchronous data transfer for LDPC decoding. Experimental results showed that the proposed GPU-based high-throughput regular LDPC decoder achieved a significant 271x speedup compared to its CPU-based single-threaded counterpart written in the C language.},
  doi      = {10.1109/JSTARS.2011.2142295},
  file     = {:pdf/Chang2011 - Accelerating Regular LDPC Code Decoders on GPUs.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {computer graphic equipment, coprocessors, iterative decoding, parallel architectures, parity check codes, remote sensing, satellite communication, LDPC code decoders, NVIDIA GPU, airborne sensors, compute unified device architecture, earth remote sensing, error prone wireless links, flexible data communication links, graphic processing unit, ground receiving station, log sum product iterative decoding algorithm, low density parity check codes, satellite sensors, Computer architecture, Decoding, Encoding, Graphics processing unit, Instruction sets, Iterative decoding, Compute unified device architecture (CUDA), graphic processing unit (GPU), log sum-product algorithm (log-SPA), low-density parity-check (LDPC), regular LDPC, remote sensing, satellite communication links},
}

@Article{Falcao2011,
  author   = {G. Falcao and J. Andrade and V. Silva and L. Sousa},
  title    = {{GPU}-Based {DVB-S}2 {LDPC} Decoder with High Throughput and Fast Error Floor Detection},
  journal  = {IET Electronics Letters},
  year     = {2011},
  volume   = {47},
  number   = {9},
  pages    = {542--543},
  month    = apr,
  issn     = {0013-5194},
  abstract = {A new strategy is proposed for implementing computationally intensive high-throughput decoders based on the long length irregular LDPC codes adopted in the DVB-S2 standard. It is supported on manycore graphics processing unit (GPU) architectures, for performing parallel multi-threaded decoding of multiple codewords with reduced accesses to global memory. This novel approach is flexible and scalable, and achieves throughputs superior to the 90 Mbit/s required by the DVB-S2 standard, while at the same time it improves error-correcting performances such as BER and error floors regarding conventional VLSI-based decoders.},
  doi      = {10.1049/el.2011.0201},
  file     = {:pdf/Falcao2011 - GPU-Based DVB-S2 LDPC Decoder with High Throughput and Fast Error Floor Detection.pdf:PDF;:pdf/Falcao2011a - Massively LDPC Decoding on Multicore Architectures.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  keywords = {decoding, digital video broadcasting, direct broadcasting by satellite, error correction codes, error statistics, parity check codes, GPU-based DVB-S2 LDPC decoder, VLSI-based decoders, bit error rate, error correcting codes, error floor detection, global memory, graphics processing unit, low density parity check codes, parallel multi-threaded decoding},
}

@Article{Falcao2011a,
  author   = {G. Falcao and L. Sousa and V. Silva},
  title    = {Massively {LDPC} Decoding on Multicore Architectures},
  journal  = {IEEE Transactions on Parallel and Distributed Systems (TPDS)},
  year     = {2011},
  volume   = {22},
  number   = {2},
  pages    = {309--322},
  month    = feb,
  issn     = {1045-9219},
  abstract = {Unlike usual VLSI approaches necessary for the computation of intensive Low-Density Parity-Check (LDPC) code decoders, this paper presents flexible software-based LDPC decoders. Algorithms and data structures suitable for parallel computing are proposed in this paper to perform LDPC decoding on multicore architectures. To evaluate the efficiency of the proposed parallel algorithms, LDPC decoders were developed on recent multicores, such as off-the-shelf general-purpose x86 processors, Graphics Processing Units (GPUs), and the CELL Broadband Engine (CELL/B.E.). Challenging restrictions, such as memory access conflicts, latency, coalescence, or unknown behavior of thread and block schedulers, were unraveled and worked out. Experimental results for different code lengths show throughputs in the order of 1 ~ 2 Mbps on the general-purpose multicores, and ranging from 40 Mbps on the GPU to nearly 70 Mbps on the CELL/B.E. The analysis of the obtained results allows to conclude that the CELL/B.E. performs better for short to medium length codes, while the GPU achieves superior throughputs with larger codes. They achieve throughputs that in some cases approach very well those obtained with VLSI decoders. From the analysis of the results, we can predict a throughput increase with the rise of the number of cores.},
  doi      = {10.1109/TPDS.2010.66},
  file     = {:pdf/Falcao2011a - Massively LDPC Decoding on Multicore Architectures.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {computer graphic equipment, coprocessors, multiprocessing systems, parallel algorithms, Cell broadband engine, graphics processing units, low-density parity-check code, multicore architectures, parallel algorithms, software-based LDPC decoders, x86 processors, Computer architecture, Data structures, Decoding, Graphics, Multicore processing, Parallel algorithms, Parallel processing, Parity check codes, Throughput, Very large scale integration, CELL, CUDA, GPU, LDPC, OpenMP., data-parallel computing, graphics processing units, multicore},
}

@Article{Ji2011,
  author    = {H. Ji and J. Cho and W. Sung},
  title     = {Memory Access Optimized Implementation of Cyclic and Quasi-Cyclic {LDPC} Codes on a {GPGPU}},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2011},
  volume    = {64},
  number    = {1},
  pages     = {149},
  month     = jul,
  abstract  = {Software based decoding of low-density parity-check (LDPC) codes frequently takes very long time, thus the general purpose graphics processing units (GPGPUs) that support massively parallel processing can be very useful for speeding up the simulation. In LDPC decoding, the parity-check matrix H needs to be accessed at every node updating process, and the size of the matrix is often larger than that of GPU on-chip memory especially when the code length is long or the weight is high. In this work, the parity-check matrix of cyclic or quasi-cyclic (QC) LDPC codes is greatly compressed by exploiting the periodic property of the matrix. Also, vacant elements are eliminated from the sparse message arrays to utilize the coalesced access of global memory supported by GPGPUs. Regular projective geometry (PG) and irregular QC LDPC codes are used for sum-product algorithm based decoding with the GTX-285 NVIDIA graphics processing unit (GPU), and considerable speed-up results are obtained.},
  date      = {2011-07-01},
  doi       = {10.1007/s11265-010-0547-9},
  file      = {:pdf/Ji2011 - Memory Access Optimized Implementation of Cyclic and Quasi-Cyclic LDPC Codes on a GPGPU.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  publisher = {Springer},
}

@InProceedings{Wang2011,
  author    = {G. Wang and M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {A Massively Parallel Implementation of {QC-LDPC} Decoder on {GPU}},
  booktitle = {Symposium on Application Specific Processors (SASP)},
  year      = {2011},
  pages     = {82--85},
  month     = jun,
  publisher = {IEEE},
  abstract  = {The graphics processor unit (GPU) is able to provide a low-cost and flexible software-based multi-core architecture for high performance computing. However, it is still very challenging to efficiently map the real-world applications to GPU and fully utilize the computational power of GPU. As a case study, we present a GPU-based implementation of a real-world digital signal processing (DSP) application: low-density parity-check (LDPC) decoder. The paper shows the efforts we made to map the algorithm onto the massively parallel architecture of GPU and fully utilize GPU's computational resources to significantly boost the performance. Moreover, several efficient data structures have been proposed to reduce the memory access latency and the memory bandwidth requirement. Experimental results show that the proposed GPU-based LDPC decoding accelerator can take advantage of the multi-core computational power provided by GPU and achieve high throughput up to 100.3Mbps.},
  doi       = {10.1109/SASP.2011.5941084},
  file      = {:pdf/Wang2011 - A Massively Parallel Implementation of QC-LDPC Decoder on GPU.pdf:PDF;:pdf/Wang2011a - GPU Accelerated Scalable Parallel Decoding of LDPC Codes.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {computer graphic equipment, coprocessors, data structures, decoding, multiprocessing systems, parity check codes, signal processing, QC-LDPC decoder, data structures, digital signal processing application, graphics processor unit, high performance computing, low density parity check decoder, massively parallel implementation, memory access latency, memory bandwidth requirement, software based multicore architecture, Decoding, Graphics processing unit, IEEE 802.11n Standard, Instruction sets, Message systems, Parity check codes, Throughput, CUDA, GPU, LDPC decoder, parallel computing},
}

@InProceedings{Wang2011a,
  author    = {G. Wang and M. Wu and Y. Sun and J. R. Cavallaro},
  title     = {{GPU} Accelerated Scalable Parallel Decoding of {LDPC} Codes},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2011},
  pages     = {2053--2057},
  month     = nov,
  publisher = {IEEE},
  abstract  = {This paper proposes a flexible low-density parity-check (LDPC) decoder which leverages graphic processor units (GPU) to provide high decoding throughput. LDPC codes are widely adopted by the new emerging standards for wireless communication systems and storage applications due to their near-capacity error correcting performance. To achieve high decoding throughput on GPU, we leverage the parallelism embedded in the check-node computation and variable-node computation and propose a parallel strategy of partitioning the decoding jobs among multi-processors in GPU. In addition, we propose a scalable multi-codeword decoding scheme to fully utilize the computation resources of GPU. Furthermore, we developed a novel adaptive performance-tuning method to make our decoder implementation more flexible and scalable. The experimental results show that our LDPC decoder is scalable and flexible, and the adaptive performance-tuning method can deliver the peak performance based on the GPU architecture.},
  doi       = {10.1109/ACSSC.2011.6190388},
  file      = {:pdf/Wang2011a - GPU Accelerated Scalable Parallel Decoding of LDPC Codes.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1058-6393},
  keywords  = {decoding, error correction codes, graphics processing units, parity check codes, GPU architecture, LDPC codes, accelerated scalable parallel decoding, check-node computation, graphic processor units, low-density parity-check decoder, multiprocessors, near-capacity error correcting performance, scalable multicodeword decoding, variable-node computation, wireless communication, Decoding, Graphics processing unit, Instruction sets, Kernel, Parity check codes, Performance evaluation, Throughput, GPGPU, adaptive performance-tuning, parallel LDPC decoder, reconfigurable and scalable algorithms},
}

@Article{Falcao2012,
  author   = {G. Falcao and V. Silva and L. Sousa and J. Andrade},
  title    = {Portable {LDPC} Decoding on Multicores Using {OpenCL}},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2012},
  volume   = {29},
  number   = {4},
  pages    = {81--109},
  month    = jul,
  issn     = {1053-5888},
  abstract = {This article proposes to address, in a tutorial style, the benefits of using Open Computing Language [1] (OpenCL) as a quick way to allow programmers to express and exploit parallelism in signal processing algorithms, such as those used in error-correcting code systems. In particular, we will show how multiplatform kernels can be developed straightforwardly using OpenCL to perform computationally intensive low-density parity-check (LDPC) decoding, targeting them to run on a large set of worldwide disseminated multicore architectures, such as x86 general purpose multicore central processing units (CPUs) and graphics processing units (GPUs). Moreover, devices with different architectures can be orchestrated to cooperatively execute these signal processing applications programmed in OpenCL. Experimental evaluation of the parallel kernels programmed with the OpenCL framework shows that high-performance can be achieved for distinct parallel computing architectures with low programming effort.},
  doi      = {10.1109/MSP.2012.2192212},
  file     = {:pdf/Falcao2012 - Portable LDPC Decoding on Multicores Using OpenCL.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, parallel architectures, parity check codes, signal processing, CPU, GPU, Open Computing Language, OpenCL framework, error-correcting code systems, graphics processing units, low density parity check decoding, multicore central processing unit, multicores, multiplatform kernels, parallel computing architecture, parallel kernels, portable LDPC decoding, signal processing, Error correction codes, Multicore processing, Parallel processing, Signal processing algorithms, Tutorials},
}

@InProceedings{Kang2012,
  author    = {S. Kang and J. Moon},
  title     = {Parallel {LDPC} Decoder Implementation on {GPU} Based on Unbalanced Memory Coalescing},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2012},
  pages     = {3692--3697},
  month     = jun,
  publisher = {IEEE},
  abstract  = {We consider flexible decoder implementation of low density parity check (LDPC) codes via compute-unified-device-architecture (CUDA) programming on graphics processing unit (GPU), a research subject of considerable recent interest. An important issue in LDPC decoder design based on CUDA-GPU is realizing coalesced memory access, a technique that reduces memory transaction time considerably. In previous works along this direction, it has not been possible to achieve coalesced memory access in both the read and write operations due to the asymmetric nature of the bipartite graph describing the LDPC code structure. In this paper, a new algorithm is proposed that enables coalesced memory access in both the read and write operations for one half of the decoding process - either the bit-to-check or the check-to-bit message passing. For the remaining half of the decoding step our scheme requires address transformation in both the read and write operations but one translating array is sufficient. We also describe the use of on-chip shared memory and texture cache. Overall, experimental results show that proposed GPU-based LDPC decoder achieves more than 234$\times$-speedup compared to CPU-based LDPC decoders and also outperforms existing GPU-based decoders by a significant margin.},
  doi       = {10.1109/ICC.2012.6363991},
  file      = {:pdf/Kang2012 - Parallel LDPC Decoder Implementation on GPU Based on Unbalanced Memory Coalescing.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1550-3607},
  keywords  = {decoding, graph theory, graphics processing units, message passing, parallel architectures, parity check codes, CUDA-GPU decoder, LDPC code structure, bipartite graph, bit-to-check message passing, check-to-bit message passing, coalesced memory access, compute-unified-device-architecture programming, decoding process, graphics processing unit, low density parity check codes, memory transaction time reduction, on-chip shared memory, parallel LDPC decoder design, texture cache, unbalanced memory coalescing, Arrays, Decoding, Error analysis, Graphics processing units, Memory management, Message passing, Parity check codes},
}

@Article{Gronroos2012,
  author    = {S. Gr\"onroos and K. Nybom and J. Bj\"orkqvist},
  title     = {Efficient {GPU} and {CPU}-Based {LDPC} Decoders for Long Codewords},
  journal   = {Springer Journal of Analog Integrated Circuits and Signal Processing (AICSP)},
  year      = {2012},
  volume    = {73},
  number    = {2},
  pages     = {583},
  month     = nov,
  abstract  = {The next generation DVB-T2, DVB-S2, and DVB-C2 standards for digital television broadcasting specify the use of low-density parity-check (LDPC) codes with codeword lengths of up to 64800 bits. The real-time decoding of these codes on general purpose computing hardware is useful for completely software defined receivers, as well as for testing and simulation purposes. Modern graphics processing units (GPUs) are capable of massively parallel computation, and can in some cases, given carefully designed algorithms, outperform general purpose CPUs (central processing units) by an order of magnitude or more. The main problem in decoding LDPC codes on GPU hardware is that LDPC decoding generates irregular memory accesses, which tend to carry heavy performance penalties (in terms of efficiency) on GPUs. Memory accesses can be efficiently parallelized by decoding several codewords in parallel, as well as by using appropriate data structures. In this article we present the algorithms and data structures used to make log-domain decoding of the long LDPC codes specified by the DVB-T2 standard—at the high data rates required for television broadcasting—possible on a modern GPU. Furthermore, we also describe a similar decoder implemented on a general purpose CPU, and show that high performance LDPC decoders are also possible on modern multi-core CPUs.},
  date      = {2012-11-01},
  doi       = {10.1007/s10470-012-9895-7},
  file      = {:pdf/Gronroos2012 - Efficient GPU and CPU-Based LDPC Decoders for Long Codewords.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  publisher = {Springer},
}

@InProceedings{Pan2013,
  author    = {Xia Pan and Xiao-fan Lu and Ming-qi Li and Rong-fang Song},
  title     = {A High Throughput {LDPC} Decoder in {CMMB} Based on Virtual Radio},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2013},
  pages     = {95--99},
  month     = apr,
  publisher = {IEEE},
  abstract  = {LDPC (Low Density Parity Check) is widely used in many telecommunication systems due to its excellent performance. However, real-time LDPC decoder in virtual radio system is difficult to realize. Taking CMMB (China Mobile Multimedia Broadcasting) for instance, this paper proposes a method to achieve high throughput decoder based on x86 processors which support SIMD (Single Instruction Multiple Data) instructions. By utilizing Normalized Min Sum decoding algorithm, normalized parameter as well as bit width of input variables and intermediate variables are determined. Then taking advantages of SIMD instructions, updating progress of variable nodes and check nodes in LDPC decoding algorithm is parallelized. Meanwhile, memory access operations are optimized as well. Tested on Intel Core i7-3960X, the throughput of the LDPC decoder using multithread processing can reach 92Mbps ~ 722Mbps.},
  doi       = {10.1109/WCNCW.2013.6533323},
  file      = {:pdf/Pan2013 - A High Throughput LDPC Decoder in CMMB Based on Virtual Radio.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {decoding, mobile radio, multimedia communication, parity check codes, radio broadcasting, CMMB, China mobile multimedia broadcasting, Intel Core i7-3960X, SIMD instructions, check nodes, high throughput LDPC decoder, high throughput low density parity check decoder, memory access operations, multithread processing, normalized min sum decoding algorithm, real-time LDPC decoder, single instruction multiple data instructions, telecommunication systems, variable nodes, virtual radio, x86 processors, Broadcasting, Decoding, Instruction sets, Iterative decoding, Signal processing algorithms, Throughput, CMMB, LDPC, Multithreading, SIMD, Virtual Radio, fixed-point},
}

@InProceedings{Han2013,
  author    = {X. Han and K. Niu and Z. He},
  title     = {Implementation of {IEEE} 802.11n {LDPC} Codes Based on General Purpose Processors},
  booktitle = {International Conference on Communication Technology (ICCT)},
  year      = {2013},
  pages     = {218--222},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Recently, General-purpose processor (GPP) soft defined radio (SDR) platforms have drawn great attention for their programmability and flexibility, and some high-speed wireless protocol stacks (e.g., IEEE 802.11a/b/g) have been implemented on them using commodity general-purpose PCs. Low-density parity-check (LDPC) codes are optionally used in IEEE 802.11n high throughput (HT) system as a high-performance error correcting code instead of convolutional codes for the near Shannon limit performance. In order to complete the implementation of IEEE 802.11n on SDR platforms, this paper presents the encoding and decoding of IEEE 802.11n LDPC codes on GPPs. We extensively use the features of contemporary processor architectures to accelerate data processing, including large low-latency caches to store lookup tables and SIMD processing on GPPs. Layered decoding is used in this paper, which can significantly reduce the number of iterations and is well suited to using SIMD instructions. The implementation results show that the throughput can meet the protocol timing requirement under the performance premise.},
  doi       = {10.1109/ICCT.2013.6820375},
  file      = {:pdf/Han2013 - Implementation of IEEE 802.11n LDPC Codes Based on General Purpose Processors.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {cache storage, microprocessor chips, parallel processing, parity check codes, software radio, table lookup, wireless LAN, IEEE 802.11n, LDPC codes, SIMD processing, data processing acceleration, general purpose processors, lookup table cache, low density parity check codes, processor architecture, soft defined radio platform, Decoding, IEEE 802.11n Standard, Iterative decoding, Parallel processing, Throughput, Vectors, General-purpose processor (GPP), IEEE 802.11n, Low-density parity-check (LDPC) codes, layered decoding},
}

@InProceedings{Gronroos2013,
  author    = {S. Grönroos and J. Björkqvist},
  title     = {Performance Evaluation of {LDPC} Decoding on a General Purpose Mobile {CPU}},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2013},
  pages     = {1278--1281},
  month     = dec,
  abstract  = {This paper explores using a mobile platform for performing the calculations required for the building blocks of telecommunication systems. The building block analyzed in this paper is LDPC (low-density parity-check) channel decoding, performed on the LDPC design used in the DVB-T2 standard. Implementation details are given, and a performance analysis on a mobile CPU is performed. The implementation is compared against a very similar implementation running on a desktop computer CPU, as well as a GPU (graphics processing unit) implementation. The results give indications of the current state of typical mobile platforms of today.},
  doi       = {10.1109/GlobalSIP.2013.6737142},
  file      = {:pdf/Gronroos2013 - Performance Evaluation of LDPC Decoding on a General Purpose Mobile CPU.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes, DVB},
  keywords  = {channel coding, graphics processing units, mobile computing, parity check codes, performance evaluation, LDPC decoding, building block, channel decoding, general purpose mobile CPU, graphics processing unit, low density parity check, mobile CPU, mobile platform, performance evaluation, telecommunication systems, Decoding, Digital video broadcasting, Graphics processing units, Mobile communication, Parity check codes, Standards, Throughput, ARM, LDPC, NEON, SDR},
}

@InProceedings{Li2013,
  author    = {R. Li and J. Zhou and Y. Dou and S. Guo and D. Zou and S. Wang},
  title     = {A Multi-Standard Efficient Column-Layered {LDPC} Decoder for Software Defined Radio on {GPUs}},
  booktitle = {International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)},
  year      = {2013},
  pages     = {724--728},
  month     = jun,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a multi-standard high-throughput column-layered (CL) low-density parity-check (LDPC) decoder for Software-Defined Radio (SDR) on a Graphics Processing Unit (GPU) platform. Multiple columns in the sub-matrix of quasi-cyclic LDPC (QC-LDPC) code are parallel performed inside a block, while multiple codewords are simultaneously decoded among many blocks on the GPU. Several optimization methods are employed to enhance the throughput, such as the compressed matrix structure, memory optimization, codeword packing scheme, two-dimension thread configuration and asynchronous data transfer. The experiment shows that our decoder has low bit error ratio and the peak throughput is 712Mbps, which is about two orders of magnitude faster than that of CPU implementation and comparable to the dedicated hardware solutions. Compared to the existing fastest GPU-based implementation, the presented decoder can achieve a performance improvement of 3.0x times.},
  doi       = {10.1109/SPAWC.2013.6612145},
  file      = {:pdf/Li2013 - A Multi-Standard Efficient Column-Layered LDPC Decoder for Software Defined Radio on GPUs.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  issn      = {1948-3244},
  keywords  = {cyclic codes, error statistics, graphics processing units, optimisation, parity check codes, software radio, GPU, LDPC decoder, QC-LDPC code, SDR, asynchronous data transfer, bit error ratio, codeword packing scheme, column-layered decoder, compressed matrix structure, graphics processing unit, low-density parity-check decoder, memory optimization, multiple codewords, multistandard decoder, quasicyclic LDPC code, software defined radio, two-dimension thread configuration, Decoding, Graphics processing units, Iterative decoding, Message systems, Registers, Throughput, GPU, LDPC Decoder, SDR, column-layered decoding},
}

@Article{Lin2014a,
  author   = {Y. Lin and W. Niu},
  title    = {High Throughput {LDPC} Decoder on {GPU}},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2014},
  volume   = {18},
  number   = {2},
  pages    = {344--347},
  month    = feb,
  issn     = {1089-7798},
  abstract = {The available Lower Density Parity Check (LDPC) decoders on Graphics Processing Unit (GPU) do not simultaneously read and write contiguous data blocks in memory because of the random nature of LDPC codes. One of these two operations has to be performed using noncontiguous accesses, resulting in long access time. To overcome this issue, we designed a multi-codeword parallel decoder with fully coalesced memory access. To test the performance of the method, we applied the method using an 8-bit compact data. The experimental results demonstrated that the method achieved more than 550Mbps throughput on Compute Unified Device Architecture (CUDA) enabled GPU.},
  doi      = {10.1109/LCOMM.2014.010214.132406},
  file     = {:pdf/Lin2014a - High Throughput LDPC Decoder on GPU.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, graphics processing units, parity check codes, GPU, compute unified device architecture, contiguous data blocks, graphics processing unit, high throughput LDPC decoder, long access time, lower density parity check decoders, Decoding, Graphics processing units, Message systems, Parity check codes, Performance evaluation, Sparse matrices, Throughput, CUDA, GPU, LDPC code, decoding, parallel processing},
}

@Article{LeGal2014a,
  author   = {B. {Le Gal} and C. J\'ego and J. Crenne},
  title    = {A High Throughput Efficient Approach for Decoding {LDPC} Codes onto {GPU} Devices},
  journal  = {IEEE Embedded Systems Letters (ESL)},
  year     = {2014},
  volume   = {6},
  number   = {2},
  pages    = {29--32},
  month    = jun,
  issn     = {1943-0663},
  abstract = {Low density parity check (LDPC) decoding process is known as compute intensive. This kind of digital communication applications was recently implemented onto graphic processing unit (GPU) devices for LDPC code performance estimation and/or for real-time measurements. Overall previous studies about LDPC decoding on GPU were based on the implementation of the flooding-based decoding algorithm that provides massive computation parallelism. More efficient layered schedules were proposed in literature because decoder iteration can be split into sublayer iterations. These schedules seem to badly fit onto GPU devices due to restricted computation parallelism and complex memory access patterns. However, the layered schedules enable the decoding convergence to speed up by two. In this letter, we show that: 1) layered schedule can be efficiently implemented onto a GPU device; and 2) this approach-implemented onto a low-cost GPU device-provides higher throughputs with identical correction performances (BER) compared to previously published results.},
  doi      = {10.1109/LES.2014.2311317},
  file     = {:pdf/LeGal2014a - A High Throughput Efficient Approach for Decoding LDPC Codes onto GPU Devices.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {graphics processing units, parity check codes, BER, GPU devices, LDPC code performance estimation, LDPC decoding process, bit error rate, computation parallelism, correction performance, decoder iteration, digital communication applications, flooding-based decoding algorithm, graphics processing unit, high throughput efficient approach, layered schedules, low density parity check codes, memory access patterns, Decoding, Graphics processing units, Iterative decoding, Kernel, Performance evaluation, Throughput, Graphic processing unit (GPU), layered-based algorithm, low density parity check (LDPC), throughput optimized},
}

@Article{Lai2016,
  author   = {B. C. C. Lai and C. Y. Lee and T. H. Chiu and H. K. Kuo and C. K. Chang},
  title    = {Unified Designs for High Performance {LDPC} Decoding on {GPGPU}},
  journal  = {IEEE Transactions on Computers (TC)},
  year     = {2016},
  volume   = {65},
  number   = {12},
  pages    = {3754--3765},
  month    = dec,
  issn     = {0018-9340},
  abstract = {Modern GPGPU's have enabled massively parallel computing with programmability that can exploit the highly parallel nature of LDPC decoding. Previous works customized the design on a GPGPU towards specific execution attributes of a particular LDPC decoding matrix. Supporting different LDPC decoding matrices requires either substantial rework on the current program, or a brand new parallel design. This paper proposes two unified designs that can achieve high performance for both regular and irregular LDPC decoding on a GPGPU. The first design introduces a node-based scheme with a versatile translation array mechanism that can efficiently handle the complex data access patterns of different LDPC decoding matrices. The second design proposes an edge-based parallel paradigm that uses more intuitive data layout. More edges than nodes in a Tanner graph also give the edge-based design higher computation parallelism when there are limited concurrent codewords. With the proposed unified designs, designers can be ignorant of the types of LDPC matrices and achieve high performance LDPC decoding. The experiments on a GTX 470 GPGPU have demonstrated up to 134.56x runtime improvement, when compared with designs on a high-end CPU. The maximum throughput can reach 80.25 Mbps. When compared with the previous customized designs, the proposed systematic designs can reach better performance while relieving the effort of customization.},
  doi      = {10.1109/TC.2016.2547379},
  file     = {:pdf/Lai2016 - Unified Designs for High Performance LDPC Decoding on GPGPU.pdf:PDF},
  groups   = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords = {decoding, graphics processing units, matrix algebra, parity check codes, GTX 470 GPGPU, complex data access patterns, decoding matrices, edge-based design, edge-based parallel paradigm, general purpose graphic processing units, high performance LDPC decoding, irregular decoding, low-density parity-check decoding, node-based scheme, parallel design, regular decoding, versatile translation array mechanism, Computer architecture, Decoding, Iterative decoding, Message passing, Message systems, Parallel processing, C.4 performance of systems, D.2.2 design tools and techniques},
}

@Article{Debbabi2016a,
  author   = {I. Debbabi and {B. Le Gal} and N. Khouja and F. Tlili and C. J\'ego},
  title    = {Real Time LP Decoding of {LDPC} Codes for High Correction Performance Applications},
  journal  = {IEEE Wireless Communications Letters (WCL)},
  year     = {2016},
  volume   = {5},
  number   = {6},
  pages    = {676--679},
  month    = dec,
  issn     = {2162-2337},
  abstract = {The alternate direction method of multipliers is a recent linear programming error correcting approach which improves the decoding performance of LDPC codes compared with the best BP decoding techniques. In this letter, an efficient implementation of the ADMM LP decoding algorithm on a multicore architecture is presented. Its throughput performance level is about one order of magnitude higher than related works on the same multicore target. The proposed decoder's throughput reaches up to 100 Mb/s which makes it viable for real time applications with tough error correction requirements.},
  doi      = {10.1109/LWC.2016.2615304},
  file     = {:pdf/Debbabi2016a - Real Time LP Decoding of LDPC Codes for High Correction Performance Applications.pdf:PDF;},
  groups   = {HoF LDPC - LP, Software Decoders, LDPC Codes},
  keywords = {decoding, error correction codes, linear programming, parity check codes, ADMM LP decoding algorithm, LDPC code, alternate direction method of multiplier, high correction performance application, linear programming error correcting approach, multicore architecture, real time LP decoding, throughput performance level, Complexity theory, Decoding, Iterative decoding, Software algorithms, Throughput, ADMM, LDPC code, LP, SIMD, SIMT, layered scheduling, multicore, software optimization, throughput},
}

@InProceedings{Esterie2012,
  author    = {P. Est\'erie and M. Gaunard and J. Falcou and J. T. Laprest\'e and B. Rozoy},
  title     = {Boost.{SIMD}: Generic programming for portable SIMDization},
  booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  year      = {2012},
  pages     = {431--432},
  month     = sep,
  publisher = {ACM/IEEE},
  abstract  = {SIMD extensions have been a feature of choice for processor manufacturers for a couple of decades. Designed to exploit data parallelism in applications at the instruction level and provide significant accelerations, these extensions still require a high level of expertise or the use of potentially fragile compiler support or vendor-specific libraries. In this poster, we present Boost.SIMD, a C++ template library that simplifies the exploitation of SIMD hardware within a standard C++ programming model.},
  file      = {:pdf/Esterie2012 - Boost.SIMD\: Generic programming for portable SIMDization.pdf:PDF;:pdf/Esterie2012a - Exploiting Multimedia Extensions in C++\: A Portable Approach.pdf:PDF},
  groups    = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords  = {C++ language, parallel processing, software libraries, Boost.SIMD, C++ programming model, C++ template library, SIMD hardware, data parallelism, portable SIMDization, C++ languages, Computer architecture, Libraries, Parallel processing, Programming, Registers, Standards, C++, Generic Programming, SIMD, Template Metaprogramming},
  doi       = {10.1145/2370816.2370881},
}

@Article{Esterie2012a,
  author   = {P. Est\'erie and M. Gaunard and J. Falcou and J. T. Laprest\'e},
  title    = {Exploiting Multimedia Extensions in {C++}: A Portable Approach},
  journal  = {IEEE Computing in Science \& Engineering (CS\&E)},
  year     = {2012},
  volume   = {14},
  number   = {5},
  pages    = {72--77},
  month    = sep,
  issn     = {1521-9615},
  abstract = {Single instruction, multiple data (SIMD) extensions have been a feature of choice for processor manufacturers for a couple of decades. Designed to provide significant accelerations, they require expertise, the use of potentially fragile compiler support, or vendor-specific libraries. Here, a C++ template library called Boost.SIMD is presented that simplifies the exploitation of SIMD hardware within a standing C++ programming model.},
  doi      = {10.1109/MCSE.2012.96},
  file     = {:pdf/Esterie2012a - Exploiting Multimedia Extensions in C++\: A Portable Approach.pdf:PDF},
  groups   = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords = {C++ language, multimedia systems, parallel processing, program compilers, software libraries, Boost, C++ programming model, C++ template library, SIMD extensions, SIMD hardware, compiler support, multimedia extensions, single instruction multiple data extensions, vendor-specific libraries, Computational modeling, Hardware, Instruction sets, Programming, Scientific computing, C\&+\&+, SIMD hardware, computational science, scientific computing, scientific programming},
}

@Article{Inoue2015,
  author     = {H. Inoue and K. Taura},
  title      = {{SIMD-} and Cache-friendly Algorithm for Sorting an Array of Structures},
  journal    = {Proceedings of the VLDB Endowment (PVLDB)},
  year       = {2015},
  volume     = {8},
  number     = {11},
  pages      = {1274--1285},
  month      = jul,
  issn       = {2150-8097},
  acmid      = {2809988},
  doi        = {10.14778/2809974.2809988},
  file       = {:pdf/Inoue2015 - SIMD- and Cache-friendly Algorithm for Sorting an Array of Structures.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD), Sort},
  issue_date = {July 2015},
  numpages   = {12},
  publisher  = {VLDB Endowment},
}

@Article{Chhugani2008,
  author     = {J. Chhugani and A. D. Nguyen and V. W. Lee and W. Macy and M. Hagog and Y-K. Chen and A. Baransi and S. Kumar and P. Dubey},
  title      = {Efficient Implementation of Sorting on Multi-core {SIMD} {CPU} Architecture},
  journal    = {Proceedings of the VLDB Endowment (PVLDB)},
  year       = {2008},
  volume     = {1},
  number     = {2},
  pages      = {1313--1324},
  month      = aug,
  issn       = {2150-8097},
  acmid      = {1454171},
  doi        = {10.14778/1454159.1454171},
  file       = {:pdf/Chhugani2008 - Efficient Implementation of Sorting on Multi-core SIMD CPU Architecture.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD), Sort},
  issue_date = {August 2008},
  numpages   = {12},
  publisher  = {VLDB Endowment},
}

@Article{Kschischang2001,
  author   = {F. R. Kschischang and B. J. Frey and H. A. Loeliger},
  title    = {Factor Graphs and the Sum-Product Algorithm},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2001},
  volume   = {47},
  number   = {2},
  pages    = {498--519},
  month    = feb,
  issn     = {0018-9448},
  abstract = {Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of {\textquotedblleft}local{\textquotedblright} functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative {\textquotedblleft}turbo{\textquotedblright} decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms},
  doi      = {10.1109/18.910572},
  file     = {:pdf/Kschischang2001 - Factor Graphs and the Sum-Product Algorithm.pdf:PDF},
  groups   = {Factor Graphs},
  keywords = {Kalman filters, Viterbi decoding, artificial intelligence, belief networks, digital communication, fast Fourier transforms, functional analysis, graph theory, hidden Markov models, iterative decoding, message passing, signal processing, turbo codes, Bayesian networks, FFT algorithms, HMM, Kalman filter, Viterbi algorithm, artificial intelligence, belief propagation algorithm, bipartite graph, computational rule, digital communications, factor graphs, factorization, fast Fourier transform, forward/backward algorithm, generic message-passing algorithm, global function, global functions, iterative turbo decoding algorithm, local functions, marginal functions, signal processing, sum-product algorithm, Artificial intelligence, Bipartite graph, Digital communication, Digital signal processing, Iterative algorithms, Iterative decoding, Signal processing algorithms, Sum product algorithm, Visualization, Viterbi algorithm},
}

@Article{Loeliger2004,
  author   = {H. A. Loeliger},
  title    = {An Introduction to Factor Graphs},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2004},
  volume   = {21},
  number   = {1},
  pages    = {28--41},
  month    = jan,
  issn     = {1053-5888},
  abstract = {Graphical models such as factor graphs allow a unified approach to a number of key topics in coding and signal processing such as the iterative decoding of turbo codes, LDPC codes and similar codes, joint decoding, equalization, parameter estimation, hidden-Markov models, Kalman filtering, and recursive least squares. Graphical models can represent complex real-world systems, and such representations help to derive practical detection/estimation algorithms in a wide area of applications. Most known signal processing techniques -including gradient methods, Kalman filtering, and particle methods -can be used as components of such algorithms. Other than most of the previous literature, we have used Forney-style factor graphs, which support hierarchical modeling and are compatible with standard block diagrams.},
  doi      = {10.1109/MSP.2004.1267047},
  file     = {:pdf/Loeliger2004 - An Introduction to Factor Graphs (Colors Edition).pdf:PDF;:pdf/Loeliger2004 - An Introduction to Factor Graphs.pdf:PDF},
  groups   = {Factor Graphs},
  keywords = {Kalman filters, error correction codes, gradient methods, graph theory, hidden Markov models, iterative decoding, least squares approximations, parity check codes, recursive estimation, signal processing, turbo codes, Forney-style factor graphs, Kalman filtering, LDPC codes, error-correcting codes, gradient methods, hidden-Markov models, joint decoding, parameter estimation, particle methods, recursive least squares, signal processing techniques, turbo codes, Filtering, Graphical models, Iterative algorithms, Iterative decoding, Kalman filters, Least squares approximation, Parameter estimation, Parity check codes, Signal processing algorithms, Turbo codes},
}

@Article{Fayyaz2014,
  author   = {U. U. Fayyaz and J. R. Barry},
  title    = {Low-Complexity Soft-Output Decoding of Polar Codes},
  journal  = {IEEE Journal on Selected Areas in Communications (JSAC)},
  year     = {2014},
  volume   = {32},
  number   = {5},
  pages    = {958--966},
  month    = may,
  issn     = {0733-8716},
  abstract = {The state-of-the-art soft-output decoder for polar codes is a message-passing algorithm based on belief propagation, which performs well at the cost of high processing and storage requirements. In this paper, we propose a low-complexity alternative for soft-output decoding of polar codes that offers better performance but with significantly reduced processing and storage requirements. In particular we show that the complexity of the proposed decoder is only 4\% of the total complexity of the belief propagation decoder for a rate one-half polar code of dimension 4096 in the dicode channel, while achieving comparable error-rate performance. Furthermore, we show that the proposed decoder requires about 39\% of the memory required by the belief propagation decoder for a block length of 32768.},
  doi      = {10.1109/JSAC.2014.140515},
  file     = {:pdf/Fayyaz2014 - Low-Complexity Soft-Output Decoding of Polar Codes.pdf:PDF},
  groups   = {Factor Graphs, Polar Codes},
  keywords = {decoding, error statistics, message passing, belief propagation decoder, dicode channel, error rate performance, low-complexity soft-output decoding, message passing algorithm, polar codes, Belief propagation, Complexity theory, Decoding, Iterative decoding, Memory management, Receivers, Polar codes, soft-output decoding, turbo equalization, SCAN},
}

@PhdThesis{Benaddi2015,
  author   = {T. Benaddi},
  title    = {Sparse Graph-Based Coding Schemes for Continuous Phase Modulations},
  school   = {Institut National Polytechnique de Toulouse},
  year     = {2015},
  month    = {December},
  abstract = {The use of the continuous phase modulation (CPM) is interesting when the channel represents a strong non-linearity and in the case of limited spectral support; particularly for the uplink, where the satellite holds an amplifier per carrier, and for downlinks where the terminal equipment works very close to the saturation region. Numerous studies have been conducted on this issue but the proposed solutions use iterative CPM demodulation/decoding concatenated with convolutional or block error correcting codes. The use of LDPC codes has not yet been introduced. Particularly, no works, to our knowledge, have been done on the optimization of sparse graph-based codes adapted for the context described here. In this study, we propose to perform the asymptotic analysis and the design of turbo-CPM systems based on the optimization of sparse graph-based codes. Moreover, an analysis on the corresponding receiver will be done.},
  file     = {:pdf/Benaddi2015 - Sparse Graph-Based Coding Schemes for Continuous Phase Modulations.pdf:PDF},
  groups   = {Turbo Codes, LDPC Codes},
  keywords = {CPM,Code design,Based codes,Turbo detection,Sparse graph,ML/MAP receivers,EXIT chart,Tarik},
  url      = {http://oatao.univ-toulouse.fr/16037/},
}

@Misc{Terboven2014,
  author = {C. Terboven and M. Klemm and E. Stotzer and B. R. {de Supinski}},
  title  = {Advanced {OpenMP} Tutorial},
  year   = {2014},
  note   = {International Supercomputing Conference},
  file   = {:pdf/Terboven2014 - Advanced OpenMP Tutorial.pdf:PDF},
  url    = {https://sharepoint.ecampus.rwth-aachen.de/units/rz/HPC/public/Shared Documents/2015_isc_openmp/isc15-advanced-openmp.pdf}
}

@Article{Kretz2012,
  author    = {M. Kretz and V. Lindenstruth},
  title     = {Vc: A {C++} Library for Explicit Vectorization},
  journal   = {Software: Practice and Experience},
  year      = {2012},
  volume    = {42},
  number    = {11},
  pages     = {1409--1430},
  doi       = {10.1002/spe.1149},
  file      = {:pdf/Kretz2012 - Vc\: A C++ library for explicit vectorization.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  publisher = {Wiley Online Library},
}

@MastersThesis{Kretz2009,
  author = {M. Kretz},
  title  = {Efficient Use of Multi- and Many-Core Systems with Vectorization and Multithreading},
  school = {University of Heidelberg},
  year   = {2009},
  file   = {:pdf/Kretz2009 - Efficient Use of Multi- and Many-Core Systems with Vectorization and Multithreading.pdf:PDF},
  groups = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
}

@PhdThesis{Kretz2015,
  author   = {M. Kretz},
  title    = {Extending {C++} for Explicit Data-Parallel Programming via {SIMD} Vector Types},
  school   = {Goethe University Frankfurt},
  year     = {2015},
  abstract = {Data-parallel programming is more important than ever since serial performance is stagnating. All mainstream computing architectures have been and are still enhancing their support for general purpose computing with explicitly data-parallel execution. For CPUs, data-parallel execution is implemented via SIMD instructions and registers. GPU hardware works very similar allowing very efficient parallel processing of wide data streams with a common instruction stream.
These advances in parallel hardware have not been accompanied by the necessary advances in established programming languages. Developers have thus not been enabled to explicitly state the data-parallelism inherent in their algorithms. Some approaches of GPU and CPU vendors have introduced new programming languages, language extensions, or dialects enabling explicit data-parallel programming. However, it is arguable whether the programming models introduced by these approaches deliver the best solution. In addition, some of these approaches have shortcomings from a hardware-specific focus of the language design. There are several programming problems for which the aforementioned language approaches are not expressive and flexible enough.
This thesis presents a solution tailored to the C++ programming language. The concepts and interfaces are presented specifically for C++ but as abstract as possible facilitating adoption by other programming languages as well. The approach builds upon the observation that C++ is very expressive in terms of types. Types communicate intention and semantics to developers as well as compilers. It allows developers to clearly state their intentions and allows compilers to optimize via explicitly defined semantics of the type system.
Since data-parallelism affects data structures and algorithms, it is not sufficient to enhance the language's expressivity in only one area. The definition of types whose operators express data-parallel execution automatically enhances the possibilities for building data structures. This thesis therefore defines low-level, but fully portable, arithmetic and mask types required to build a flexible and portable abstraction for data-parallel programming. On top of these, it presents higher-level abstractions such as fixed-width vectors and masks, abstractions for interfacing with containers of scalar types, and an approach for automated vectorization of structured types.
The Vc library is an implementation of these types. I developed the Vc library for researching data-parallel types and as a solution for explicitly data-parallel programming. This thesis discusses a few example applications using the Vc library showing the real-world relevance of the library. The Vc types enable parallelization of search algorithms and data structures in a way unique to this solution. It shows the importance of using the type system for expressing data-parallelism. Vc has also become an important building block in the high energy physics community. Their reliance on Vc shows that the library and its interfaces were developed to production quality.},
  file     = {:pdf/Kretz2015 - Extending C++ for Explicit Data-Parallel Programming via SIMD Vector Types.pdf:PDF},
  groups   = {C++, Single Instruction Multiple Data (SIMD), Wrapper},
  keywords = {C++, SIMD, data parallel, parallel programming, vectorization},
}

@Online{Kanapickas,
  author   = {P. Kanapickas},
  title    = {libsimdpp},
  year     = {2017},
  abstract = {libsimdpp is a portable header-only zero-overhead C++ wrapper around single-instruction multiple-data (SIMD) intrinsics found in many compilers. The library presents a single interface over several instruction sets in such a way that the same source code may be compiled for different instruction sets. The resulting object files then may be hooked into internal dynamic dispatch mechanism.

The library resolves differences between instruction sets by implementing the missing functionality as a combination of several intrinsics. Moreover, the library supplies a lot of additional, commonly used functionality, such as various variants of matrix transpositions, interleaving loads/stores, optimized compile-time shuffling instructions, etc. Each of these are implemented in the most efficient manner for the target instruction set. Finally, it's possible to fall back to native intrinsics when necessary, without compromising maintanability.

The library sits somewhere in the middle between programming directly in intrinsics and even higher-level SIMD libraries. As much control as possible is given to the developer, so that it's possible to exactly predict what code the compiler will generate.},
  groups   = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  url      = {https://github.com/p12tic/libsimdpp},
}

@Online{Mabille,
  author   = {J. Mabille},
  title    = {xsimd},
  year     = {2017},
  abstract = {SIMD (Single Instruction, Multiple Data) is a feature of microprocessors that has been available for many years. SIMD instructions perform a single operation on a batch of values at once, and thus provide a way to significantly accelerate code execution. However, these instructions differ between microprocessor vendors and compilers.

xsimd provides a unified means for using these features for library authors. Namely, it enables manipulation of batches of numbers with the same arithmetic operators as for single values. It also provides accelerated implementation of common mathematical functions operating on batches.

You can find out more about this implementation of C++ wrappers for SIMD intrinsics at the The C++ Scientist. The mathematical functions are a lightweight implementation of the algorithms used in boost.SIMD.},
  groups   = {Single Instruction Multiple Data (SIMD), C++, Wrapper},
  url      = {https://github.com/xtensor-stack/xsimd},
}

@Online{Cassagne,
  author   = {A. Cassagne},
  title    = {My Intrinsics++ ({MIPP})},
  year     = {2013},
  abstract = {MIPP is a portable and Open-source wrapper (MIT license) for vector intrinsic functions (SIMD) written in C++11. It works for SSE, AVX, AVX512 and ARM NEON instructions. MIPP wrapper supports simple/double precision floating-point numbers and also signed integer arithmetic (32-bit, 16-bit and 8-bit).

With the MIPP wrapper you do not need to write a specific intrinsic code anymore. Just use provided functions and the wrapper will automatically generates the right intrisic calls for your specific architecture.},
  groups   = {Single Instruction Multiple Data (SIMD), Wrapper, C++},
  url      = {https://github.com/aff3ct/MIPP},
  keywords = {Cassagne},
}

@Online{Fog,
  author   = {A. Fog},
  title    = {{C++} Vector Class Library ({VCL})},
  year     = {2017},
  abstract = {This is a collection of C++ classes, functions and operators that makes it easier to use the the vector instructions (Single Instruction Multiple Data instructions) of modern CPUs without using assembly language. Supports the SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX512, FMA, and XOP instruction sets. Includes standard mathematical functions. Can compile for different instruction sets from the same source code.},
  file     = {:pdf/Fog - C++ Vector Class library (VCL).pdf:PDF},
  groups   = {Wrapper, C++},
  url      = {http://www.agner.org/optimize/#vectorclass},
}

@Online{West,
  author   = {N. West},
  title    = {The Vector Optimized Library of Kernels ({VOLK})},
  month    = {Jul},
  year     = {2016},
  abstract = {VOLK is the Vector-Optimized Library of Kernels. It is a free library, currently offered under the GPLv3, that contains kernels of hand-written SIMD code for different mathematical operations. Since each SIMD architecture can be very different and no compiler has yet come along to handle vectorization properly or highly efficiently, VOLK approaches the problem differently.

For each architecture or platform that a developer wishes to vectorize for, a new proto-kernel is added to VOLK. At runtime, VOLK will select the correct proto-kernel. In this way, the users of VOLK call a kernel for performing the operation that is platform/architecture agnostic. This allows us to write portable SIMD code that is optimized for a variety of platforms.},
  groups   = {Kernel},
  url      = {http://libvolk.org/},
}

@Article{Parri2011,
  author     = {J. Parri and D. Shapiro and M. Bolic and V. Groza},
  title      = {Returning Control to the Programmer: {SIMD} Intrinsics for Virtual Machines},
  journal    = {Communications of the ACM},
  year       = {2011},
  volume     = {54},
  number     = {4},
  pages      = {38--43},
  month      = apr,
  issn       = {0001-0782},
  abstract   = {Exposing SIMD units within interpreted languages could simplify programs and unleash floods of untapped processor power.

Server an d workstatio n hardware architecture is continually improving, yet interpreted languages—most importantly, Java—have failed to keep pace with the proper utilization of modern processors. SIMD (single instruction, multiple data) units are available.},
  acmid      = {1924437},
  address    = {New York, NY, USA},
  doi        = {10.1145/1924421.1924437},
  file       = {:pdf/Parri2011 - Returning Control to the Programmer\: SIMD Intrinsics for Virtual Machines.pdf:PDF},
  groups     = {Single Instruction Multiple Data (SIMD)},
  issue_date = {April 2011},
  numpages   = {6},
  publisher  = {ACM},
}

@PhdThesis{Marchand2010,
  author   = {C. Marchand},
  title    = {Implementation of an {LDPC} Decoder for the {DVB-S2}, {-T2} and {-C2} Standards},
  school   = {University of Bretagne Sud},
  year     = {2010},
  month    = {Nov},
  abstract = {LDPC codes are, like turbo-codes, able to achieve decoding performance close to the Shannon limit. The performance associated with relatively easy implementation makes this solution very attractive to the digital communication systems. This is the case for the Digital video broadcasting by satellite in the DVB-S2 standard that was the first standard including an LDPC.
This thesis subject is about the optimization of the implementation of an LDPC decoder for the DVB-S2, -T2 and -C2 standards. After a state-of-the-art overview, the layered decoder is chosen as the basis architecture for the decoder implementation. We had to deal with the memory conflicts due to the matrix structure specific to the DVB-S2, -T2, -C2 standards. Two new contributions have been studied to solve the problem. The first is based on the construction of an equivalent matrix and the other relies on the repetition of layers. The conflicts inherent to the pipelined architecture are solved by an efficient scheduling found with the help of graph theories.
Memory size is a major point in term of area and consumption, therefore the reduction to a minimum of this memory is studied. A well defined saturation and an optimum partitioning of memory bank lead to a significant reduction compared to the state-of-the-art. Moreover, the use of single port RAM instead of dual port RAM is studied to reduce memory cost.
In the last chapter we answer to the need of a decoder able to decode in parallel x streams with a reduced cost compared to the use of x decoders.},
  file     = {:pdf/Marchand2010 - Implementation of an LDPC Decoder for the DVB-S2, -T2 and -C2 Standards.pdf:PDF},
  groups   = {LDPC Codes, DVB},
  keywords = {LDPC codes, implementation, memory conflicts, DVB-S2},
  url      = {https://hal.archives-ouvertes.fr/tel-01151985},
}

@Article{Sarkis2016a,
  author   = {G. Sarkis and I. Tal and P. Giard and A. Vardy and C. Thibeault and W. J. Gross},
  title    = {Flexible and Low-Complexity Encoding and Decoding of Systematic Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2016},
  volume   = {64},
  number   = {7},
  pages    = {2732--2745},
  month    = jul,
  issn     = {0090-6778},
  abstract = {In this paper, we present hardware and software implementations of flexible polar systematic encoders and decoders. The proposed implementations operate on polar codes of any length less than a maximum and of any rate. We describe the low-complexity, highly parallel, and flexible systematic-encoding algorithm that we use and prove its correctness. Our hardware implementation results show that the overhead of adding code rate and length flexibility is little, and the impact on operation latency minor compared with code-specific versions. Finally, the flexible software encoder and decoder implementations are also shown to be able to maintain high throughput and low latency.},
  doi      = {10.1109/TCOMM.2016.2574996},
  file     = {:pdf/Sarkis2016a - Flexible and Low-Complexity Encoding and Decoding of Systematic Polar Codes.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, Hardware Decoders},
  keywords = {codes, decoding, code rate, decoding, length flexibility, low-complexity encoding, software decoder, software encoder, systematic encoding algorithm, systematic polar code, Decoding, Encoding, Hardware, Software, Software algorithms, Systematics, Throughput, Polar codes, multi-code decoders, multi-code encoders, systematic encoding},
}

@InProceedings{Pohl2016,
  author    = {A. Pohl and B. Cosenza and M. A. Mesa and C. C. Chi and B. Juurlink},
  title     = {An Evaluation of Current {SIMD} Programming Models for {C++}},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2016},
  pages     = {3:1--3:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {SIMD extensions were added to microprocessors in the mid '90s to speed-up data-parallel code by vectorization. Unfortunately, the SIMD programming model has barely evolved and the most efficient utilization is still obtained with elaborate intrinsics coding. As a consequence, several approaches to write efficient and portable SIMD code have been proposed. In this work, we evaluate current programming models for the C++ language, which claim to simplify SIMD programming while maintaining high performance.

The proposals were assessed by implementing two kernels: one standard floating-point benchmark and one real-world integer-based application, both highly data parallel. Results show that the proposed solutions perform well for the floating point kernel, achieving close to the maximum possible speed-up. For the real-world application, the programming models exhibit significant performance gaps due to data type issues, missing template support and other problems discussed in this paper.},
  acmid     = {2870653},
  articleno = {3},
  doi       = {10.1145/2870650.2870653},
  file      = {:pdf/Pohl2016 - An Evaluation of Current SIMD Programming Models for C++.pdf:PDF},
  groups    = {Wrapper},
  isbn      = {978-1-4503-4060-1},
  keywords  = {C++, SIMD, parallel programming, programming model, vectorization},
  location  = {Barcelona, Spain},
  numpages  = {8},
}

@Misc{OpenMP2013,
  author = {{OpenMP Architecture Review Board}},
  title  = {{OpenMP} Application Program Interface},
  year   = {2013},
  file   = {:pdf/OpenMP2013 - OpenMP Application Program Interface.pdf:PDF},
  groups = {Single Instruction Multiple Data (SIMD)},
  url    = {http://www.openmp.org/mp-documents/OpenMP4.0.0.pdf},
}

@InProceedings{Pharr2012,
  author    = {M. Pharr and W. R. Mark},
  title     = {ispc: A {SPMD} Compiler for High-Performance {CPU} Programming},
  booktitle = {Innovative Parallel Computing (InPar)},
  year      = {2012},
  pages     = {1--13},
  month     = may,
  publisher = {IEEE},
  abstract  = {SIMD parallelism has become an increasingly important mechanism for delivering performance in modern CPUs, due its power efficiency and relatively low cost in die area compared to other forms of parallelism. Unfortunately, languages and compilers for CPUs have not kept up with the hardware's capabilities. Existing CPU parallel programming models focus primarily on multi-core parallelism, neglecting the substantial computational capabilities that are available in CPU SIMD vector units. GPU-oriented languages like OpenCL support SIMD but lack capabilities needed to achieve maximum efficiency on CPUs and suffer from GPU-driven constraints that impair ease of use on CPUs. We have developed a compiler, the Intel R{\textregistered} SPMD Program Compiler (ispc), that delivers very high performance on CPUs thanks to effective use of both multiple processor cores and SIMD vector units. ispc draws from GPU programming languages, which have shown that for many applications the easiest way to program SIMD units is to use a single-program, multiple-data (SPMD) model, with each instance of the program mapped to one SIMD lane. We discuss language features that make ispc easy to adopt and use productively with existing software systems and show that ispc delivers up to 35x speedups on a 4-core system and up to 240$\times$ speedups on a 40-core system for complex workloads (compared to serial C++ code).},
  doi       = {10.1109/InPar.2012.6339601},
  file      = {:pdf/Pharr2012 - ispc\: A SPMD Compiler for High-Performance CPU Programming.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  keywords  = {graphics processing units, multiprocessing systems, parallel programming, program compilers, CPU SIMD vector units, GPU programming languages, GPU-oriented languages, ISPC, Intel R SPMD program compiler, OpenCL, SIMD parallelism, high-performance CPU parallel programming, multicore parallelism, multiple processor cores, single-program multiple-data model, Graphics processing unit, Hardware, Parallel processing, Productivity, Programming, Vectors},
}

@Article{Robison2013,
  author   = {A. D. Robison},
  title    = {Composable Parallel Patterns with {Intel} {Cilk Plus}},
  journal  = {IEEE Computing in Science \& Engineering (CS\&E)},
  year     = {2013},
  volume   = {15},
  number   = {2},
  pages    = {66--71},
  month    = mar,
  issn     = {1521-9615},
  abstract = {Intel Cilk Plus extends C and C++ to enable writing composable deterministic parallel software that can exploit both the thread and vector parallelism commonly available in modern hardware.},
  doi      = {10.1109/MCSE.2013.21},
  file     = {:pdf/Robison2013 - Composable Parallel Patterns with Intel Cilk Plus.pdf:PDF},
  groups   = {Single Instruction Multiple Data (SIMD)},
  keywords = {C++ language, multi-threading, C++, Intel Cilk Plus, composable deterministic parallel software, composable parallel pattern, parallel thread, parallel vector, Message systems, Parallel processing, Program processors, Programming, Scientific computing, Vectors, Intel Cilk Plus, OpenMP, parallel programming, scientific programming, thread parallelism, vector parallelism},
}

@Misc{Howes2015,
  author    = {L. Howes},
  title     = {The {OpenCL} Specification},
  year      = {2015},
  note      = {Version 2.1, Revision 23},
  file      = {:pdf/Howes2015 - The OpenCL Specification.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  publisher = {Khronos Group},
  url       = {https://www.khronos.org/registry/OpenCL/specs/opencl-2.1.pdf},
}

@TechReport{Moller2016,
  author      = {R. M\"oller},
  title       = {Design of a Low-Level {C++} Template {SIMD} Library},
  institution = {Bielefeld University, Faculty of Technology, Computer Engineering Group},
  year        = {2016},
  abstract    = {T-SIMD is a low-level C++ template SIMD library which wraps built-in vector data types and built-in vector intrincics in template classes and template functions or overloaded functions, respectively. Templates parameters are the element data type of the vectors and the vector width in bytes (e.g. 16 for SSE* and NEON, 32 for AVX/AVX2). This makes it possible to flexibly change the data type and the vector instruction set for entire portions of the code. SSE*, AVX/AVX2, and ARM NEON vector instruction sets are currently supported.},
  file        = {:pdf/Moller2016 - Design of a Low-Level C++ Template SIMD Library.pdf:PDF},
  groups      = {Wrapper},
  url         = {http://www.ti.uni-bielefeld.de/html/people/moeller/tsimd_warpingsimd.html},
}

@Online{IntelIntrinsicsGuide,
  author   = {{Intel Corp}},
  title    = {Intel Intrinsics Guide},
  year     = {2017},
  abstract = {The Intel Intrinsics Guide is an interactive reference tool for Intel intrinsic instructions, which are C style functions that provide access to many Intel instructions - including Intel® SSE, AVX, AVX-512, and more - without the need to write assembly code.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://software.intel.com/sites/landingpage/IntrinsicsGuide/},
}

@Online{NeonIntrinsicsGuide,
  author   = {{ARM Ltd}},
  title    = {{NEON} Intrinsics Reference},
  year     = {2017},
  abstract = {The NEON Intrinsics Reference is an interactive reference tool for NEON intrinsic instructions, which are C style functions that provide access to many NEON instructions - including NEON, NEONv2, NEON 64-bit, NEONv2 64-bit, and more - without the need to write assembly code.},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://developer.arm.com/technologies/neon/intrinsics},
}

@Misc{AltivecManual,
  author   = {{Freescale Semiconductor}},
  title    = {{AltiVec} Technology Programming Interface Manual},
  year     = {1999},
  abstract = {The primary objective of this manual is to help programmers to provide software that is
compatible across the family of PowerPC processors using AltiVec technology.
To locate any published errata or updates for this document, refer to the website at
http://www.mot.com/SPS/PowerPC/.
This book is one of two that discuss the AltiVec architecture, the two books are:
- AltiVec: The Programming Interface Manual (AltiVec PIM) is used as a reference
guide for high-level programmers. The AltiVec PIM provides a mechanism for
programmers to access AltiVec functionality from programming languages such as
C and C++. The AltiVec PIM defines a programming model for use with the AltiVec
instruction set extension to the PowerPC architecture.
- AltiVec: The Programming Environments Manual (AltiVec PEM) is used as a
reference guide for assembler programmers (or intrinsics). The AltiVec PEM provides a
description for each instruction that includes the instruction format, an
individualized legend that provides such information as the level(s) of the PowerPC
architecture in which the instruction may be found, the privilege level of the
instruction, and figures to help in understanding how the instruction works.
It is beyond the scope of this manual to describe individual AltiVec technology
implementations on PowerPC processors. It must be kept in mind that each PowerPC
processor is unique in its implementation of the AltiVec technology.
The information in this book is subject to change without notice, as described in the
disclaimers on the title page of this book. As with any technical documentation, it is the
readers’ responsibility to be sure they are using the most recent version of the
documentation. For more information, contact your sales representative or visit our website
at: http://www.mot.com/SPS/PowerPC/. },
  file     = {:pdf/AltivecManual - AltiVec Technology Programming Interface Manual.pdf:PDF},
  groups   = {Single Instruction Multiple Data (SIMD)},
  url      = {https://www.nxp.com/docs/en/reference-manual/ALTIVECPIM.pdf},
}

@Article{Stephens2017,
  author   = {N. Stephens and S. Biles and M. Boettcher and J. Eapen and M. Eyole and G. Gabrielli and M. Horsnell and G. Magklis and A. Martinez and N. Premillieu and A. Reid and A. Rico and P. Walker},
  title    = {The {ARM} Scalable Vector Extension},
  journal  = {IEEE Micro},
  year     = {2017},
  volume   = {37},
  number   = {2},
  pages    = {26--39},
  month    = mar,
  issn     = {0272-1732},
  abstract = {This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.},
  comment  = {Ce papier parle de la nouvelle extension SIMD pou ARM (SVE).

Il est important de noter que cette extension adresse des registres SIMD de 128 bits à 2048 bits.
Le code assembleur est le même quel que soit la taille des registres (il ne contient pas d'information sur la taille des instructions SIMD).

Comme pour AVX-512, il y a des registres de masquage (nommés "predicate").
C'est un peu plus puissant que AVX-512 dans le masquages car il y a des instructions "while" spécifiques qui utilisent des registres predicates (il y a aussi plus de registres predicates en SVE qu'en AVX-512: 16 contre 8).
Ca permet du coup d'améliorer l'auto-vectorisation des codes.
Il y a aussi des mécanismes pour s'arreter si on déborde en mémoire, c'est nottament utile pour vectoriser des codes comme "strlen" où l'on ne connait pas la fin du tableau.

Les performances sont pour le moment assez moyenne mais le taux de vectorisation (auto) du code est très bon par rapport à NEON et c'est prometteur pour la suite.},
  doi      = {10.1109/MM.2017.35},
  file     = {:pdf/Stephens2017 - The ARM Scalable Vector Extension (bis).pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Single Instruction Multiple Data (SIMD)},
  keywords = {program compilers, programming, vectors, ARM AArch64 execution state, ARM scalable vector extension, CPU designers, SVE, area targets, compiler autovectorization technologies, computational requirements, computer vision, data analytics, high-performance computing, machine learning, performance targets, power targets, vector length, vector processing capability, vector-length agnostic programming model, Computer architecture, Encoding, High performance computing, Programming, Scalability, Software engineering, Vectors, ARM, HPC, SIMD, SVE, Scalable Vector Extension, VLA, Vector Length Agnostic, autovectorization, data parallelism, high-performance computing, instruction set architecture, predication, scalable vector architecture, vector length agnostic},
}

@Article{Chen2005,
  author   = {J. Chen and A. Dholakia and E. Eleftheriou and M. P. C. Fossorier and X.-Y. Hu},
  title    = {Reduced-Complexity Decoding of {LDPC} Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2005},
  volume   = {53},
  number   = {8},
  pages    = {1288--1299},
  month    = aug,
  issn     = {0090-6778},
  abstract = {Various log-likelihood-ratio-based belief-propagation (LLR-BP) decoding algorithms and their reduced-complexity derivatives for low-density parity-check (LDPC) codes are presented. Numerically accurate representations of the check-node update computation used in LLR-BP decoding are described. Furthermore, approximate representations of the decoding computations are shown to achieve a reduction in complexity by simplifying the check-node update, or symbol-node update, or both. In particular, two main approaches for simplified check-node updates are presented that are based on the so-called min-sum approximation coupled with either a normalization term or an additive offset term. Density evolution is used to analyze the performance of these decoding algorithms, to determine the optimum values of the key parameters, and to evaluate finite quantization effects. Simulation results show that these reduced-complexity decoding algorithms for LDPC codes achieve a performance very close to that of the BP algorithm. The unified treatment of decoding techniques for LDPC codes presented here provides flexibility in selecting the appropriate scheme from performance, latency, computational-complexity, and memory-requirement perspectives.},
  doi      = {10.1109/TCOMM.2005.852852},
  file     = {:pdf/Chen2005 - Reduced-Complexity Decoding of LDPC Codes.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {approximation theory, computational complexity, iterative decoding, maximum likelihood decoding, parity check codes, quantisation (signal), LDPC codes, LLR-BP decoding, check-node update, computational-complexity, finite quantization effects, log-likelihood-ratio-based belief-propagation decoding, low-density parity-check codes, memory-requirement perspective, min-sum approximation, reduced-complexity decoding, symbol-node update, Algorithm design and analysis, Approximation algorithms, Computational modeling, Delay, Iterative algorithms, Iterative decoding, Parity check codes, Performance analysis, Quantization, Sum product algorithm, Belief-propagation (BP) decoding, density evolution (DE), iterative decoding, low-density parity-check (LDPC) codes, reduced-complexity decoding, complexity},
}

@InProceedings{Leonardon2018a,
  author    = {M. L\'eonardon and C. Leroux and D. Binet and J.M. P. Langlois and C. J\'ego and Y. Savaria},
  title     = {Custom Low Power Processor for Polar Decoding},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2018},
  pages     = {1--5},
  month     = may,
  publisher = {IEEE},
  abstract  = {Cloud Radio Access Network is foreseen as one of the key features of the future 5G mobile communication standard. In this context, all the baseband processing is intended to be performed on CPUs in order to keep a high level of flexibility. The challenge is then to propose efficient software implementations of baseband processing algorithms that guarantee a sufficient throughput, while limiting the energy consumption. In this paper, as an alternative to general purpose processors, we propose an implementation of an Application Specific Instruction set Processor customized for the Successive Cancellation decoding of polar codes. The resulting software decoder achieves throughputs similar to state-of-the-art ARM processor implementations, while reducing the energy consumption by a factor 10.},
  doi       = {10.1109/ISCAS.2018.8351739},
  file      = {:pdf/Leonardon2018a - Custom Low Power Processor for Polar Decoding.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders, AFF3CT, ASIP},
  issn      = {0271-4302},
  keywords  = {5G mobile communication;Decoding;Energy consumption;Hardware;Registers;Software;Throughput},
}

@Misc{ETSI2018,
  author = {ETSI},
  title  = {{3GPP} - {TS} 38.212 - {Multiplexing} and Channel Coding ({R.} 15)},
  month  = aug,
  year   = {2018},
  file   = {:pdf/ETSI2018 - 3GPP - TS 38.212 - Multiplexing and Channel Coding (R. 15).pdf:PDF},
  groups = {5G},
  url    = {https://www.etsi.org/deliver/etsi_ts/138200_138299/138212/15.02.00_60/ts_138212v150200p.pdf},
}

@InProceedings{Cassagne2018,
  author    = {A. Cassagne and O. Aumage and D. Barthou and C. Leroux and C. J\'ego},
  title     = {{MIPP}: A Portable {C++} {SIMD} Wrapper and its use for Error Correction Coding in {5G} Standard},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2018},
  address   = {V\"osendorf/Wien, Austria},
  month     = feb,
  publisher = {ACM},
  abstract  = {Error correction code (ECC) processing has so far been performed on dedicated hardware for previous generations of mobile communication standards, to meet latency and bandwidth constraints.
As the 5G mobile standard, and its associated channel coding algorithms, are now being specified, modern CPUs are progressing to the point where software channel decoders can viably be contemplated. A key aspect in reaching this transition point is to get the most of CPUs SIMD units on the decoding algorithms being pondered for 5G mobile standards. The nature and diversity of such algorithms requires highly versatile programming tools. This paper demonstrates the virtues and versatility of our MIPP SIMD wrapper in implementing a high performance portfolio of key ECC decoding algorithms.},
  doi       = {10.1145/3178433.3178435},
  file      = {:pdf/Cassagne2018 - MIPP\: A Portable C++ SIMD Wrapper and its use for Error Correction Coding in 5G Standard.pdf:PDF;:pdf/Cassagne2018 - MIPP\: A Portable C++ SIMD Wrapper and its use for Error Correction Coding in 5G Standard [slides].pdf:PDF},
  groups    = {Wrapper, Polar Codes, LDPC Codes, Software Decoders, Pseudo-Random Number Generator (PRNG), 5G, AFF3CT},
  keywords  = {SIMD, wrapper, C++, channel code, SSE, AVX, AVX-512, NEON, Cassagne},
}

@TechReport{Cassagne2017,
  author      = {A. Cassagne and M. L\'eonardon and O. Hartmann and T. Tonnellier and G. Delbergue and V. Giraud and C. Leroux and R. Tajan and B. {Le Gal} and C. J\'ego and O. Aumage and D. Barthou},
  title       = {{AFF3CT} : Un environnement de simulation pour le codage de canal},
  institution = {GdR SoC2},
  year        = {2017},
  month       = jun,
  abstract    = {Dans cet article nous présentons un environnement de simulation de Monte Carlo pour les systèmes de communications numériques. Nous nous focalisons en particulier sur les fonctions associées au codage de
canal. Après avoir présenté les enjeux liés à la simulation, nous identifions trois problèmes inhérents à ce type de simulation. Puis nous présentons les principales caractéristiques de l’environnement AFF3CT.},
  doi         = {10.13140/RG.2.2.13492.91520},
  file        = {:pdf/Cassagne2017 - AFF3CT \: Un environnement de simulation pour le codage de canal [poster].pdf:PDF;:pdf/Cassagne2017 - AFF3CT \: Un environnement de simulation pour le codage de canal.pdf:PDF},
  groups      = {Software Decoders, AFF3CT},
  keywords    = {Cassagne},
}

@Conference{Cassagne2017a,
  author    = {A. Cassagne and O. Hartmann and M. L\'eonardon and T. Tonnellier and G. Delbergue and C. Leroux and R. Tajan and B. {Le Gal} and C. J\'ego and O. Aumage and D. Barthou},
  title     = {Fast Simulation and Prototyping with {AFF3CT}},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2017},
  month     = oct,
  publisher = {IEEE},
  abstract  = {This demonstration intends to present AFF3CT (A Fast Forward 3rror Correction Tool). The main objective of AFF3CT is to provide a portable, open source, fast and flexible software to the channel coding community in such a way that researchers can spend more time on channel coding / algorithmic problems instead of software development issues. It is also intended to facilitate the process of hardware verification and debug with the objective of fast prototyping.},
  doi       = {10.13140/RG.2.2.10295.42409/1},
  file      = {:pdf/Cassagne2017a - Fast Simulation and Prototyping with AFF3CT [poster].pdf:PDF;:pdf/Cassagne2017a - Fast Simulation and Prototyping with AFF3CT [abstract].pdf:PDF},
  groups    = {Software Decoders, Hardware Decoders, AFF3CT},
  url       = {https://sips2017.sciencesconf.org/data/demo_9.pdf},
  keywords  = {Cassagne},
}

@TechReport{Eltablawy2017,
  author      = {Alaa Eltablawy and Andrey Vladimirov},
  title       = {Capabilities of Intel {AVX-512} in Intel Xeon Scalable Processors (Skylake)},
  institution = {Colfax International},
  year        = {2017},
  month       = sep,
  abstract    = {This paper reviews the Intel Advanced Vector Extensions 512 (Intel AVX-512) instruction set and answers two critical questions:
1. How do Intel Xeon Scalable processors based on the Skylake architecture (2017) compare to their predecessors based on Broadwell due to AVX-512?
2. How are Intel Xeon processors based on Skylake different from their alternative, Intel Xeon Phi processors with the Knights Landing architecture, which also feature AVX-512?
We address these questions from the programmer’s perspective by demonstrating C language code of microkernels benefitting from AVX-512. For each example, we dig deeper and analyze the compilation practices, resultant assembly, and optimization reports.
In addition to code studies, the paper contains performance measurements for a synthetic benchmark with guidelines on estimating peak performance. In conclusion, we outline the workloads and application
domains that can benefit from the new features of AVX-512 instructions.},
  file        = {:pdf/Eltablawy2017 - Capabilities of Intel AVX-512 in Intel Xeon Scalable Processors (Skylake).pdf:PDF},
  groups      = {Single Instruction Multiple Data (SIMD)},
  url         = {https://colfaxresearch.com/skl-avx512/},
}

@Book{Ryan2009,
  title     = {Channel Codes: Classical and Modern},
  publisher = {Cambridge University Press},
  year      = {2009},
  author    = {W. Ryan and S. Lin},
  month     = sep,
  isbn      = {978-0-511-64182-4},
  abstract  = {Channel coding lies at the heart of digital communication and data storage, and this detailed introduction describes the core theory as well as decoding algorithms, implementation details, and performance analyses.
Professors Ryan and Lin, known for the clarity of their writing, provide the latest information on modern channel codes, including turbo and low-density parity-check (LDPC) codes. They also present detailed coverage of BCH codes, Reed–Solomon codes, convolutional codes, finite-geometry codes, and product codes, providing a one-stop resource for both classical and modern coding techniques.
The opening chapters begin with basic theory to introduce newcomers to the subject, assuming no prior knowledge in the field of channel coding. Subsequent chapters cover the encoding and decoding of the most widely used codes and extend to advanced topics such as code ensemble performance analyses and algebraic code design. Numerous varied and stimulating end-of-chapter problems, 250 in total,
are also included to test and enhance learning, making this an essential resource for students and practitioners alike.},
  file      = {:pdf/Ryan2009 - Channel codes\: Classical and modern.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC)},
  doi       = {10.1017/CBO9780511803253},
}

@InProceedings{Wang2014a,
  author    = {H. Wang and P. Wu and I. G. Tanase and M. J. Serrano and J. E. Moreira},
  title     = {Simple, Portable and Fast {SIMD} Intrinsic Programming: Generic {SIMD} Library},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2014},
  pages     = {9--16},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Using SIMD (Single Instruction Multiple Data) is a cost-effective way to explore data parallelism on modern processors. Most processor vendors today provide SIMD engines, such as Altivec/VSX for POWER, SSE/AVX for Intel processors, and NEON for ARM. While high-level SIMD programming models are rapidly evolving, for many SIMD developers, the most effective way to get the performance out of SIMD is still by programming directly via vendor-provided SIMD intrinsics. However, intrinsics programming is both tedious and error-prone, and worst of all, introduces non-portable codes.

This paper presents the Generic SIMD Library (https://github.com/genericsimd/generic_simd/), an open-source, portable C++ interface that provides an abstraction of short vectors and overloads most C/C++ operators for short vectors. The library provides several mappings from platform-specific intrinsics to the generic SIMD intrinsic interface so that codes developed based on the library are portable across different SIMD platforms.

We have evaluated the library with several applications from the multimedia, data analytics and math domains. Compared with platform-specific intrinsics codes, using Generic SIMD Library results in less line-of-code, a 22\% reduction on average, and achieves similar performance as platform-specific intrinsics versions.},
  acmid     = {2568059},
  doi       = {10.1145/2568058.2568059},
  file      = {:pdf/Wang2014a - Simple, Portable and Fast SIMD Intrinsic Programming\: Generic Simd Library.pdf:PDF},
  groups    = {Wrapper},
  isbn      = {978-1-4503-2653-7},
  keywords  = {AVX, SIMD, SSE, altivec, generic programming},
  location  = {Orlando, Florida, USA},
  numpages  = {8},
}

@InProceedings{Leissa2014,
  author    = {R. Lei\ssa and I. Haffner and S. Hack},
  title     = {Sierra: A {SIMD} Extension for {C++}},
  booktitle = {Workshop on Programming Models for SIMD/Vector Processing (WPMVP)},
  year      = {2014},
  pages     = {17--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Nowadays, SIMD hardware is omnipresent in computers. Nonetheless, many software projects make hardly use of SIMD instructions: Applications are usually written in general-purpose languages like C++. However, general-purpose languages only provide poor abstractions for SIMD programming enforcing an error-prone, assembly-like programming style. An alternative are data-parallel languages. They indeed offer more convenience to target SIMD architectures but introduce their own set of problems. In particular, programmers are often unwilling to port their working C++ code to a new programming language.

In this paper we present Sierra: a SIMD extension for C++. It combines the full power of C++ with an intuitive and effective way to address SIMD hardware. With Sierra, the programmer can write efficient, portable and maintainable code. It is particularly easy to enhance existing code to run efficiently on SIMD machines.

In contrast to prior approaches, the programmer has explicit control over the involved vector lengths.},
  acmid     = {2568062},
  doi       = {10.1145/2568058.2568062},
  file      = {:pdf/Leissa2014 - Sierra\: A SIMD Extension for C++.pdf:PDF},
  groups    = {Single Instruction Multiple Data (SIMD)},
  isbn      = {978-1-4503-2653-7},
  keywords  = {SIMD, c++, vectorization},
  location  = {Orlando, Florida, USA},
  numpages  = {8},
}

@InProceedings{Tonnellier2016,
  author    = {T. Tonnellier and C. Leroux and B. {Le Gal} and C. J\'ego and B. Gadat and N. Van Wambeke},
  title     = {Hardware Architecture for Lowering the Error Floor of {LTE} Turbo Codes},
  booktitle = {Design and Architectures for Signal and Image Processing Conference (DASIP)},
  year      = {2016},
  pages     = {107--112},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Turbo codes are well known error-correcting codes used in many communication standards. However, they suffer from error floors. Recently, a method - denoted as the flip and check algorithm - that lowers the error floor of turbo codes was proposed. This method relies on the identification of the least reliable bits during the turbo decoding process. Gains of about one order of magnitude were reached in terms of error rate performance. In this article, the first hardware implementation of the method is presented. The feasibility and hardware complexity are addressed by studying the impact of the algorithmic parameters of the technique. Synthesis results for FPGA implementations are reported and compared to turbo decoders implementations.},
  doi       = {10.1109/DASIP.2016.7853805},
  file      = {:pdf/Tonnellier2016 - Hardware Architecture for Lowering the Error Floor of LTE Turbo Codes.pdf:PDF},
  groups    = {Turbo Codes},
  keywords  = {Long Term Evolution, decoding, error correction codes, field programmable gate arrays, turbo codes, FPGA, LTE, Long Term Evolution, error floor, error-correcting codes, flip-and-check algorithm, hardware architecture, hardware complexity, turbo codes, turbo decoding process, Computer architecture, Decoding, Error analysis, Hardware, Iterative decoding, Standards, Turbo codes},
}

@InProceedings{Tonnellier2016a,
  author    = {T. Tonnellier and C. Leroux and B. {Le Gal} and C. J\'ego and B. Gadat and N. Van Wambeke},
  title     = {Lowering the Error Floor of Double-Binary Turbo Codes: The Flip and Check Algorithm},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2016},
  pages     = {156--160},
  month     = sep,
  publisher = {IEEE},
  abstract  = {This article addresses the error floor reduction of double-binary turbo codes. The proposed approach is an extension of a low complexity method originally proposed for decoding binary turbo codes. This method's interest is that it does not need to modify the turbo coding scheme as long as an error detection code is serially concatenated with the turbo code. Simulation results showed that error rate performance gains can reach one order of magnitude in the cases of the DVB-RCS and DVB-RCS2 standards, while keeping a well-handled computational complexity.},
  doi       = {10.1109/ISTC.2016.7593096},
  file      = {:pdf/Tonnellier2016a - Lowering the Error Floor of Double-Binary Turbo Codes\: The Flip and Check Algorithm.pdf:PDF},
  groups    = {Turbo Codes, AFF3CT},
  keywords  = {binary codes, computational complexity, convolutional codes, decoding, error detection codes, recursive estimation, turbo codes, DVB-RCS standard, DVB-RCS2 standard, binary turbo code decoding, computational complexity, double-binary turbo code error floor reduction, error detection code, error rate performance, flip and check algorithm, low complexity method, recursive systematic convolutional code, Decoding, Measurement},
}

@Article{Tonnellier2016b,
  author   = {T. Tonnellier and C. Leroux and B. {Le Gal} and B. Gadat and C. J\'ego and N. Van Wambeke},
  title    = {Lowering the Error Floor of Turbo Codes With {CRC} Verification},
  journal  = {IEEE Wireless Communications Letters (WCL)},
  year     = {2016},
  volume   = {5},
  number   = {4},
  pages    = {404--407},
  month    = aug,
  issn     = {2162-2337},
  abstract = {Decoding performance of turbo codes can flatten at moderately high signal-to-noise ratio. This letter proposes a low complexity method for lowering this error floor. This method rests on the observation of the extrinsic information during the iterative decoding process. A set of q most unreliable bits are identified based on their associated extrinsic information. A total of 2\textsuperscript{q} test patterns are then built by inverting the most unreliable bits. The decoded codeword is identified thanks to a cyclic redundancy check detector. This method keeps the turbo coding scheme unchanged as long as an error detection code is serially concatenated with the turbo code. Simulations were performed on a rate-1/3 Long-Term Evolution turbo code and show an improvement of at least one decade in terms of frame error rate in the error floor region. This low complexity method paves the way for further improvements in lowering the error floor of turbo codes.},
  doi      = {10.1109/LWC.2016.2571283},
  file     = {:pdf/Tonnellier2016b - Lowering the Error Floor of Turbo Codes With CRC Verification.pdf:PDF},
  groups   = {Turbo Codes, AFF3CT},
  keywords = {concatenated codes, cyclic redundancy check codes, error detection codes, error statistics, iterative decoding, turbo codes, CRC verification, Long-Term Evolution turbo code, cyclic redundancy check detector, decoded codeword identificaion, error detection code, frame error rate, iterative decoding process, low complexity method, serially concatenated code, signal-to-noise ratio, turbo code error floor, Cyclic redundancy check codes, Decoding, Error analysis, Iterative decoding, Measurement, Standards, Turbo codes, CRC codes, Turbo codes, error floor region, extrinsic information, iterative decoding process},
}

@Article{Tanner1981,
  author   = {R. Tanner},
  title    = {A Recursive Approach to Low Complexity Codes},
  journal  = {IEEE Transactions on Information Theory (TIT)},
  year     = {1981},
  volume   = {27},
  number   = {5},
  pages    = {533--547},
  month    = sep,
  issn     = {0018-9448},
  abstract = {A method is described for constructing long error-correcting codes from one or more shorter error-correcting codes, referred to as subcodes, and a bipartite graph. A graph is shown which specifies carefully chosen subsets of the digits of the new codes that must be codewords in one of the shorter subcodes. Lower bounds to the rate and the minimum distance of the new code are derived in terms of the parameters of the graph and the subeodes. Both the encoders and decoders proposed are shown to take advantage of the code's explicit decomposition into subcodes to decompose and simplify the associated computational processes. Bounds on the performance of two specific decoding algorithms are established, and the asymptotic growth of the complexity of decoding for two types of codes and decoders is analyzed. The proposed decoders are able to make effective use of probabilistic information supplied by the channel receiver, e.g., reliability information, without greatly increasing the number of computations required. It is shown that choosing a transmission order for the digits that is appropriate for the graph and the subcodes can give the code excellent burst-error correction abilities. The construction principles},
  doi      = {10.1109/TIT.1981.1056404},
  file     = {:pdf/Tanner1981 - A Recursive Approach to Low Complexity Codes.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {Error-correcting codes, Algorithm design and analysis, Bipartite graph, Complexity theory, Decoding, Error correction codes, Helium, Merging, Performance analysis, Product codes, Sorting},
}

@InProceedings{LeGal2017,
  author    = {B. {Le Gal} and C. J\'ego},
  title     = {Low-Latency Software {LDPC} Decoders for x86 Multi-Core Devices},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2017},
  pages     = {1--6},
  month     = oct,
  publisher = {IEEE},
  abstract  = {LDPC codes are a family of error correcting codes used in most modern digital communication standards even in future 3GPP 5G standard. Thanks to their high processing power and their parallelization capabilities, prevailing multi-core and many-core devices facilitate real-time implementations of digital communication systems, which were previously implemented on dedicated hardware targets. Through massive frame decoding parallelization, current LDPC decoders throughputs range from hundreds of Mbps up to Gbps. However, inter-frame parallelization involves latency penalties, while in future 5G wireless communication systems, the latency should be reduced as far as possible. To this end, a novel LDPC parallelization approach for LDPC decoding on a multi-core processor device is proposed in this article. It reduces the processing latency down to some microseconds as highlighted by x86 multi-core experimentations.},
  doi       = {10.1109/SiPS.2017.8110001},
  file      = {:pdf/LeGal2017 - Low-Latency Software LDPC Decoders for x86 Multi-Core Devices.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC), LDPC Codes, HoF LDPC - BP},
  keywords  = {3G mobile communication, 5G mobile communication, decoding, digital communication, error correction codes, parity check codes, software radio, LDPC codes, LDPC decoding, digital communication systems, error correcting codes, future 3GPP 5G standard, hardware targets, high processing power, inter-frame parallelization, latency penalties, low-latency software LDPC decoders, many-core devices, massive frame decoding parallelization, modern digital communication standards, multicore processor device, novel LDPC parallelization approach, parallelization capabilities, processing latency, real-time implementations, x86 multicore devices, x86 multicore experimentations, Decoding, Parallel processing, Parity check codes, Program processors, Standards, Throughput},
}

@Article{Richardson2003,
  author   = {T. Richardson and R. Urbanke},
  title    = {The Renaissance of Gallager's Low-Density Parity-Check Codes},
  journal  = {IEEE Communications Magazine},
  year     = {2003},
  volume   = {41},
  number   = {8},
  pages    = {126--131},
  month    = aug,
  issn     = {0163-6804},
  abstract = {LDPC codes were invented in 1960 by R. Gallager. They were largely ignored until the discovery of turbo codes in 1993. Since then, LDPC codes have experienced a renaissance and are now one of the most intensely studied areas in coding. In this article we review the basic structure of LDPC codes and the iterative algorithms that are used to decode them. We also briefly consider the state of the art of LDPC design.},
  comment  = {C'est un papier introductif sur les codes LDPCs.

Les LDPCs sont définis par le décodeur.
Il y a deux grandes familles de décodeur :
  - ceux par message (message passing) (onéreux),
  - les bits flipping (cheap).

Le design du code est essentiel, souvent les codes qui ont une mauvaise convergence sont bon dans le floor.
C'est pas évident d'avoir un code super bon dans la convergence et dans le floor.

Multi-edge type LDPC, je n'ai pas bien compris en quoi ça consiste, c'est sensé améliorer les codes LDPC...},
  doi      = {10.1109/MCOM.2003.1222728},
  file     = {:pdf/Richardson2003 - The Renaissance of Gallager's Low-Density Parity-Check Codes.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {graph theory, iterative decoding, parity check codes, reviews, Gallager's low-density parity-check codes, LDPC codes, basic structure, decoding, iterative algorithms, Belief propagation, Computational complexity, Iterative algorithms, Iterative decoding, Iterative methods, Maximum likelihood decoding, Parity check codes, Redundancy, Routing, Signal processing},
}

@InProceedings{MacKay1995,
  author    = {D. J. C. MacKay and R. M. Neal},
  title     = {Good Codes Based on Very Sparse Matrices},
  booktitle = {IMA International Conference on Cryptography and Coding (IMA-CCC)},
  year      = {1995},
  pages     = {100--111},
  address   = {UK},
  month     = dec,
  publisher = {Springer},
  doi       = {10.1007/3-540-60693-9_13},
  file      = {:pdf/MacKay1995 - Good Codes Based on Very Sparse Matrices.pdf:PDF},
  groups    = {LDPC Codes},
  isbn      = {978-3-540-49280-1},
}

@PhdThesis{Alevizos2012,
  author   = {P. Alevizos},
  title    = {Factor Graphs: Theory and Applications},
  school   = {Technical University of Crete},
  year     = {2012},
  abstract = {Factor graphs (FGs) represent graphically the factorization of a global function into a product of local sub-functions. The global function is usually a multi-variable probability density function (pdf ), where the calculation of a marginal pdf is usually intractable. The sum-product algorithm (SPA) is applied on the FG through message-passing, i.e. exchange of functions, between the FG nodes in a distributed way; the output is a marginal pdf with respect to a variable of interest. Factor graph theory has several applications in many interdisciplinary fields, such as error correction coding theory, detection and estimation, wireless networking, artificial intelligence and many others.
This thesis provides the basic theoretical background in a tutorial way, from first principles. Furthermore, specific FG applications found in the literature are presented. Specifically, coding problems (LDPC, convolutional and parallel concatenated Turbo codes), Bayesian estimation (in the context of network localization) and wireless multi-hop networking (in the context of time scheduling) are analyzed within the FG framework. In all cases, the respective graph, the associated SPA, the message-passing scheduling and the final output are thoroughly presented. The power of FGs as a distributed inference tool is vividly demonstrated.},
  file     = {:pdf/Alevizos2012 - Factor Graphs\: Theory and Applications.pdf:PDF},
  groups   = {Factor Graphs},
}

@Misc{Yedidia2004,
  author    = {J. S. Yedidia and E. Martinian},
  title     = {Quantizing Signals Using Sparse Generator Factor Graph Codes},
  month     = aug,
  year      = {2004},
  note      = {US Patent 6,771,197},
  abstract  = {A method quantizes an input signal of N samples into a string of k symbols drawn from a q-ary alphabet. A complementary method reproduces a minimally distorted version of the input signal from the quantized string, given some distortion measure. First, an [N,k]q linear error-correcting code that has a sparse generator factor graph representation is selected. A fixed mapping from q-ary symbols to samples is selected. A soft-input decoder and an encoder for the SGFG codes is selected. A cost function is determined from the input signal and a distortion measure, using the fixed mapping. The decoder determines an information block corresponding to a code word of the SGFG code with a low cost for the input signal. The input signal can be reproduced using the encoder for the SGFG code, in combination with the fixed mapping. },
  file      = {:pdf/Yedidia2004 - Quantizing Signals Using Sparse Generator Factor Graph Codes.pdf:PDF},
  url       = {http://www.freepatentsonline.com/6771197.pdf},
  groups    = {Factor Graphs},
  publisher = {Google Patents},
}

@Misc{Wolf2008,
  author = {J. K. Wolf},
  title  = {An Introduction Error Correcting Codes (Part 1)},
  year   = {2008},
  file   = {:pdf/Wolf2008 - An Introduction Error Correcting Codes (Part 1).pdf:PDF},
  groups = {Error-Correcting Codes (ECC)},
  url    = {http://acsweb.ucsd.edu/~afazelic/ece154c/ErrorCorrection-JackWolf.pdf}
}

@Article{Chandesris2018,
  author   = {L. Chandesris and V. Savin and D. Declercq},
  title    = {Dynamic-{SC}Flip Decoding of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2018},
  volume   = {PP},
  number   = {99},
  pages    = {1},
  issn     = {0090-6778},
  abstract = {This paper proposes a generalization of the recently introduced Successive Cancellation Flip (SCFlip) decoding of polar codes, characterized by a number of extra decoding attempts, where one or several positions are flipped from the standard Successive Cancellation (SC) decoding. To make such an approach effective, we first introduce the concept of higher-order bit-flips, and propose a new metric to determine the bit-flips that are more likely to correct the trajectory of the SC decoding. We then propose a generalized SCFlip decoding algorithm, referred to as Dynamic-SCFlip (D-SCFlip), which dynamically builds a list of candidate bit-flips, while guaranteeing that the next attempt has the highest probability of success among the remaining ones. Simulation results show that D-SCFlip is an effective alternative to SC-List decoding of polar codes, by providing very good error correcting performance, with an average computation complexity close to the one of the SC decoder.},
  doi      = {10.1109/TCOMM.2018.2793887},
  file     = {:pdf/Chandesris2018 - Dynamic-SCFlip Decoding of Polar Codes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Computational complexity, Decoding, Error correction, Measurement, Signal to noise ratio, Standards, Polar Codes, SCFlip decoding, order statistic decoding, successive cancellation decoding, SC flip},
}

@InProceedings{Afisiadis2014,
  author    = {O. Afisiadis and A. Balatsoukas-Stimming and A. Burg},
  title     = {A Low-Complexity Improved Successive Cancellation Decoder for Polar Codes},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2014},
  pages     = {2116--2120},
  month     = nov,
  publisher = {IEEE},
  abstract  = {Under successive cancellation (SC) decoding, polar codes are inferior to other codes of similar blocklength in terms of frame error rate. While more sophisticated decoding algorithms such as list- or stack-decoding partially mitigate this performance loss, they suffer from an increase in complexity. In this paper, we describe a new flavor of the SC decoder, called the SC flip decoder. Our algorithm preserves the low memory requirements of the basic SC decoder and adjusts the required decoding effort to the signal quality. In the waterfall region, its average computational complexity is almost as low as that of the SC decoder.},
  doi       = {10.1109/ACSSC.2014.7094848},
  file      = {:pdf/Afisiadis2014 - A Low-Complexity Improved Successive Cancellation Decoder for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {computational complexity, decoding, error statistics, signal processing, average computational complexity, frame error rate, low-complexity improved SC flip decoder, polar codes, signal quality, successive cancellation decoding, Computational complexity, Decoding, Error analysis, Memory management, Signal to noise ratio, SCFlip},
}

@PhdThesis{Tonnellier2017,
  author   = {T. Tonnellier},
  title    = {Contribution to the Improvement of the Decoding Performance of Turbo Codes : Algorithms and Architecture},
  school   = {Universit{\'e} de Bordeaux},
  year     = {2017},
  abstract = {Since their introduction in the 90’s, turbo codes are considered as one of the most powerful error-correcting code. Thanks to their excellent trade-off between computational complexity and decoding performance, they were chosen in many communication standards.

One way to characterize error-correcting codes is the evolution of the bit error rate as a function of signal-to-noise ratio (SNR). The turbo code error rate performance is divided in two different regions : the waterfall region and the error floor region. In the waterfall region, a slight increase in SNR results in a significant drop in error rate. In the error floor region, the error rate performance is only slightly improved as the SNR grows. This error floor can prevent turbo codes from being used in applications with low error rates requirements. Therefore various constructions optimizations that lower the error floor of turbo codes has been proposed in recent years by scientific community. However, these approaches can not be considered for already standardized turbo codes.

This thesis addresses the problem of lowering the error floor of turbo codes without allowing any modification of the digital communication chain at the transmitter side.
For this purpose, the state-of-the-art post-processing decoding method for turbo codes is detailed. It appears that efficient solutions are expensive to implement due to the required multiplication of computational resources or can strongly impact the overall decoding latency.

Firstly, two decoding algorithms based on the monitoring of decoder’s internal metrics are proposed. The waterfall region is enhanced by the first algorithm. However, the second one marginally lowers the error floor. Then, the study shows that in the error floor region, frames decoded by the turbo decoder are really close to the word originally transmitted. This is demonstrated by a proposition of an analytical prediction of the distribution of the number of bits in errors per erroneous frame. This prediction rests on the distance spectrum of turbo codes. Since the appearance of error floor region is due to only few bits in errors, an identification metric is proposed. This lead to the proposal of an algorithm that can correct residual errors. This algorithm, called Flip-and-Check, rests on the generation of candidate words, followed by verification according to an error-detecting code. Thanks to this decoding algorithm, the error floor of turbo codes encountered in different standards (LTE, CCSDS, DVB-RCS and DVB-RCS2) is lowered by one order of magnitude. This performance improvement is obtained without considering an important vcomputational complexity overhead.

Finally, a hardware decoding architecture implementing the Flip-and-Check algorithm is presented. A preliminary study of the impact of the different parameters of this algorithm is carried out. It leads to the definition of optimal values for some of these parameters. Others has to be adapted according to the gains targeted in terms of decoding performance. The possible integration of this algorithm along with existing turbo decoders is demonstrated thanks to this hardware architecture. This therefore enables the lowering of the error floors of standardized turbo codes.},
  file     = {:pdf/Tonnellier2017 - Contribution to the Improvement of the Decoding Performance of Turbo Codes \: Algorithms and Architecture.pdf:PDF},
  groups   = {Turbo Codes, AFF3CT},
  keywords = {Turbo codes, Residual Errors, Post-processing, Hardware architecture},
  url      = {https://tel.archives-ouvertes.fr/tel-01580476},
}

@InProceedings{Ghaffari2017,
  author    = {A. Ghaffari and M. L\'eonardon and Y. Savaria and C. J\'ego and C. Leroux},
  title     = {Improving Performance of {SCMA} {MPA} Decoders using Estimation of Conditional Probabilities},
  booktitle = {International Conference on New Circuits and Systems (NEWCAS)},
  year      = {2017},
  pages     = {21--24},
  month     = jun,
  abstract  = {Sparse code multiple access (SCMA) is a new type of non-orthogonal modulation suggested for 5G systems offering lower bit-error rate and higher spectral efficiency. There are many challenges when designing high throughput SCMA message passing decoders to meet the standards expected from 5G networks. Particularly, the message passing algorithm (MPA) needs many exponential computations to calculate conditional probabilities in case of Gaussian noise channels. This paper describes a sub-optimal modeling of noise using polynomial probability distributions rather than a normal distribution to eliminate the exponential calculations for MPA detectors. Simulation results demonstrate that an estimated SCMA MPA reaches the desired bit-error rate performance with much lower computational/hardware complexity.},
  doi       = {10.1109/NEWCAS.2017.8010095},
  file      = {:pdf/Ghaffari2017 - Improving Performance of SCMA MPA Decoders using Estimation of Conditional Probabilities.pdf:PDF},
  groups    = {Turbo Codes, AFF3CT},
  keywords  = {5G mobile communication, Gaussian channels, error statistics, multi-access systems, probability, 5G systems, Gaussian noise channels, SCMA MPA decoders, bit-error rate, conditional probabilities, message passing algorithm, nonorthogonal modulation, polynomial probability distributions, sparse code multiple access, 5G mobile communication, Algorithm design and analysis, Decoding, Estimation, Message passing, Probability density function, Throughput, 5G, BER, MPA Decoder, Sparse Code Multiple Access (SCMA), Uplink, iterative multi-user detection},
}

@PhdThesis{Guilloud2004,
  author   = {F. Guilloud},
  title    = {Generic Architecture for {LDPC} Codes Decoding},
  school   = {T{\'e}l{\'e}com ParisTech},
  year     = {2004},
  abstract = {The Low-Density Parity-Check codes are among the most powerful forward error correcting codes, since they enable to get as close as a fraction of a dB from the Shannon limit.
This astonishing performance combined with their relatively simple decoding algorithm make these codes very attractive for the next digital transmission system generations. It is already the case for the next digital satellite broadcasting standard (DVB-S2), where an irregular LDPC code has been chosen to protect the downlink information.

In this thesis, we focused our research on the iterative decoding algorithms and their hardware implementations. We proposed first a suboptimal algorithm named the λ-min algorithm. It reduces significantly the complexity of the decoder without any significant performance loss, as compared to the belief propagation (BP) algorithm.

Then we studied and designed a generic architecture of an LDPC decoder, which has been implemented on a FPGA based platform. This hardware decoder enables to accelerate the simulations more than 500 times as compared to software simulations. Moreover, based on an all-tunable design, our decoder features many facilities: It is possible to configure it for a very wide code family, so that the research for good codes is processed
faster ; thanks to the genericity of the processing components, it is also possible to optimize the internal coding format, and even to compare various decoding algorithms and various processing schedules.

Finally, our experience in the area of LDPC decoders led us to propose a formal framework for analysing the architectures of LDPC decoders. This framework encompasses both the datapath (parallelism, node processors architectures) and the control mode associated to the several decoding schedules. Thus within this framework, a classification of the different state-of-the-art LDPC decoders is proposed. Moreover, some synthesis of efficient and unpublished architectures have been also proposed.},
  file     = {:pdf/Guilloud2004 - Generic Architecture for LDPC Codes Decoding.pdf:PDF},
  groups   = {LDPC Codes, Hardware Decoders},
  keywords = {Low-density parity-check codes, Design, Ldpc,Decoder,Fpga,Vhdl,Architecture,decoder,low density codes,vertical layered, horizontal layered, vertical, horizontal},
  url      = {https://pastel.archives-ouvertes.fr/pastel-00000806/},
}

@InProceedings{Zhang2002,
  author    = {J. Zhang and M. Fossorier},
  title     = {Shuffled Belief Propagation Decoding},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {2002},
  volume    = {1},
  pages     = {8--15},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a shuffled version of the belief propagation (BP) algorithm for the decoding of low-density parity-check (LDPC) codes. We show that when the Tanner graph of the code is acyclic and connected, the proposed scheme is optimal in the sense of MAP decoding and converges faster (or at least no slower) than the standard BP algorithm. Interestingly, this new version keeps the computational advantages of the forward-backward implementations of BP decoding. Both serial and parallel implementations are considered. We show by simulation that the new schedule offers better performance/complexity trade-offs.},
  comment   = {C'est le papier qui a introduit le vertical layered.},
  doi       = {10.1109/ACSSC.2002.1197141},
  file      = {:pdf/Zhang2002 - Shuffled Belief Propagation Decoding.pdf:PDF;:pdf/Zhang2002 - Shuffled Belief Propagation Decoding [slides].pdf:PDF},
  groups    = {LDPC Codes},
  issn      = {1058-6393},
  keywords  = {computational complexity, graph theory, maximum likelihood decoding, maximum likelihood estimation, parity check codes, BP algorithm, LDPC codes, MAP decoding, Tanner graph, computational complexity, forward-backward implementations, low-density parity-check codes, maximum a posteriori probability, parallel implementation, serial implementation, shuffled belief propagation decoding, Belief propagation, Code standards, Computational modeling, Concurrent computing, Iterative algorithms, Iterative decoding, Parallel processing, Parity check codes, Processor scheduling, Standards development, vertical layered, layered scheduling, layered},
}

@InProceedings{Yeo2001,
  author    = {E. Yeo and P. Pakzad and B. Nikolic and V. Anantharam},
  title     = {High Throughput Low-Density Parity-Check Decoder Architectures},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2001},
  volume    = {5},
  pages     = {3019--3024},
  publisher = {IEEE},
  abstract  = {Two decoding schedules and the corresponding serialized architectures for low-density parity-check (LDPC) decoders are presented. They are applied to codes with parity-check matrices generated either randomly or using geometric properties of elements in Galois fields. Both decoding schedules have low computational requirements. The original concurrent decoding schedule has a large storage requirement that is dependent on the total number of edges in the underlying bipartite graph, while a new, staggered decoding schedule which uses an approximation of the belief propagation, has a reduced memory requirement that is dependent only on the number of bits in the block. The performance of these decoding schedules is evaluated through simulations on a magnetic recording channel},
  comment   = {Article qui introduit l'horizontal layered pour la première fois.},
  doi       = {10.1109/GLOCOM.2001.965981},
  file      = {:pdf/Yeo2001 - High Throughput Low-Density Parity-Check Decoder Architectures.pdf:PDF},
  groups    = {LDPC Codes},
  keywords  = {Galois fields, belief maintenance, decoding, error detection codes, magnetic recording, Galois fields, LDPC decoders, belief propagation, bipartite graph, concurrent decoding, decoding schedules, low-density parity-check decoders, magnetic recording channel, parity check matrices, serialized architectures, staggered decoding schedule, Bipartite graph, Computational modeling, Computer architecture, Galois fields, Iterative decoding, Magnetic recording, Message passing, Parity check codes, Processor scheduling, Throughput, layered, horizontal layered},
}

@PhdThesis{Li2012a,
  author   = {M. Li},
  title    = {Design, Implementation and Prototyping of an Iterative Receiver for Bit-Interleaved Coded Modulation System dedicated to {DVB-T2}},
  school   = {University of Bretagne Sud},
  year     = {2012},
  abstract = {In 2008, the European Digital Video Broadcasting (DVB) standardization committee issued the second generation of Digital Video Broadcasting-Terrestrial (DVB-T2) standard in order to enable the wide broadcasting of high definition and 3D TV programmes. DVB-T2 has adopted several new technologies to provide more robust reception compared to the first genaration standard. One important technology is the bit interleaved coded modulation (BICM) with doubled signal space diversity plus the usage of low-density parity check (LDPC) codes. Both techniques can be combined at the receiver side through an iterative process between the decoder and demapper in order to further increase the system performance. The object of my study was to design and prototype a DVB-T2 receiver which supports iterative process. The two main contributions to the demapper design are the proposal of a linear approximation of Euclidean distance computation and the derivation of a sub-region detection algorithm for the two-dimensional demapper. Both contributions allows the computational complexity of the demapper to be reduced for its hardware implementation. In order to enable iterative processing between the demapper and the decoder, we investigated the use of vertical shuffled Min-Sum LDPC decoding algorithm. A novel vertical shuffled iterative structure aiming at reducing the latency of iterative processing and the corresponding architecture of the decoder were proposed. The proposed demapper and decoder have been integrated in a real DVB-T2 demodulator and tested in order to validate the efficiency of the proposed architecture. The prototype of a simplified DVB-T2 transceiver has been implemented, in which the receiver supports both non-iterative process and iterative process. We published the first paper related to a DVB-T2 iterative receiver. },
  file     = {:pdf/Li2012a - Design, Implementation and Prototyping of an Iterative Receiver for Bit-Interleaved Coded Modulation System dedicated to DVB-T2.pdf:PDF},
  groups   = {LDPC Codes, Hardware Decoders},
  keywords = {DVB-T2 standard, signal-space diversity, rotated QAM, 2D Demapper, LDPC decoder, iterative receiver, turbo-demodulation, FPGA, prototype, Standard DVB-T2, Diversit{\'e} de constellation, MAQ tourn{\'e}e, D{\'e}mappeur 2D, R{\'e}cepteur it{\'e}ratif, Turbo demodulation, FPGA prototype, layered, vertical layered},
  url      = {https://tel.archives-ouvertes.fr/tel-00719312},
}

@Misc{Cassagne2018a,
  author   = {A. Cassagne and O. Hartmann and M. L\'eonardon and C. Leroux and C. J\'ego},
  title    = {A Fast Forward Error Correction Toolbox: Seminary},
  month    = mar,
  year     = {2018},
  note     = {Presentation at the IMS laboratory, Bordeaux, France.},
  file     = {:pdf/Cassagne2018a - A Fast Forward Error Correction Toolbox\: Seminary.pdf:PDF},
  groups   = {AFF3CT, Error-Correcting Codes (ECC), Polar Codes, Turbo Codes, LDPC Codes},
  keywords = {AFF3CT, Cassagne},
  url      = {https://aff3ct.github.io/publications/Cassagne2018a - A Fast Forward Error Correction Toolbox Seminary.pdf}
}

@Article{Ghaffari2019,
  author   = {A. Ghaffari and M. L\'eonardon and A. Cassagne and C. Leroux and Y. Savaria},
  title    = {Toward High Performance Implementation of {5G} {SCMA} Algorithms},
  journal  = {IEEE Access},
  year     = {2019},
  volume   = {7},
  pages    = {10402--10414},
  issn     = {2169-3536},
  abstract = { The recent evolution of mobile communication systems toward a 5G network is associated with the search for new types of non-orthogonal modulations such as Sparse Code Multiple Access (SCMA). Such modulations are proposed in response to demands for increasing the number of connected users. SCMA is a non-orthogonal multiple access technique that offers improved Bit-Error Rate (BER) performance and higher spectral efficiency than other comparable techniques, but these improvements come at the cost of complex decoders. There are many challenges in designing near-optimum high throughput SCMA decoders. This paper explores means to enhance the performance of SCMA decoders. To achieve this goal, in this paper, various improvements to the MPA algorithms are proposed. They notably aim at adapting SCMA decoding to the Single Instruction Multiple Data (SIMD) paradigm. An approximate modeling of noise is performed to reduce the complexity of floating point calculations. The effects of forward error corrections such as Polar codes, Turbo codes and LDPC, as well as different ways of accessing memory and improving power efficiency of  modified MPAs are investigated. The results show that the throughput of a SCMA decoder can be increased by 3.1 to 21 times when compared to the original MPA on different computing platforms using the suggested improvements. },
  doi      = {10.1109/ACCESS.2019.2891597},
  file     = {:pdf/Ghaffari2019 - Toward High Performance Implementation of 5G SCMA Algorithms.pdf:PDF;:pdf/Ghaffari2019 - Toward High Performance Implementation of 5G SCMA Algorithms [word].pdf:PDF},
  keywords = {5G, SCMA, Maximum Likelihood (ML), Message Passing Algorithm (MPA), log-MPA, iterative multi-user detection, BER, Single Instruction Multiple Data (SIMD), Intel Advanced Vector Extensions (AVX), Streaming SIMD Extension (SSE), Knights Corner Instruction (KNCI), power efficiency, exponential estimations, Cassagne},
}

@Article{Li2016a,
  author   = {A. Li and R. G. Maunder and B. M. Al-Hashimi and L. Hanzo},
  title    = {Implementation of a Fully-Parallel Turbo Decoder on a General-Purpose Graphics Processing Unit},
  journal  = {IEEE Access},
  year     = {2016},
  volume   = {4},
  pages    = {5624--5639},
  issn     = {2169-3536},
  abstract = {Turbo codes comprising a parallel concatenation of upper and lower convolutional codes are widely employed in the state-of-the-art wireless communication standards, since they facilitate transmission throughputs that closely approach the channel capacity. However, this necessitates high processing throughputs in order for the turbo code to support real-time communications. In the state-of-the-art turbo code implementations, the processing throughput is typically limited by the data dependences that occur within the forward and backward recursions of the Log-BCJR algorithm, which is employed during turbo decoding. In contrast to the highly serial Log-BCJR turbo decoder, we have recently proposed a novel fully parallel turbo decoder (FPTD) algorithm, which can eliminate the data dependences and perform fully parallel processing. In this paper, we propose an optimized FPTD algorithm, which reformulates the operation of the FPTD algorithm so that the upper and lower decoders have identical operation, in order to support single instruction multiple data operation. This allows us to develop a novel general purpose graphics processing unit (GPGPU) implementation of the FPTD, which has application in software-defined radios and virtualized cloud-radio access networks. As a benefit of its higher degree of parallelism, we show that our FPTD improves the higher processing throughput of the Log-BCJR turbo decoder by between 2.3 and 9.2 times, when employing a high-specification GPGPU. However, this is achieved at the cost of a moderate increase of the overall complexity by between 1.7 and 3.3 times.},
  doi      = {10.1109/ACCESS.2016.2586309},
  file     = {:pdf/Li2016a - Implementation of a Fully-Parallel Turbo Decoder on a General-Purpose Graphics Processing Unit.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Turbo Codes, Software Decoders, HoF Turbo - FPTD, HoF Turbo - MAP},
  keywords = {convolutional codes, graphics processing units, parallel processing, radio networks, telecommunication computing, turbo codes, GPGPU, general purpose graphics processing unit, optimized FPTD algorithm, parallel processing, data dependences, Log-BCJR algorithm, channel capacity, wireless communication standards, convolutional codes, turbo codes, fully-parallel turbo decoder, Decoding, Turbo codes, Software defined radio, Radio access networks, Throughput, Parallel processing, Wireless communication, Convolutional codes, Graphics processing units, Fully-parallel turbo decoder, parallel processing, GPGPU computing, software defined radio, could radio access network},
}

@InProceedings{Cammerer2017,
  author    = {S. Cammerer and B. Leible and M. Stahl and J. Hoydis and S. ten Brink},
  title     = {Combining Belief Propagation and Successive Cancellation List Decoding of Polar Codes on a {GPU} Platform},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {3664--3668},
  month     = mar,
  publisher = {IEEE},
  abstract  = {The decoding performance of polar codes strongly depends on the decoding algorithm used, while also the decoder throughput and its latency mainly depend on the decoding algorithm. In this work, we implement the powerful successive cancellation list (SCL) decoder on a GPU and identify the bottlenecks of this algorithm with respect to parallel computing and its difficulties. The inherent serial decoding property of the SCL algorithm naturally limits the achievable speed-up gains on GPUs when compared to CPU implementations. In order to increase the decoding throughput, we use a hybrid decoding scheme based on the belief propagation (BP) decoder, which can be intra- and inter-frame parallelized. The proposed scheme combines excellent decoding performance and high throughput within the signal-to-noise ratio (SNR) region of interest.},
  doi       = {10.1109/ICASSP.2017.7952840},
  file      = {:pdf/Cammerer2017 - Combining Belief Propagation and Successive Cancellation List Decoding of Polar Codes on a GPU Platform.pdf:PDF},
  groups    = {HoF Turbo - FPTD, Software Decoders, Polar Codes, HoF Polar - SCL},
  issn      = {2379-190X},
  keywords  = {belief maintenance, graphics processing units, parallel processing, parity check codes, telecommunication computing, SNR, signal-to-noise ratio region of interest, BP, hybrid decoding scheme, CPU implementations, serial decoding property, speed-up gains, parallel computing, SCL, successive cancellation list decoder, decoder throughput, decoding algorithm, GPU platform, polar codes, successive cancellation list decoding, belief propagation, Decoding, Throughput, Graphics processing units, Iterative decoding, Belief propagation, Signal to noise ratio, Complexity theory},
}

@InProceedings{Li2016b,
  author    = {Y. Li and R. Liu},
  title     = {High Throughput {GPU} Polar Decoder},
  booktitle = {International Conference on Computer and Communications (ICCC)},
  year      = {2016},
  pages     = {1123--1127},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this paper, a polar decoder with simplified decoding algorithm, optimized thread execution and memory access on a Graphics Processing Unit (GPU) is proposed. Four main aspects are optimized to improve the throughput of the polar decoder. Firstly, decoding information is arranged to ensure global memory coalesced access. Secondly, to avoid extra computation and access delay of frozen set, semi-unrolled architecture is adopted. Finally, the branch divergence is optimized to decrease the branch delay. The experiments are carried out on Nvidia Tesla K20 platform. Results demonstrate that the proposed decoder achieves about 700Mbps.},
  doi       = {10.1109/CompComm.2016.7924879},
  file      = {:pdf/Li2016b - High Throughput GPU Polar Decoder.pdf:PDF},
  groups    = {Software Decoders, Polar Codes, HoF Polar - SC},
  keywords  = {block codes, decoding, error correction codes, graphics processing units, linear codes, multi-threading, parallel architectures, high throughput GPU polar decoder, decoding algorithm, optimized thread execution, graphics processing unit, global memory coalesced access, access delay, frozen set semiunrolled architecture, branch divergence, branch delay, Nvidia Tesla K20 platform, Message systems, Graphics, Decoding, polar code, Semi-Fast-SSC, CUDA, semiunrolled, coalesced access, branch divergence},
}

@InProceedings{Han2017,
  author    = {X. Han and R. Liu and Z. Liu and L. Zhao},
  title     = {Successive-Cancellation List Decoder of Polar Codes based on {GPU}},
  booktitle = {International Conference on Computer and Communications (ICCC)},
  year      = {2017},
  pages     = {2065--2070},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, a Graphics Processing Unit (GPU) implementation of a Successive-Cancellation List (SCL) decoder for polar codes is proposed. Based on the compute unified device architecture (CUDA), the strategy to parallelize the decoding procedure is designed to reduce the latency. Moreover, the optimal design of data structures for several kinds of intermediate information is presented. In order to reduce the global memory accesses in list pruning, The GPU-adapted lazy-copy strategy is elaborated. By means of the parallel computing capabilities of GPUs, the proposed decoder achieves high throughput of 41Mbps on NVIDIA GTX 980 and 65Mbps on TITAN X while decoding the code with length of 1024 bits and 32 lists.},
  doi       = {10.1109/CompComm.2017.8322900},
  file      = {:pdf/Han2017 - Successive-Cancellation List Decoder of Polar Codes Based on GPU.pdf:PDF},
  groups    = {HoF Polar - SC, Software Decoders, Polar Codes, HoF Polar - SCL},
  keywords  = {data structures, decoding, graphics processing units, polar codes, GPU, Successive-Cancellation List decoder, compute unified device architecture, data structures, global memory accesses, list pruning, lazy-copy strategy, Graphics Processing Unit, NVIDIA GTX, TITAN X, CUDA, SCL decoder, Arrays, Graphics processing units, Maximum likelihood decoding, Indexes, Mathematical model, GPU, CUDA, polar codes, SCL, parallel decoding},
}

@Article{Keskin2017b,
  author   = {S. Keskin and T. Kocak},
  title    = {{GPU}-Based Gigabit {LDPC} Decoder},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {8},
  pages    = {1703--1706},
  month    = aug,
  issn     = {1089-7798},
  abstract = {In this letter, we present design and implementation of a parallel software low-density parity-check (LDPC) decoding algorithm on graphics processing units (GPUs). As a solution for the LDPC decoder, dedicated application-specific integrated circuit or field programmable gate array implementations is proposed in recent years in order to support high throughput despite their long deployment cycle, high design, and especially fixed functionalities. On the other hand, the implementations on GPU as a software solution provide flexible, scalable, and less expensive solutions in a shorter deployment cycle. We present some GPU-based optimizations for a major LDPC decoder algorithm to obtain high throughput in digital communication systems. Experimental results demonstrate that the proposed LDPC decoder achieves more than 1.27 Gb/s peak throughput on a single GPU.},
  doi      = {10.1109/LCOMM.2017.2704113},
  file     = {:pdf/Keskin2017b - GPU-Based Gigabit LDPC Decoder.pdf:PDF},
  groups   = {HoF Polar - SCL, Software Decoders, LDPC Codes, HoF LDPC - BP},
  keywords = {application specific integrated circuits, decoding, digital communication, field programmable gate arrays, graphics processing units, optimisation, parallel algorithms, parallel architectures, parity check codes, digital communication systems, GPU-based optimizations, deployment cycle, field programmable gate array, application-specific integrated circuit, graphics processing units, parallel software low-density parity-check decoding algorithm, GPU-based gigabit LDPC decoder, Graphics processing units, Decoding, Throughput, Instruction sets, Iterative decoding, MATLAB, High throughput, decoding, MSA, LDPC codes, CUDA, GPU, concatenated decoders},
}

@InProceedings{Keskin2017a,
  author    = {S. Keskin and T. Kocak},
  title     = {{GPU} Accelerated Gigabit Level {BCH} and {LDPC} Concatenated Coding System},
  booktitle = {High Performance Extreme Computing Conference (HPEC)},
  year      = {2017},
  pages     = {1--4},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Increasing data traffic and multimedia services in recent years have paved the way for the development of optical transmission methods to be used in high bandwidth communications systems. In order to meet the very high throughput requirements, dedicated application specific integrated circuit and field programmable gate array solutions for low-density parity-check decoding are proposed in recent years. Conversely, software solutions are less expensive, scalable, and flexible and have shorter development cycle. A natural solution to lower the error floor is to concatenate the LDPC code with an algebraic outer code to clean up the residual errors. In this paper, we present the design and parallel software implementation of a major computation algorithm for LDPC decoding on general purpose graphics processing units as inner code and BCH decoding algorithm as outer code to achieve excellent error-correcting performance. The experimental results show that the proposed GPU-based concatenated decoder achieves the maximum decoding throughput of 1.82Gbps at 10 iterations with low bit-error rate (BER).},
  doi       = {10.1109/HPEC.2017.8091021},
  file      = {:pdf/Keskin2017a - GPU Accelerated Gigabit Level BCH and LDPC Concatenated Coding System.pdf:PDF},
  groups    = {HoF Polar - SCL, Software Decoders, LDPC Codes, HoF LDPC - BP},
  keywords  = {application specific integrated circuits, BCH codes, channel coding, concatenated codes, decoding, error correction codes, error statistics, field programmable gate arrays, graphics processing units, parity check codes, multimedia services, high bandwidth communications systems, dedicated application specific integrated circuit, field programmable gate array solutions, low-density parity-check decoding, LDPC code, algebraic outer code, residual errors, low bit-error rate, general purpose graphics processing units, BCH concatenated coding system, LDPC concatenated coding system, BCH decoding algorithm, GPU-based concatenated decoder, Instruction sets, Graphics processing units, Decoding, Iterative decoding, Throughput, Registers},
}

@InProceedings{Kun2018,
  author    = {D. Kun},
  title     = {High Throughput {GPU} {LDPC} Encoder and Decoder for {DVB-S}2},
  booktitle = {Aerospace Conference (AeroConf)},
  year      = {2018},
  pages     = {1--9},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Previous studies that used Graphics Processing Units (GPUs) to decode Low Density Parity Check (LDPC) codes for DVB-S2 employed inter-codeword parallelism and/or Turbo Decoding Message Passing (TDMP) to achieve high throughput. By converting the LDPC parity check matrix into a quasi-cyclic structure, we show that LDPC encoding can be efficiently implemented on a GPU, and a different approach to LDPC decoding with intra-codeword parallelism and early termination that can achieve approximately 100 Mbps increase in throughput per 0.1 dB increase in signal-to-noise ratio and, for some cases, achieve 1 Gbps or greater overall throughput.},
  doi       = {10.1109/AERO.2018.8396831},
  file      = {:pdf/Kun2018 - High Throughput GPU LDPC Encoder and Decoder for DVB-S2.pdf:PDF},
  groups    = {HoF LDPC - BP, Software Decoders, LDPC Codes},
  keywords  = {decoding, digital video broadcasting, message passing, parity check codes, turbo codes, Graphics Processing Units, Low Density Parity Check codes, DVB-S2 employed inter-codeword parallelism, Turbo Decoding Message Passing, LDPC parity check matrix, LDPC decoding, Parity check codes, Graphics processing units, Decoding, Kernel, Instruction sets, Buffer storage, Indexes},
}

@InProceedings{Bioglio2017a,
  author    = {V. Bioglio and F. Gabry and I. Land},
  title     = {Low-Complexity Puncturing and Shortening of Polar Codes},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2017},
  pages     = {1--6},
  month     = mar,
  publisher = {IEEE},
  abstract  = {In this work, we address the low-complexity construction of shortened and punctured polar codes from a unified view. While several independent puncturing and shortening designs were attempted in the literature, our goal is a unique, low-complexity construction encompassing both techniques in order to achieve any code length and rate. We observe that our solution significantly reduces the construction complexity as compared to state-of-the-art solutions while providing a block error rate performance comparable to constructions that are highly optimized for specific lengths and rates. This makes the constructed polar codes highly suitable for practical application in future communication systems requiring a large set of polar codes with different lengths and rates.},
  doi       = {10.1109/WCNCW.2017.7919040},
  file      = {:pdf/Bioglio2017a - Low-Complexity Puncturing and Shortening of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {block codes, communication complexity, error correction codes, linear codes, low-complexity puncturing, low-complexity shortening, shortened polar codes, punctured polar codes, low-complexity code construction, code length, code rate, construction complexity reduction, block error rate performance, communication systems, Decoding, Complexity theory, Measurement, Reliability, Algorithm design and analysis, Encoding, Error analysis},
}

@InProceedings{Gabry2017,
  author    = {F. Gabry and V. Bioglio and I. Land and J. Belfiore},
  title     = {Multi-Kernel Construction of Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2017},
  pages     = {761--765},
  month     = may,
  publisher = {IEEE},
  abstract  = {We propose a generalized construction for binary polar codes based on mixing multiple kernels of different sizes in order to construct polar codes of block lengths that are not only powers of integers. This results in a multi-kernel polar code with very good performance while the encoding complexity remains low and the decoding follows the same general structure as for the original Arikan polar codes. The construction provides numerous practical advantages as more code lengths can be achieved without puncturing or shortening. We observe numerically that the error-rate performance of our construction outperforms state-of-the-art constructions using puncturing methods.},
  comment   = {- point de départ des codes polaires multi-kernels
- combinaison de 2x2 (Arikan) et 3x3 spécifique qui est donné et semble avoir de bonnes propriétés},
  doi       = {10.1109/ICCW.2017.7962750},
  file      = {:pdf/Gabry2017 - Multi-Kernel Construction of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2474-9133},
  keywords  = {binary codes, block codes, multikernel construction, binary polar codes, block lengths, multikernel polar code, Arikan polar codes, Kernel, Decoding, Encoding, Nickel, 5G mobile communication, Complexity theory, Reliability, Polar Codes, Multiple Kernels, Successive Cancellation Decoding, multi kernel, multi-kernel},
}

@Article{Coppolino2018,
  author   = {G. Coppolino and C. Condo and G. Masera and W. J. Gross},
  title    = {A Multi-Kernel Multi-Code Polar Decoder Architecture},
  journal  = {IEEE Transactions on Circuits and Systems (TCAS)},
  year     = {2018},
  pages    = {1--10},
  issn     = {1549-8328},
  abstract = {Polar codes have received increasing attention in the past decade, and have been selected for the next generation of the wireless communication standard. Most research on polar codes has focused on codes constructed from a 2x2 polarization matrix, called binary kernel: codes constructed from binary kernels have code lengths that are bound to powers of 2. A few recent works have proposed construction methods based on multiple kernels of different dimensions, not only binary ones, allowing code lengths different from powers of 2. In this paper, we design and implement the first multi-kernel successive cancellation polar code decoder in literature. It can decode any code constructed with binary and ternary kernels: the architecture, sized for a maximum code length Nmax, is fully flexible in terms of code length, code rate, and kernel sequence. The decoder can achieve a frequency of over 1 GHz in 65 nm CMOS technology, and a throughput of 615 Mb/s. The area occupation ranges between 0.11 mm² for Nmax=256 and 2.01 mm² for Nmax=4096. Implementation results show an unprecedented degree of flexibility: with Nmax=4096, up to 55 code lengths can be decoded with the same hardware, along with any kernel sequence and code rate.},
  doi      = {10.1109/TCSI.2018.2855679},
  file     = {:pdf/Coppolino2018 - A Multi-Kernel Multi-Code Polar Decoder Architecture.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Decoding, Kernel, Computer architecture, Encoding, Nickel, Hardware, Indexes, Polar codes, multi-kernel, successive-cancellation decoding, hardware implementation, multi kernel, multi-kernel},
}

@InProceedings{Bioglio2017b,
  author    = {V. Bioglio and F. Gabry and I. Land and J. Belfiore},
  title     = {Minimum-Distance Based Construction of Multi-Kernel Polar Codes},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2017},
  pages     = {1--6},
  month     = dec,
  publisher = {IEEE},
  abstract  = {In this paper, we propose a construction for multi-kernel polar codes based on the maximization of the minimum distance. Compared to the original construction based on density evolution, our new design shows particular advantages for short code lengths, where the polarization effect has less impact on the performance than the distances of the code. We introduce and compute the minimum-distance profile and provide a simple greedy algorithm for the code design. Compared to state-of-the-art punctured or shortened Arikan polar codes, multi-kernel polar codes with our new design show significantly improved error-rate performance.},
  doi       = {10.1109/GLOCOM.2017.8254147},
  file      = {:pdf/Bioglio2017b - Minimum-Distance Based Construction of Multi-Kernel Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {block codes, channel coding, computational complexity, error correction codes, error statistics, greedy algorithms, linear codes, minimum-distance based construction, multikernel polar codes, polarization effect, Arikan polar codes, greedy algorithm, Kernel, Decoding, Encoding, Error analysis, Reliability engineering, Algorithm design and analysis, multi kernel, multi-kernel},
}

@InProceedings{Benammar2017,
  author    = {M. Benammar and V. Bioglio and F. Gabry and I. Land},
  title     = {Multi-Kernel Polar Codes: Proof of Polarization and Error Exponents},
  booktitle = {Information Theory Workshop (ITW)},
  year      = {2017},
  pages     = {101--105},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In this paper, we investigate a novel family of polar codes based on multi-kernel constructions, proving that this construction actually polarizes. To this end, we derive a new and more general proof of polarization, which gives sufficient conditions for kernels to polarize. Finally, we derive the convergence rate of the multi-kernel construction and relate it to the convergence rate of each of the constituent kernels.},
  doi       = {10.1109/ITW.2017.8277949},
  file      = {:pdf/Benammar2017 - Multi-Kernel Polar Codes\: Proof of Polarization and Error Exponents.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {convergence, theorem proving, proof of polarization, constituent kernels, convergence rate, multikernel construction, error exponents, multikernel polar codes, Kernel, Convergence, Reliability, Random variables, Decoding, Conferences, multi kernel, multi-kernel},
}

@Article{Hanif2017,
  author   = {M. Hanif and M. Ardakani},
  title    = {Fast Successive-Cancellation Decoding of Polar Codes: Identification and Decoding of New Nodes},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {11},
  pages    = {2360--2363},
  month    = nov,
  issn     = {1089-7798},
  abstract = {The decoding latency of polar codes can be reduced by implementing fast parallel decoders in the last stages of decoding. In this letter, we present five such decoders corresponding to different frozen-bit sequences to improve the decoding speed of polar codes. Implementing them achieves significant latency reduction without tangibly altering the bit-error-rate performance of the code.},
  comment  = {node, nodes, type 1, type 2, type 3, type 4, type 5, type I, type II, type III, type IV, type V},
  doi      = {10.1109/LCOMM.2017.2740305},
  file     = {:pdf/Hanif2017 - Fast Successive-Cancellation Decoding of Polar Codes\: Identification and Decoding of New Nodes.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {Maximum likelihood decoding, Systematics, Indexes, Corporate acquisitions, Block codes, Polar codes, maximum a posteriori, maximum likelihood, systematic codes, non-systematic codes, node, nodes, type 1, type 2, type 3, type 4, type 5, type I, type II, type III, type IV, type V},
}

@Article{Giard2018,
  author    = {P. Giard and A. Balatsoukas-Stimming and G. Sarkis and C. Thibeault and W. J. Gross},
  title     = {Fast Low-Complexity Decoders for Low-Rate Polar Codes},
  journal   = {Springer Journal of Signal Processing Systems (JSPS)},
  year      = {2018},
  volume    = {90},
  number    = {5},
  pages     = {675},
  month     = may,
  abstract  = {Polar codes are capacity-achieving error-correcting codes with an explicit construction that can be decoded with low-complexity algorithms. In this work, we show how the state-of-the-art low-complexity decoding algorithm can be improved to better accommodate low-rate codes. More constituent codes are recognized in the updated algorithm and dedicated hardware is added to efficiently decode these new constituent codes. We also alter the polar code construction to further decrease the latency and increase the throughput with little to no noticeable effect on error-correction performance. Rate-flexible decoders for polar codes of length 1024 and 2048 are implemented on FPGA. Over the previous work, they are shown to have from 22\% to 28\% lower latency and 26\% to 34\% greater throughput when decoding low-rate codes. On 65 nm ASIC CMOS technology, the proposed decoder for a (1024, 512) polar code is shown to compare favorably against the state-of-the-art ASIC decoders. With a clock frequency of 400 MHz and a supply voltage of 0.8 V, it has a latency of 0.41 μs and an area efficiency of 1.8 Gbps/mm 2 for an energy efficiency of 77 pJ/info. bit. At 600 MHz with a supply of 1 V, the latency is reduced to 0.27 μs and the area efficiency increased to 2.7 Gbps/mm 2 at 115 pJ/info. bit.},
  date      = {2018-05-01},
  doi       = {10.1007/s11265-016-1173-y},
  file      = {:pdf/Giard2018 - Fast Low-Complexity Decoders for Low-Rate Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {nodes, node, rep 1, rep one, rep1, rep-1, 0RepSpc, zero rep spc, 0-rep-spc, 001},
  publisher = {Springer},
}

@Article{Shin2013,
  author   = {D. Shin and S. Lim and K. Yang},
  title    = {Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2013},
  volume   = {61},
  number   = {7},
  pages    = {2593--2599},
  month    = jul,
  issn     = {0090-6778},
  abstract = {Length-compatible polar codes are a class of polar codes which can support a wide range of lengths with a single pair of encoder and decoder. In this paper we propose a method to construct length-compatible polar codes by employing the reduction of the 2\textsuperscript{n}× 2\textsuperscript{n}polarizing matrix proposed by Arikan. The conditions under which a reduced matrix becomes a polarizing matrix supporting a polar code of a given length are first analyzed. Based on these conditions, length-compatible polar codes are constructed in a suboptimal way by codeword-puncturing and information-refreezing processes. They have low encoding and decoding complexity since they can be encoded and decoded in a similar way as a polar code of length 2\textsuperscript{n}. Numerical results show that length-compatible polar codes designed by the proposed method provide a performance gain of about 1.0 - 5.0 dB over those obtained by random puncturing when successive cancellation decoding is employed.},
  doi      = {10.1109/TCOMM.2013.052013.120543},
  file     = {:pdf/Shin2013 - Design of Length-Compatible Polar Codes Based on the Reduction of Polarizing Matrices.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {decoding, matrix algebra, random codes, polarizing matrix reduction, encoder, decoder, length-compatible polar code, codeword-puncturing, information-refreezing process, encoding complexity, decoding complexity, Decoding, Encoding, Vectors, Probability density function, Complexity theory, Symmetric matrices, Matrix decomposition, Polar codes, polarizing matrix, density evolution, frozen bit, matrix reduction, puncturing},
}

@Article{Bose1960,
  author   = {R.C. Bose and D.K. Ray-Chaudhuri},
  title    = {On a Class of Error Correcting Binary Group Codes},
  journal  = {Elsevier Information and Control},
  year     = {1960},
  volume   = {3},
  number   = {1},
  pages    = {68--79},
  issn     = {0019-9958},
  abstract = {A general method of constructing error correcting binary group codes is obtained. A binary group code with n places, k of which are information places is called an (n,k) code. An explicit method of constructing t-error correcting (n,k) codes is given for n = 2m−1 and k = 2m−1−R(m,t) ≧ 2m−1−mt where R(m,t) is a function of m and t which cannot exceed mt. An example is worked out to illustrate the method of construction.},
  file     = {:pdf/Bose1960 - On a Class of Error Correcting Binary Group Codes.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {BCH},
  doi      = {10.1016/S0019-9958(60)90287-4},
}

@InProceedings{Divsalar1998,
  author    = {D. Divsalar and H. Jin, and R. J. McEliece},
  title     = {Coding Theorems for "Turbo-like" Codes},
  booktitle = {Allerton Conference on Communication, Control and Computing},
  year      = {1998},
  pages     = {201--210},
  month     = sep,
  abstract  = {In this paper we discuss AWGN coding theorems for ensembles of coding systems which are built from fixed convolutional codes interconnected with random interleavers.  We call these systems “turbo-like” codes and they include as special cases both the classical turbo codes [1,2,3] and the serial concatentation of interleaved convolutional codes [4]. We offer a general conjecture about the behavior of the ensemble (maximum-likelihood decoder) word error probability as the word length approches infinity.  We prove this conjecture for a simple class of rate 1/q serially concatenated codes where the outer code is a q-fold repetition code and the inner code is a rate 1 convolutional code with transfer function 1 / (1 + D). We believe this represents the first rigorous proof of a coding theorem for turbo-like codes.},
  file      = {:pdf/Divsalar1998 - Coding Theorems for Turbo-like Codes.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC)},
  keywords  = {Repeat and Accumulate, RA},
  url       = {https://pdfs.semanticscholar.org/b5cc/c94d4f9ea6df991190f17359ddd7ac47f005.pdf},
}

@Article{Reed1960,
  author   = {I. Reed and G. Solomon},
  title    = {Polynomial Codes Over Certain Finite Fields},
  journal  = {Journal of the Society for Industrial and Applied Mathematics},
  year     = {1960},
  volume   = {8},
  number   = {2},
  pages    = {300--304},
  doi      = {10.1137/0108018},
  file     = {:pdf/Reed1960 - Polynomial Codes Over Certain Finite Fields.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC)},
  keywords = {Reed-Solomon, RS},
}

@InProceedings{Leonardon2018b,
  author    = {M. L\'eonardon and C. Leroux and P. J{\"a}{\"a}skel{\"a}inen and C. J\'ego and Y. Savaria},
  title     = {Transport Triggered Polar Decoders},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2018},
  publisher = {IEEE},
  abstract  = {In this paper, the first transport triggered architecture (TTA) customized for the decoding of polar codes is proposed. A first version of this programmable processor is optimized for the successive cancellation (SC) decoding of polar codes while a second architecture is further specialized to also support Soft CANcellation (SCAN) decoding. Both architectures were fully validated on FPGA device by prototyping. The first architecture was also synthesized in 28nm ASIC technology. The designed processor runs at a frequency of 800 MHz and reaches a throughput of 352 Mbps for a (1024, 512) polar code decoded with the SC algorithm. Compared to previous work, the energy consumption is reduced by an order of magnitude (0.14 nJ / bit) and the throughput is increased fivefold. Compared to an optimized software implementation on a general purpose processor (x86 architecture), the throughput is 37\% higher and the energy consumption is two orders of magnitude lower. TTA can be seen as a way to reduce the gap between programmable and dedicated polar decoders.},
  doi       = {10.1109/ISTC.2018.8625310},
  file      = {:pdf/Leonardon2018b - Transport Triggered Polar Decoders.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders, ASIP},
  keywords  = {hardware decoder, SCAN, SC, TTA, Transport Triggered Architectures, polar decoder},
}

@InProceedings{Leroux2011,
  author    = {C. Leroux and I. Tal and A. Vardy and W. J. Gross},
  title     = {Hardware Architectures for Successive Cancellation Decoding of Polar Codes},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2011},
  pages     = {1665-1668},
  month     = {May},
  publisher = {IEEE},
  abstract  = {The recently-discovered polar codes are widely seen as a major breakthrough in coding theory. These codes achieve the capacity of many important channels under successive cancellation decoding. Motivated by the rapid progress in the theory of polar codes, we pro pose a family of architectures for efficient hardware implementation of successive cancellation decoders. We show that such decoders can be implemented with O(n) processing elements and O(n) memory elements, while providing constant throughput. We also pro pose a technique for overlapping the decoding of several consecutive codewords, thereby achieving a significant speed-up factor. We furthermore show that successive cancellation decoding can be implemented in the logarithmic domain, thereby eliminating the multiplication and division operations and greatly reducing the complexity of each processing element.},
  doi       = {10.1109/ICASSP.2011.5946819},
  file      = {:pdf/Leroux2011 - Hardware Architectures for Successive Cancellation of Polar Codes.pdf:PDF},
  groups    = {Hardware Decoders, Polar Codes},
  issn      = {2379-190X},
  keywords  = {block codes;channel coding;computational complexity;error correction codes;linear codes;hardware architecture;successive cancellation decoding;polar code;consecutive codeword;speed-up factor;logarithmic domain;Decoding;Computer architecture;Complexity theory;Hardware;Registers;Throughput;Processor scheduling;Polar codes;successive cancellation decoding;hardware implementation;VLSI, block codes;channel coding;computational complexity;error correction codes;linear codes;consecutive codeword;hardware architecture;logarithmic domain;polar code;speed-up factor;successive cancellation decoding;Complexity theory;Computer architecture;Decoding;Hardware;Processor scheduling;Registers;Throughput;Polar codes;VLSI;hardware implementation;successive cancellation decoding},
}

@Article{Giard2017,
  author    = {P. Giard and A. Balatsoukas-Stimming and T. C. Müller and A. Bonetti and C. Thibeault and W. J. Gross and P. Flatresse and A. Burg},
  title     = {PolarBear: A 28-nm {FD-SOI} {ASIC} for Decoding of Polar Codes},
  journal   = {Journal on Emerging and Selected Topics in Circuits and Systems},
  year      = {2017},
  volume    = {7},
  number    = {4},
  pages     = {616--629},
  month     = {Dec},
  issn      = {2156-3357},
  abstract  = {Polar codes are a recently proposed class of block codes that provably achieve the capacity of various communication channels. They received a lot of attention as they can do so with low-complexity encoding and decoding algorithms, and they have an explicit construction. Their recent inclusion in a 5G communication standard will only spur more research. However, only a couple of ASICs featuring decoders for polar codes were fabricated, and none of them implements a list-based decoding algorithm. In this paper, we present ASIC measurement results for a fabricated 28-nm CMOS chip that implements two different decoders: the first decoder is tailored toward error-correction performance and flexibility. It supports any code rate as well as three different decoding algorithms: successive cancellation (SC), SC flip, and SC list (SCL). The flexible decoder can also decode both non-systematic and systematic polar codes. The second decoder targets speed and energy efficiency. We present measurement results for the first silicon-proven SCL decoder, where its coded throughput is shown to be of 306.8 Mbps with a latency of 3.34 us and an energy per bit of 418.3 pJ/b at a clock frequency of 721 MHz for a supply of 1.3 V. The energy per bit drops down to 178.1 pJ/b with a more modest clock frequency of 308 MHz, lower throughput of 130.9 Mbps and a reduced supply voltage of 0.9 V. For the other two operating modes, the energy per bit is shown to be of approximately 95 pJ/b. The less flexible high-throughput unrolled decoder can achieve a coded throughput of 9.2 Gbps and a latency of 628 ns for a measured energy per bit of 1.15 pJ/b at 451 MHz.},
  doi       = {10.1109/JETCAS.2017.2745704},
  file      = {:pdf/Giard2017 - PolarBear\: A 28-nm FD-SOI ASIC for Decoding of Polar Codes.pdf:PDF},
  groups    = {Hardware Decoders, Polar Codes},
  keywords  = {application specific integrated circuits;block codes;CMOS integrated circuits;decoding;elemental semiconductors;error correction codes;silicon;UHF integrated circuits;block codes;communication channels;5G communication standard;error-correction performance;systematic polar codes;CMOS chip;PolarBear;low-complexity encoding algorithm;low-complexity decoding algorithm;FD-SOI ASIC measurement;list-based decoding algorithm;successive cancellation;SC flip;SC list;nonsystematic polar code;energy efficiency;silicon-proven SCL decoder;frequency 451.0 MHz;frequency 721.0 MHz;voltage 1.3 V;frequency 308.0 MHz;voltage 0.9 V;time 628.0 ns;size 28 nm;bit rate 306.8 Mbit/s;time 3.34 mus;bit rate 130.9 Mbit/s;bit rate 9.2 Gbit/s;Si;Decoding;Block codes;Error correction;Application specific integrated circuits;5G mobile communication;Polar codes;ASIC;successive cancellation;SC flip;SC list},
  publisher = {IEEE},
}

@InProceedings{Giard2016c,
  author    = {P. Giard and G. Sarkis and A. Balatsoukas-Stimming and Y. Fan and C. Tsui and A. Burg and C. Thibeault and W. J. Gross},
  title     = {Hardware Decoders for Polar Codes: An Overview},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2016},
  pages     = {149--152},
  month     = {May},
  publisher = {IEEE},
  abstract  = {Polar codes are an exciting new class of error correcting codes that achieve the symmetric capacity of memoryless channels. Many decoding algorithms were developed and implemented, addressing various application requirements: from error-correction performance rivaling that of LDPC codes to very high throughput or low-complexity decoders. In this work, we review the state of the art in polar decoders implementing the successive-cancellation, belief propagation, and list decoding algorithms, illustrating their advantages.},
  doi       = {10.1109/ISCAS.2016.7527192},
  file      = {:pdf/Giard2016c - Hardware Decoders for Polar Codes\: An Overview.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  issn      = {2379-447X},
  keywords  = {decoding;error correction codes;hardware decoders;polar codes;error correcting codes;memoryless channels;decoding algorithms;LDPC codes;high throughput decoders;low-complexity decoders;successive-cancellation algorithm;belief propagation algorithm;list decoding algorithm;Decoding;Iterative decoding;Hardware;Throughput;Clocks},
}

@Article{Dizdar2016,
  author    = {O. Dizdar and E. Arıkan},
  title     = {A High-Throughput Energy-Efficient Implementation of Successive Cancellation Decoder for Polar Codes Using Combinational Logic},
  journal   = {Transactions on Circuits and Systems I: Regular Papers (TCAS1)},
  year      = {2016},
  volume    = {63},
  number    = {3},
  pages     = {436--447},
  month     = {March},
  issn      = {1549-8328},
  abstract  = {This paper proposes a high-throughput energy-efficient Successive Cancellation (SC) decoder architecture for polar codes based on combinational logic. The proposed combinational architecture operates at relatively low clock frequencies compared to sequential circuits, but takes advantage of the high degree of parallelism inherent in such architectures to provide a favorable tradeoff between throughput and energy efficiency at short to medium block lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder that combines the advantageous aspects of the combinational decoder with the low-complexity nature of sequential-logic decoders. Performance characteristics on ASIC and FPGA are presented with a detailed power consumption analysis for combinational decoders. Finally, the paper presents an analysis of the complexity and delay of combinational decoders, and of the throughput gains obtained by hybrid-logic decoders with respect to purely synchronous architectures.},
  doi       = {10.1109/TCSI.2016.2525020},
  file      = {:pdf/Dizdar2016 - A High-Throughput Energy-Efficient Implementation of Successive Cancellation Decoder for Polar Codes Using Combinational Logic.pdf:PDF},
  groups    = {Hardware Decoders, Polar Codes},
  keywords  = {application specific integrated circuits;combinational circuits;decoding;field programmable gate arrays;sequential circuits;polar codes;combinational logic;high-throughput energy-efficient successive cancellation decoder architecture;low clock frequency;energy efficiency;medium block lengths;hybrid-logic SC decoder;sequential-logic decoders;combinational decoder delay;ASIC;FPGA;power consumption analysis;synchronous architectures;Decoding;Throughput;Clocks;Complexity theory;Hardware;Encoding;Power demand;Energy efficiency;error correcting codes;polar codes;successive cancellation decoder;VLSI;Energy efficiency;error correcting codes;polar codes;successive cancellation decoder;VLSI},
  publisher = {IEEE},
}

@InProceedings{LeGal2016a,
  author    = {B. Le Gal and C. Leroux and C. Jego},
  title     = {A Scalable 3-Phase Polar Decoder},
  booktitle = {International Symposium on Circuits and Systems (ISCAS)},
  year      = {2016},
  pages     = {417--420},
  month     = {May},
  publisher = {IEEE},
  abstract  = {In this paper, we propose a 3-phase polar codes Successive Cancellation (SC) decoder. Benefiting from the local properties of the decoding tree, 3 zones are defined and associated to 3 distinct sub-decoders. This approach reduces the memory footprint while guaranteeing a better scalability in comparison with state of the art SC decoders. Several 3-phase SC decoders are implemented on an FPGA circuit and compare favorably to state-of-the-art implementations in terms of distributed hardware resources (LUT, D-FF) and throughput. Moreover, the memory of the decoder is reduced by 37\% for a N = 2^21 polar codes.},
  doi       = {10.1109/ISCAS.2016.7527259},
  file      = {:pdf/LeGal2016a - A Scalable 3-Phase Polar Decoder.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  issn      = {2379-447X},
  keywords  = {codes;decoding;field programmable gate arrays;trees (mathematics);scalable 3-phase polar decoder;distributed hardware resources;FPGA circuit;memory footprint reduction;distinct subdecoders;decoding tree;3-phase SC decoders;3-phase polar codes successive cancellation decoder;Decoding;Random access memory;Computer architecture;Registers;Scalability;Pipelines;Clocks},
}

@InProceedings{Giard2015a,
  author    = {Giard, P. and Sarkis, G. and Thibeault, C. and Gross, W. J.},
  title     = {{A 638 {Mbps} Low-Complexity Rate 1/2 Polar Decoder on {FPGAs}}},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2015},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Polar codes are capacity-achieving error-correcting codes with an explicit construction that can be decoded with low-complexity algorithms. In this work, we show how the state-of-the-art low-complexity decoding algorithm can be improved to better accommodate low-rate codes. Dedicated hardware is added to efficiently decode new constituent codes. Also, we use polar code construction alteration to further improve the latency and throughput. A polar decoder for a (1024, 512) code is implemented on two different FPGAs. It has 25\% lower latency over the previous work and a coded throughput of 436 Mbps and 638 Mbps on the Xilinx Virtex 6 and Altera Stratix IV FPGAs, respectively.},
  doi       = {10.1109/SiPS.2015.7345007},
  file      = {:pdf/Giard2015a - A 638 Mbps Low-Complexity Rate 1 2 Polar Decoder on FPGAs.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {Altera Stratix IV FPGA, capacity-achieving error-correcting codes, Clocks, decoding, error correction codes, field programmable gate arrays, Field programmable gate arrays, Hardware, low-complexity decoding algorithm, Maximum likelihood decoding, polar code construction alteration, polar decoder, Reliability, Throughput, Xilinx Virtex 6},
}

@InProceedings{Berhault2015,
  author    = {Berhault, G. and Leroux, C. and J\'ego, C. and Dallet, D.},
  title     = {{Hardware Implementation of a Soft Cancellation Decoder for Polar Codes}},
  booktitle = {Design and Architectures for Signal and Image Processing Conference (DASIP)},
  year      = {2015},
  publisher = {IEEE},
  abstract  = {Polar Codes can provably achieve the capacity of discrete memoryless channels. In order to make practical, it is necessary to propose efficient hardware decoder architectures. In this paper, the first hardware decoder architecture implementing the Soft-output CANcellation (SCAN) decoding algorithm, is presented. This decoder was implemented on Field Programmable Gate Array (FPGA) devices. The proposed architecture is parametrizable for any number of iterations without adding hardware complexity. The SCAN decoder architecture is compared to another soft-output decoder that implements a Belief Propagation (BP) algorithm. The SCAN decoder can reach a higher throughput than a BP decoder, with a lower memory footprint. Moreover, only one iteration with the SCAN algorithm leads to better decoding performance than 50 iterations of the BP algorithm.},
  doi       = {10.1109/DASIP.2015.7367252},
  file      = {:pdf/Berhault2015 - Hardware Implementation of a Soft Cancellation Decoder for Polar Codes.pdf:PDF},
  groups    = {Polar Codes, Hardware Decoders},
  keywords  = {belief propagation, BP algorithm, BP decoder, codecs, codes, Computer architecture, decoding, Decoding, discrete memoryless channels, Encoding, field programmable gate array, field programmable gate arrays, FPGA devices, Hardware, hardware decoder architectures, Iterative decoding, iterative methods, logic design, Logic gates, polar codes, SCAN decoder architecture, SCAN decoding algorithm, soft cancellation decoder, soft-output cancellation, Systematics},
}

@InBook{Jaaskelainen2017,
  pages     = {147--164},
  title     = {HW/SW Co-design Toolset for Customization of Exposed Datapath Processors},
  publisher = {Springer International Publishing},
  year      = {2017},
  author    = {J{\"a}{\"a}skel{\"a}inen, Pekka and Viitanen, Timo and Takala, Jarmo and Berg, Heikki},
  editor    = {Hussain, Waqar and Nurmi, Jari and Isoaho, Jouni and Garzia, Fabio},
  isbn      = {978-3-319-49679-5},
  booktitle = {Computing Platforms for Software-Defined Radio},
  doi       = {10.1007/978-3-319-49679-5_8},
  file      = {:pdf/Jaaskelainen2017 - HW SW Co-design Toolset for Customization of Exposed Datapath Processors.pdf:PDF},
  groups    = {ASIP},
  keywords  = {tta,tce},
}

@InProceedings{Esko2010,
  author    = {O. Esko and P. Jaaskelainen and P. Huerta and C. S. de La Lama and J. Takala and J. I. Martinez},
  title     = {Customized Exposed Datapath Soft-Core Design Flow with Compiler Support},
  booktitle = {International Conference on Field Programmable Logic and Applications (FPL)},
  year      = {2010},
  pages     = {217--222},
  month     = {Aug},
  publisher = {IEEE},
  abstract  = {A popular way to exploit high level programming languages in FPGA designs is to use a soft-core with accompanying software development tools. However, a common shortcoming with the current soft-core offerings is their limited software execution capability: the required performance for the implementation can be often reached only with instruction set extensions. In this paper, we propose and evaluate an application-specific processor design toolset that uses a multi-issue exposed data path processor architecture template. The main benefit of the architecture is scalability with respect to instruction-level parallelism (ILP). The design flow allows the designer to freely customize the data path resources in the core to exploit the ILP available in computation intensive kernels. The design toolset includes a retargetable C compiler and an architecture simulator, making design space exploration feasible. The experiments show that a relatively small soft-core tailored with the toolset provides significant speedups on software execution without using any instruction set extensions. The best measured speedup in comparison to the major commercial soft-cores was fourfold in applications from the CHStone benchmark suite, while the amount of consumed FPGA resources remained moderate.},
  doi       = {10.1109/FPL.2010.51},
  file      = {:pdf/Esko2010 - Customized Exposed Datapath Soft-Core Design Flow with Compiler Support.pdf:PDF},
  groups    = {ASIP},
  issn      = {1946-1488},
  keywords  = {application specific integrated circuits;field programmable gate arrays;high level languages;instruction sets;logic design;program compilers;software tools;high level programming languages;FPGA designs;software development tools;software execution capability;instruction set;application specific processor design toolset;data path processor architecture template;instruction level parallelism;soft core design flow;ILP;retargetable C compiler;architecture simulator;CHStone benchmark suite;Field programmable gate arrays;Computer architecture;Hardware;VLIW;Radio frequency;Generators;Programming},
}

@InProceedings{Chandesris2017,
  author    = {L. Chandesris and V. Savin and D. Declercq},
  title     = {On Puncturing Strategies for Polar Codes},
  booktitle = {International Conference on Communications Workshops (ICC)},
  year      = {2017},
  pages     = {766--771},
  month     = {May},
  publisher = {IEEE},
  abstract  = {This paper introduces a class of specific puncturing patterns, called symmetric puncturing patterns, which can be characterized and generated from the rows of the generator matrix G^N. They are first shown to be non-equivalent, then a low-complexity method to generate symmetric puncturing patterns is proposed, which performs a search tree algorithm with limited depth, over the rows of G^N. Symmetric patterns are further optimized by density evolution, and shown to yield better performance than state-of-the-art rate compatible code constructions, relying on either puncturing or shortening techniques.},
  doi       = {10.1109/ICCW.2017.7962751},
  file      = {:pdf/Chandesris2017 - On Puncturing Strategies for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2474-9133},
  keywords  = {error correction codes;matrix algebra;tree searching;symmetric puncturing patterns;generator matrix GN;low-complexity method;search tree algorithm;density evolution;puncturing techniques;shortening techniques;polar codes;Decoding;Encoding;Optimization;Electronics packaging;5G mobile communication;Generators;Symmetric matrices;Punctured polar codes;equivalent puncturing patterns;symmetric puncturing patterns},
}

@InProceedings{Chandesris2016,
  author    = {L. Chandesris and V. Savin and D. Declercq},
  title     = {An Improved {SC}Flip Decoder for Polar Codes},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2016},
  pages     = {1--6},
  month     = {Dec},
  publisher = {IEEE},
  abstract  = {This paper focuses on the recently introduced Successive Cancellation Flip (SCFlip) decoder of polar codes. Our contribution is twofold. First, we propose the use of an optimized metric to determine the flipping positions within the SCFlip decoder, which improves its ability to find the first error that occurred during the initial SC decoding attempt. We also show that the proposed metric allows closely approaching the performance of an ideal SCFlip decoder. Second, we introduce a generalisation of the SCFlip decoder to a number of ω nested flips, denoted by SCFlip-ω, using a similar optimized metric to determine the positions of the nested flips. We show that the SCFlip-2 decoder yields significant gains in terms of decoding performance and competes with the performance of the CRC-aided SC-List decoder with list size L=4, while having an average decoding complexity similar to that of the standard SC decoding, at medium to high signal to noise ratio.},
  doi       = {10.1109/GLOCOM.2016.7841594},
  file      = {:pdf/Chandesris2016 - An Improved SCFlip Decoder for Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {codecs;codes;SCFlip decoder;polar codes;successive cancellation flip decoder;signal to noise ratio;Maximum likelihood decoding;Measurement;Standards;Signal to noise ratio;Computational complexity},
}

@Article{Ercan2018,
  author   = {F. Ercan and C. Condo and W. J. Gross},
  title    = {Improved Bit-Flipping Algorithm for Successive Cancellation Decoding of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2018},
  pages    = {1},
  issn     = {0090-6778},
  abstract = {The interest in polar codes has been increasing significantly since their adoption for use in the 5th generation wireless systems standard. Successive cancellation (SC) decoding algorithm has low implementation complexity, but yields mediocre error-correction performance at the code lengths of interest. SC-Flip algorithm improves the error-correction performance of SC by identifying possibly erroneous decisions made by SC and re-iterates after flipping one bit. It was recently shown that only a portion of bit-channels are most likely to be in error. In this work, we investigate the average log-likelihood ratio (LLR) values and their distribution related to the erroneous bitchannels, and develop the Thresholded SC-Flip (TSCF) decoding algorithm. We also replace the LLR selection and sorting of SCFlip with a comparator to reduce the implementation complexity. Simulation results demonstrate that for practical code lengths and a wide range of rates, TSCF shows negligible loss compared to the error-correction performance obtained when all singleerrors are corrected. At matching maximum iterations, TSCF has an error-correction performance gain of up to 0.45 dB compared with SC-Flip decoding. At matching error-correction performance, the computational complexity of TSCF is reduced by up to 40\% on average, and requires up to 5× lower maximum number of iterations.},
  doi      = {10.1109/TCOMM.2018.2873322},
  file     = {:pdf/Ercan2018 - Improved Bit-Flipping Algorithm for Successive Cancellation Decoding of Polar Codes.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Polar Codes},
  keywords = {Decoding;Iterative decoding;Encoding;5G mobile communication;Reliability;Estimation;Frequency estimation;polar codes;SC-Flip decoding;5G;error-correction performance},
}

@InProceedings{Zhang2017a,
  author    = {Z. Zhang and K. Qin and L. Zhang and H. Zhang and G. T. Chen},
  title     = {Progressive Bit-Flipping Decoding of Polar Codes over Layered Critical Sets},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2017},
  pages     = {1--6},
  month     = {Dec},
  publisher = {IEEE},
  abstract  = {In successive cancellation (SC) polar decoding, an incorrect estimate of any prior unfrozen bit may bring about severe error propagation in the following decoding, thus it is desirable to find out and correct an error as early as possible. In this paper, we first construct a critical set S of unfrozen bits, which with high probability (typically &gt;99\%) includes the bit where the first error happens. Then we develop a progressive multi- level bit-flipping decoding algorithm to correct multiple errors over the multiple-layer critical sets each of which is constructed using the remaining undecoded subtree associated with the previous layer. The level in fact indicates the number of independent errors that could be corrected. We show that as the level increases, the block error rate (BLER) performance of the proposed progressive bit flipping decoder competes with the corresponding cyclic redundancy check (CRC) aided successive cancellation list (CA-SCL) decoder, e.g., a level 4 progressive bit-flipping decoder is comparable to the CA-SCL decoder with a list size of L=32. Furthermore, the average complexity of the proposed algorithm is much lower than that of a SCL decoder (and is similar to that of SC decoding) at medium to high signal to noise ratio (SNR).},
  doi       = {10.1109/GLOCOM.2017.8254149},
  file      = {:pdf/Zhang2017a - Progressive Bit-Flipping Decoding of Polar Codes Over Layered Critical Sets.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC), Polar Codes},
  keywords  = {cyclic redundancy check codes;decoding;error correction codes;probability;set theory;trees (mathematics);unfrozen bits;progressive multi level bit;decoding algorithm;multiple errors;multiple-layer critical sets;independent errors;block error rate performance;CA-SCL decoder;SC decoding;progressive bit-flipping decoding;polar codes;layered critical sets;successive cancellation polar decoding;error propagation;error correction codes;probability;undecoded subtree;BLER performance;cyclic redundancy check aided successive cancellation list decoder;CRC decoder;average complexity;signal to noise ratio;SNR;Decoding;Encoding;Binary trees;Iterative decoding;Signal to noise ratio;Computational complexity},
}

@Article{Zhang2018,
  author   = {Z. Zhang and K. Qin and L. Zhang and G. T. Chen},
  title    = {Progressive Bit-Flipping Decoding of Polar Codes: A Critical-Set Based Tree Search Approach},
  journal  = {IEEE Access},
  year     = {2018},
  volume   = {6},
  pages    = {57738--57750},
  issn     = {2169-3536},
  abstract = {In successive cancellation (SC) polar decoding, an incorrect estimate of any prior unfrozen bit may bring about severe error propagation in the following decoding, and thus it is desirable to find out and correct an error as early as possible. In this paper, we investigate a progressive bit-flipping decoder which corrects at most L-independent errors in SC decoding. In particular, we first study the distribution of the first error position in SC decoding, and a critical set which with high probability includes the bit where the first error occurs regardless of the channel realizations is proposed. Second, a progressive bit-flipping decoding algorithm is proposed based on a search tree, which is established with a modified critical set in a progressive manner. The maximum level of the search tree is shown to coincide well with the number of independent errors that could be corrected. On this basis, the lower bound on BLER performance of a progressive bit-flipping decoder which corrects at most L errors is derived, and we show the bound can be tightly achieved by the proposed algorithm for some L. Moreover, an early-terminated bit-flipping (ET-BitFlipping) decoder is proposed to reduce the computational complexity and decoding latency of the original progressive bit-flipping scheme. Finally, numerical results show that the proposed ET-bit-flipping decoders can provide almost the same BLER performance as the state-of-the-art cyclic redundancy check-aided SC list decoders, with an average computational complexity and decoding latency similar to that of the SC decoder at medium to a high SNR regime.},
  doi      = {10.1109/ACCESS.2018.2873821},
  file     = {:pdf/Zhang2018 - Progressive Bit-Flipping Decoding of Polar Codes\: A Critical-Set Based Tree Search Approach.pdf:PDF},
  groups   = {Error-Correcting Codes (ECC), Polar Codes},
  keywords = {computational complexity;decoding;interference suppression;probability;tree searching;progressive bit-flipping decoding;successive cancellation polar decoding;progressive bit-flipping decoder;L-independent errors;SC decoding;search tree;ET-bit-flipping decoders;state-of-the-art cyclic redundancy check-aided SC list decoders;SC decoder;Decoding;Computational complexity;Encoding;Reliability;Iterative decoding;Cyclic redundancy check;Critical set;progressive bit-flipping;polar codes;search tree},
}

@InProceedings{Giard2018a,
  author    = {P. Giard and A. Burg},
  title     = {Fast-{SSC}-Flip Decoding of Polar Codes},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2018},
  pages     = {73--77},
  month     = {April},
  publisher = {IEEE},
  abstract  = {Polar codes are widely considered as one of the most exciting recent discoveries in channel coding. For short to moderate block lengths, their error-correction performance under list decoding can outperform that of other modern error-correcting codes. However, high-speed list-based decoders with moderate complexity are challenging to implement. Successive-cancellation (SC)-flip decoding was shown to be capable of a competitive error-correction performance compared to that of list decoding with a small list size, at a fraction of the complexity, but suffers from a variable execution time and a higher worst-case latency. In this work, we show how to modify the state-of-the-art high-speed SC decoding algorithm to incorporate the SC-flip ideas. The algorithmic improvements are presented as well as average execution-time results tailored to a hardware implementation. The results show that the proposed fast-SSC-flip algorithm has a decoding speed close to an order of magnitude better than the previous works while retaining a comparable error-correction performance.},
  doi       = {10.1109/WCNCW.2018.8369026},
  file      = {:pdf/Giard2018a - Fast-SSC-Flip Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {channel coding;decoding;error correction codes;parity check codes;fast-SSC-flip decoding;polar codes;channel coding;list decoding;variable execution time;fast-SSC-flip algorithm;error-correcting codes;error-correction performance;high-speed list-based decoders;successive-cancellation-flip decoding;high-speed SC decoding;Encoding;Throughput;Conferences;Complexity theory;Maximum likelihood decoding;Hardware},
}

@InProceedings{Condo2018,
  author    = {C. Condo and F. Ercan and W. J. Gross},
  title     = {Improved Successive Cancellation Flip Decoding of Polar Codes Based on Error Distribution},
  booktitle = {Wireless Communications and Networking Conference Workshops (WCNCW)},
  year      = {2018},
  pages     = {19--24},
  month     = {April},
  publisher = {IEEE},
  abstract  = {Polar codes are a class of linear block codes that provably achieves channel capacity, and have been selected as a coding scheme for 5thgeneration wireless communication standards. Successive-cancellation (SC) decoding of polar codes has mediocre error-correction performance on short to moderate codeword lengths: the SC-Flip decoding algorithm is one of the solutions that have been proposed to overcome this issue. On the other hand, SC-Flip has a higher implementation complexity compared to SC due to the required log-likelihood ratio (LLR) selection and sorting process. Moreover, it requires a high number of iterations to reach good error-correction performance. In this work, we propose two techniques to improve the SC-Flip decoding algorithm for low-rate codes, based on the observation of channel-induced error distributions. The first one is a fixed index selection (FIS) scheme to avoid the substantial implementation cost of LLR selection and sorting with no cost on error-correction performance. The second is an enhanced index selection (EIS) criterion to improve the error-correction performance of SC-Flip decoding. A reduction of 24.6\% in the implementation cost of logic elements is estimated with the FIS approach, while simulation results show that EIS leads to an improvement on error-correction performance improvement up to 0.34 dB at a target FER of 10-4.},
  doi       = {10.1109/WCNCW.2018.8368991},
  file      = {:pdf/Condo2018 - Improved Successive Cancellation Flip Decoding of Polar Codes Based on Error Distribution.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {block codes;channel capacity;decoding;error correction codes;error statistics;linear codes;parity check codes;error-correction performance improvement;improved successive cancellation flip decoding;polar codes;error distribution;linear block codes;coding scheme;successive-cancellation decoding;mediocre error-correction performance;SC-Flip decoding algorithm;sorting process;good error-correction performance;low-rate codes;channel-induced error distributions;fixed index selection scheme;enhanced index selection criterion;channel capacity;5th generation wireless communication standards;log-likelihood ratio selection;Decoding;Estimation;Indexes;Conferences;Encoding;Iterative decoding;Complexity theory},
}

@InProceedings{Ercan2018a,
  author    = {F. Ercan and C. Condo and S. A. Hashemi and W. J. Gross},
  title     = {Partitioned Successive-Cancellation Flip Decoding of Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2018},
  pages     = {1--6},
  month     = {May},
  publisher = {IEEE},
  abstract  = {Polar codes are a class of channel capacity achieving codes that has been selected for the next generation of wireless communication standards. Successive-cancellation (SC) is the first proposed decoding algorithm, suffering from mediocre errorcorrection performance at moderate code lengths. In order to improve the error-correction performance of SC, two approaches are available: (i) SC-List decoding which keeps a list of candidates by running a number of SC decoders in parallel, thus increasing the implementation complexity, and (ii) SC-Flip decoding that relies on a single SC module, and keeps the computational complexity close to SC. In this work, we propose the partitioned SC-Flip (PSCF) decoding algorithm, which outperforms SCFlip in terms of error-correction performance and average computational complexity, leading to higher throughput and reduced energy consumption per codeword. We also introduce a partitioning scheme that best suits our PSCF decoder. Simulation results show that at equivalent frame error rate, PSCF has up to 4.1× less computational complexity than the SC-Flip decoder. At equivalent average number of iterations, the error-correction performance of PSCF outperforms SC-Flip by up to 0.26 dB at frame error rate of 10-3.},
  doi       = {10.1109/ICC.2018.8422464},
  file      = {:pdf/Ercan2018a - Partitioned Successive-Cancellation Flip Decoding of Polar Codes.pdf:PDF},
  groups    = {Error-Correcting Codes (ECC), Polar Codes},
  issn      = {1938-1883},
  keywords  = {block codes;channel capacity;channel coding;computational complexity;decoding;error correction codes;iterative methods;linear codes;PSCF decoder;equivalent frame error rate;polar codes;channel capacity;wireless communication standards;SC-List decoding;SC decoders;implementation complexity;partitioned SC-Flip decoding algorithm;average computational complexity;partitioning scheme;mediocre error-correction performance;code lengths;partitioned successive-cancellation flip decoding;equivalent average number-of-iterations;Decoding;Partitioning algorithms;Iterative decoding;Computational complexity;Reliability;Signal to noise ratio;Encoding},
}

@Article{Guan2019,
  author   = {D. Guan and K. Niu and C. Dong and P. Zhang},
  title    = {Successive Cancellation Priority Decoding of Polar Codes},
  journal  = {IEEE Access},
  year     = {2019},
  pages    = {1},
  issn     = {2169-3536},
  abstract = {The successive cancellation list (SCL) decoding of polar codes can achieve the performance close to that of maximum-likelihood (ML) decoding. Nevertheless, a large list size results in high computational complexity. In this paper, a successive cancellation priority (SCP) decoding algorithm is proposed to reduce the time complexity. The SCP decoder performs a priority-first decoding, which is composed of a priority queue and a trellis. During the SCP decoding, The priority queue interact with the trellis iteratively. Conceptually, the priority queue stores the priority information and guides the extension of the candidate path. The trellis calculates and stores the intermediate results. Since most of the unnecessary path extensions are avoided by using the priority queue, the time complexity of the SCP decoder is much lower than that of the standard SCL decoder. Then a quantized priority queue (QPQ) is introduced to avoid the comparison operations in the path selection and to simplify the SCP decoder. Furthermore, we prove that the path extension of the SCP decoder is equivalent to the extension of the most reliable paths of the standard SCL decoder. Thus, the SCP decoder can achieve the same decoding performance as the standard SCL decoder.},
  doi      = {10.1109/ACCESS.2019.2890838},
  file     = {:pdf/Guan2019 - Successive Cancellation Priority Decoding of Polar Codes.pdf:PDF},
  keywords = {Maximum likelihood decoding;Standards;Measurement;Time complexity;Iterative decoding;Polar code;priority decoding;successive cancellation decoding;successive cancellation list},
}

@PhdThesis{Chandesris2019,
  author = {L. Chandesris},
  title  = {Contribution à la Construction et au Décodage des Codes Polaires},
  school = {Universit\'e de Cergy-Pontoise},
  year   = {2019},
  file   = {:pdf/Chandesris2019 - Contribution À La Construction Et Au Décodage Des Codes Polaires.pdf:PDF},
  groups = {Polar Codes},
}

@PhdThesis{Korada2009,
  author      = {Korada, Satish Babu},
  title       = {Polar Codes for Channel and Source Coding},
  school      = {Ecole Polytechnique Fédérale de Lausanne (EPFL)},
  year        = {2009},
  address     = {Lausanne},
  abstract    = {The two central topics of information theory are the  compression and the transmission of data. Shannon, in his  seminal work, formalized both these problems and determined  their fundamental limits. Since then the main goal of  coding theory has been to find practical schemes that  approach these limits. Polar codes, recently invented by  Arikan, are the first "practical" codes that are known to  achieve the capacity for a large class of channels. Their  code construction is based on a phenomenon called "channel  polarization". The encoding as well as the decoding  operation of polar codes can be implemented with O(N log N)  complexity, where N is the blocklength of the code. We show  that polar codes are suitable not only for channel coding  but also achieve optimal performance for several other  important problems in information theory. The first problem  we consider is lossy source compression. We construct polar  codes that asymptotically approach Shannon's  rate-distortion bound for a large class of sources. We  achieve this performance by designing polar codes according  to the "test channel", which naturally appears in Shannon's  formulation of the rate-distortion function. The encoding  operation combines the successive cancellation algorithm of  Arikan with a crucial new ingredient called "randomized  rounding". As for channel coding, both the encoding as well  as the decoding operation can be implemented with O(N log  N) complexity. This is the first known "practical" scheme  that approaches the optimal rate-distortion trade-off. We  also construct polar codes that achieve the optimal  performance for the Wyner-Ziv and the Gelfand-Pinsker  problems. Both these problems can be tackled using "nested"  codes and polar codes are naturally suited for this  purpose. We further show that polar codes achieve the  capacity of asymmetric channels, multi-terminal scenarios  like multiple access channels, and degraded broadcast  channels. For each of these problems, our constructions are  the first known "practical" schemes that approach the  optimal performance. The original polar codes of Arikan  achieve a block error probability decaying exponentially in  the square root of the block length. For source coding, the  gap between the achieved distortion and the limiting  distortion also vanishes exponentially in the square root  of the blocklength. We explore other polar-like code  constructions with better rates of decay. With this  generalization, we show that close to exponential decays  can be obtained for both channel and source coding. The new  constructions mimic the recursive construction of Arikan  and, hence, they inherit the same encoding and decoding  complexity. We also propose algorithms based on  message-passing to improve the finite length performance of  polar codes. In the final two chapters of this thesis we  address two important problems in graphical models related  to communications. The first problem is in the area of  low-density parity-check codes (LDPC). For practical  lengths, LDPC codes using message-passing decoding are  still the codes to beat. The current analysis, using  density evolution, evaluates the performance of these  algorithms on a tree. The tree assumption corresponds to  using an infinite length code. But in practice, the codes  are of finite length. We analyze the message-passing  algorithms for this scenario. The absence of tree  assumption introduces correlations between various  messages. We show that despite this correlation, the  prediction of the tree analysis is accurate. The second  problem we consider is related to code division multiple  access (CDMA) communication using random spreading. The  current analysis mainly focuses on the information  theoretic limits, i.e., using Gaussian input distribution.  However in practice we use modulation schemes like binary  phase-shift keying (BPSK), which is far from being  Gaussian. The effects of the modulation scheme cannot be  analyzed using traditional tools which are based on  spectrum of large random matrices. We follow a new approach  using tools developed for random spin systems in  statistical mechanics. We prove a tight upper bound on the  capacity of the system when the user input is BPSK. We also  show that the capacity depends only on the power of the  spreading sequences and is independent of their exact  distribution.},
  doi         = {10.5075/epfl-thesis-4461},
  file        = {:pdf/Korada2009 - Polar Codes for Channel and Source Coding.pdf:PDF},
  institution = {ISC},
  pages       = {181},
  publisher   = {EPFL},
  url         = {http://infoscience.epfl.ch/record/138655},
}

@PhdThesis{Leonardon2018c,
  author = {Mathieu L\'eonardon},
  title  = {D\'ecodage de Codes Polaires sur des Architectures Programmables},
  school = {Universit\'e de Bordeaux},
  year   = {2018},
  file   = {:pdf/Leonardon2018c - Decodage De Codes Polaires Sur Des Architectures Programmables.pdf:PDF},
  groups = {Polar Codes, Software Decoders, Hardware Decoders},
}

@Article{Leonardon2019,
  author   = {M. L\'eonardon and A. Cassagne and C. Leroux and C. J\'ego and L-P. Hamelin and Y. Savaria},
  title    = {Fast and Flexible Software Polar List Decoders},
  journal  = {Springer Journal of Signal Processing Systems (JSPS)},
  year     = {2019},
  volume   = {91},
  pages    = {937--952},
  month    = jan,
  issn     = {1939-8115},
  abstract = {Flexibility is one mandatory aspect of channel coding in modern wireless communication systems. Among other things, the channel decoder has to support several code lengths and code rates. This need for flexibility applies to polar codes that are considered for control channels in the future 5G standard. This paper presents a new generic and flexible implementation of a software Successive Cancellation List (SCL) decoder. A large set of parameters can be fine-tuned dynamically without re-compiling the software source code: the code length, the code rate, the frozen bits set, the puncturing patterns, the cyclic redundancy check, the list size, the type of decoding algorithm, the tree-pruning strategy and the data quantization. This generic and flexible SCL decoder enables to explore tradeoffs between throughput, latency and decoding performance. Several optimizations are proposed to achieve a competitive decoding speed despite the constraints induced by the genericity and the flexibility. The resulting polar list decoder is about 4 times faster than a generic software decoder and only 2 times slower than a non-flexible unrolled decoder. Thanks to the flexibility of the decoder, the fully adaptive SCL algorithm can be easily implemented and achieves higher throughput than any other similar decoder in the literature (up to 425 Mb/s on a single processor core for N = 2048 and K = 1723 at 4.5 dB).},
  day      = {18},
  doi      = {10.1007/s11265-018-1430-3},
  file     = {:pdf/Leonardon2019 - Fast and Flexible Software Polar List Decoders.pdf:PDF},
  groups   = {Polar Codes, Software Decoders, HoF Polar - SCL, AFF3CT},
  keywords = {Polar codes, Adaptive successive cancellation list decoder, Software implementation, 5G standard, Generic decoder, Flexible decoder, Cassagne},
}

@InProceedings{Cavatassi2019b,
  author    = {A. Cavatassi and T. Tonnellier and W. J. Gross},
  title     = {Asymmetric Construction of Low-Latency and Length-Flexible Polar Codes},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2019},
  pages     = {1--6},
  publisher = {IEEE},
  abstract  = {Polar  codes  are  a  class  of  capacity-achieving  errorcorrecting  codes  that  have  been  selected  for  use  in  enhancedmobile  broadband  in  the  3GPP  5thgeneration  (5G)  wirelessstandard. Most polar code research examines the original Arıkanpolar coding scheme, which is limited in block length to powersof  two.  This  constraint  presents  a  considerable  obstacle  sincepractical  applications  call  for  all  code  lengths  to  be  readilyavailable. Puncturing and shortening techniques allow for flexiblepolar   codes,   while   multi-kernel   polar   codes   produce   nativecode   lengths   that   are   powers   of   two   and/or   three.   In   thiswork,  we  propose  a  new  low  complexity  coding  scheme  calledasymmetric  polar  coding  that  allows  for  any  arbitrary  blocklength.  We  present  details  on  the  generator  matrix,  frozen  setdesign,  and  decoding  schedule. Our  scheme  offers  flexible  polarcode  lengths  with  decoding  complexity  lower  than  equivalentstate-of-the-art  length-compatible  approaches  under  successivecancellation decoding. Further, asymmetric decoding complexityis  directly  dependent  on  the  codeword  length  rather  than  thenearest  valid  polar  code  length.  We  compare  our  scheme  withother length matching techniques, and simulations are presented.Results show  that asymmetric  polar  codes present  similar errorcorrection performance to the competing schemes, while dividingthe  number  of  SC  decoding  operations  by  up  to  a  factor  of  2using  the  same  codeword  length.},
  comment   = {- construire des codes polaires de tailles variables à partir de codes de base 2 (Arikan)
- possibilité d'appliquer le même élagage que pour les codes Arikan classique (à confirmer pour SPC et Rep)
- complexité et performances de décodage équivalents au MK
- un SCL-CRC est utilisé pour les courbes de FER
- je ne comprends pas très bien la construction... mais je pense que c'est facilement reproductible par "géométrie"
},
  doi       = {10.1109/ICC.2019.8761129},
  file      = {:pdf/Cavatassi2019b - Asymmetric Construction of Low-Latency and Length-Flexible Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {multi-kernel, multi kernel, polar},
}

@InProceedings{Cavatassi2019a,
  author    = {A. Cavatassi and T. Tonnellier and W. J. Gross},
  title     = {Fast Decoding of Multi-Kernel Polar Codes},
  booktitle = {Wireless Communications and Networking Conference (WCNC)},
  year      = {2019},
  publisher = {IEEE},
  abstract  = {Polar  codes  are  a  class  of  linear  error  correctioncodes   which   provably   attain   channel   capacity   with   infinitecodeword  lengths.  Finite  length  polar  codes  have  been  adoptedinto  the  5th  Generation  3GPP  standard  for  New  Radio,  thoughtheir  native  length  is  limited  to  powers  of  2.  Utilizing  multiplepolarizing matrices increases the length flexibility of polar codesat the expense of a more complicated decoding process. Successivecancellation  (SC)  is  the  standard  polar  decoder  and  has  timecomplexityO(NlogN)due  to  its  sequential  nature.  However,some  patterns  in  the  frozen  set  mirror  simple  linear  codes  withlow latency decoders, which allows for a significant reduction inSC latency by pruning the decoding schedule. Such fast decodingtechniques have only previously been used for traditional Arıkanpolar codes, causing multi-kernel polar codes to be an impracticallength-compatibility  technique  with  no  fast  decoders  available.We propose fast simplified successive cancellation decoding nodepatterns,  which  are  compatible  with  polar  codes  constructedwith  both  the  Arıkan  and  ternary  kernels,  and  generalizationtechniques.  We  outline  efficient  implementations,  made  possibleby  imposing  constraints  on  ternary  node  parameters.  We  showthat  fast  decoding  of  multi-kernel  polar  codes  has  at  least  72\% reduced  latency  compared  with  an  SC  decoder  in  all  casesconsidered  where  codeword  lengths  are  (96,  432,  768,  2304).},
  comment   = {- se focalisent sur du multi kernel avec du Arikan (2x2) et du 3x3 donné par Gabry et Bioglio (Huawei)
- propose des simplifications dans l'arbre de décodage : 'R0', 'R1', 'SPC' et 'Rep'
- 'R0', 'R1' et 'SPC', pareil que sur du Arikan classique
- 'Rep': change un peu},
  doi       = {10.1109/WCNC.2019.8885698},
  file      = {:pdf/Cavatassi2019a - Fast Decoding of Multi-Kernel Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  keywords  = {multi-kernel, multi kernel, polar},
}

@PhdThesis{Berhault2015a,
  author   = {G. Berhault},
  title    = {Exploration Architecturale pour le Décodage de Codes Polaires},
  school   = {Universit\'e de Bordeaux},
  year     = {2015},
  abstract = {Les applications dans le domaine des communications numériques deviennent de plusen plus complexes et diversifiées. En témoigne la nécessité de corriger les erreursdes messages transmis. Pour répondre à cette problématique, des codes correcteursd’erreurs sont utilisés. En particulier, les Codes Polaires qui font l’objet de cette thèse. Ils ont étédécouverts récemment (2008) par Arıkan. Ils sont considérés comme une découverte importantedans le domaine des codes correcteurs d’erreurs. Leur aspect pratique va de paire avec la capacitéà proposer une implémentation matérielle de décodeur.Le sujet de cette thèse porte sur l’exploration architecturale de décodeurs de Codes Polairesimplémentant des algorithmes de décodage particuliers. Ainsi, le sujet gravite autour de deuxalgorithmes de décodage : un premier algorithme de décodage à décisions dures et un autrealgorithme de décodage à décisions souples.Le premier algorithme de décodage, à décisions dures, traité dans cette thèse repose sur l’algo-rithme par annulation successive (SC) comme proposé originellement. L’analyse des implémen-tations de décodeurs montre que l’unité de calcul des sommes partielles est complexe. De plus,la quantité mémoire ressort de cette analyse comme étant un point limitant de l’implémentationde décodeurs de taille importante. Les recherches menées afin de palier ces problèmes montrentqu’une architecture de mise à jour des sommes partielles à base de registres à décalages permet deréduire la complexité de cette unité. Nous avons également proposé une nouvelle méthodologiepermettant de revoir la conception d’une architecture de décodeur déjà existante de manièrerelativement simple afin de réduire le besoin en mémoire. Des synthèses en technologie ASIC etsur cibles FPGA ont été effectués pour caractériser ces contributions.Le second algorithme de décodage, à décisions souples, traité dans ce mémoire, est l’algorithmeSCAN. L’étude de l’état de l’art montre que le seul autre algorithme à décisions souples implé-menté est l’algorithme BP. Cependant, il nécessite une cinquantaine d’itérations pour obtenir desperformances de décodages au niveau de l’algorithme SC. De plus, son besoin mémoire le rendnon implémentable pour des tailles de codes élevées. L’intérêt de l’algorithme SCAN réside dansses performances qui sont meilleures que celles de l’algorithme BP avec seulement 2 itérations.De plus, sa plus faible empreinte mémoire le rend plus pratique et permet l’implémentation dedécodeurs plus grands. Nous proposons dans cette thèse une première implémentation de cetalgorithme sur cibles FPGA. Des synthèses sur cibles FPGA ont été effectuées pour pouvoircomparer le décodeur SCAN avec les décodeurs BP de l’état de l’art.Les contributions proposées dans cette thèse ont permis d’apporter une réduction de la complexitématérielle du calcul des sommes partielles ainsi que du besoin général du décodeur en élémentsde mémorisation. Le décodeur SCAN peut être utilisé dans la chaîne de communication avecd’autres blocs nécessitant des entrées souples. Cela permet alors d’ouvrir le champ d’applicationsdes Codes Polaires à ces blocs.},
  file     = {:pdf/Berhault2015a - Exploration Architecturale pour le Decodage de Codes Polaires.pdf:PDF},
  groups   = {Polar Codes, Hardware Decoders},
  keywords = {Codes Polaires, SC, SCAN, Implémentation FPGA, Implémentation ASIC, Architecture matérielle, Télécommunication, Code correcteur d’erreurs, Traitement du signal.},
}

@InProceedings{Bioglio2018b,
  author    = {V. Bioglio and I. Land and F. Gabry and J. Belfiore},
  title     = {Flexible Design of Multi-Kernel Polar Codes by Reliability and Distance Properties},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2018},
  publisher = {IEEE},
  pages     = {1--5},
  month     = dec,
  abstract  = {This paper proposes a new polar code construction for multi-kernel polar codes by design of the frozen set. The new design allows to trade reliabilities of the input bits against distance properties of the code, and thus to trim the polar code to successive-cancellation list decoding. Though the benefits are pronounced for multi-kernel polar codes, the principle is also applicable to Arikan polar codes or other polar codes. We motivate the new design principle, provide a low-complexity design algorithm, and demonstrate the performance advantages by numerical examples.},
  doi       = {10.1109/ISTC.2018.8625319},
  file      = {:pdf/Bioglio2018b - Flexible Design of Multi-Kernel Polar Codes by Reliability and Distance Properties.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2165-4719},
  keywords  = {computational complexity;decoding;polar codes;telecommunication network reliability;low-complexity design algorithm;successive-cancellation list decoding;reliability properties;input bits;Arikan polar codes;polar code construction;distance properties;multikernel polar codes;Kernel;Maximum likelihood decoding;Reliability engineering;Indexes;Complexity theory;multi kernel; multi-kernel},
}

@InProceedings{Cheng2018,
  author    = {L. Cheng and L. Zhang and Q. Sun},
  title     = {Exponents of Hybrid Multi-Kernel Polar Codes},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2018},
  publisher = {IEEE},
  pages     = {1--5},
  month     = dec,
  abstract  = {Multi-kernel polar codes are constructed from the Kronecker product. For ordinary multi-kernels, constituent polarizing matrices of the same stage are identical. However, in a stage, applying different constituent matrices based on channel property can further improve performance. This family of polar codes is defined as hybrid multi-kernel polar codes. Exponents of hybrid multi-kernels are analyzed. The matrix of a hybrid multi-kernel is generated by a Kronecker-like product operation, and so are partial distances. All exponents of hybrid multi-kernels take values between the maximum and the minimum exponents among all constituent matrices. Exponents are related to the error probability of the original channel and exhibit a trend of convergence as block-length increases. The convergence rate is also related to the original error probability.},
  doi       = {10.1109/ISTC.2018.8625322},
  file      = {:pdf/Cheng2018 - Exponents of Hybrid Multi-Kernel Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2165-4719},
  keywords  = {error statistics;polar codes;hybrid multikernel polar codes;ordinary multikernels;constituent polarizing matrices;original channel error probability;Turbo codes;Information processing;Hybrid power systems;Kernel;Error analysis;Error probability;Convergence;exponent;polar codes;hybrid multi-kernel;partial distances;multi kernel; multi-kernel},
}

@InProceedings{Tonnellier2019a,
  author    = {T. {Tonnellier} and A. {Cavatassi} and W. J. {Gross}},
  booktitle = {Conference on Information Sciences and Systems (CISS)},
  publisher = {IEEE},
  title     = {Length-Compatible Polar Codes: A Survey : (Invited Paper)},
  year      = {2019},
  file      = {:pdf/Tonnellier2019a - Length-Compatible Polar Codes\: A Survey.pdf:PDF},
  volume    = {},
  number    = {},
  pages     = {1--6},
  abstract  = {Polar codes natively lack the flexibility that is desired for practical application. Namely, Arikan's polar code definition can only achieve code lengths that are powers of two. Rate-matching techniques, known as puncturing and shortening, have been applied to polar codes to grant a flexible block length. By considering polarizing kernels of alternate dimensions, Multi-kernel polar codes improve natural block length flexibility. With the recent advent of the 3GPP 5th generation New Radio specification, there now exists an industry standard for length-flexible polar codes. This paper outlines various state-of-the-art flexible polar coding schemes, such as puncturing, shortening, and multi-kernel construction, and evaluates their efficacy with respect to the newly designed 3GPP standard. Simulations and an in-depth analysis are presented.},
  keywords  = {polar codes;multi kernel;multi-kernel;3G mobile communication;5G mobile communication;polar codes;natural block length flexibility;length-flexible polar codes;multikernel construction;length-compatible polar codes;Arikan's polar code definition;code lengths;rate-matching techniques;Multikernel polar codes;polar coding schemes;block length;3GPP;5th generation new radio specification},
  doi       = {10.1109/CISS.2019.8692789},
  ISSN      = {},
  month     = {March},
  groups    = {Polar Codes},
}

@InProceedings{Bioglio2018a,
  author    = {V. Bioglio and C. Condo and I. Land},
  title     = {Memory Management in Successive-Cancellation based Decoders for Multi-Kernel Polar Codes},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  publisher = {IEEE},
  year      = {2018},
  pages     = {472--476},
  month     = oct,
  abstract  = {Multi-kernel polar codes have recently been proposed to construct polar codes of lengths different from powers of two. Decoder implementations for multi-kernel polar codes need to account for this feature, that becomes critical in memory management. We propose an efficient, generalized memory management framework for implementation of successive-cancellation decoding of multi-kernel polar codes. It can be used on many types of hardware architectures and different flavors of SC decoding algorithms. We illustrate the proposed solution for small kernel sizes, and give complexity estimates for various kernel combinations and code lengths.},
  doi       = {10.1109/ACSSC.2018.8645480},
  file      = {:pdf/Bioglio2018a - Memory Management in Successive-Cancellation based Decoders for Multi-Kernel Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2576-2303},
  keywords  = {Decoding;Kernel;Memory management;Hardware;Indexes;Micromechanical devices;Polar Codes;Successive Cancellation Decoding;Decoder Architectures;multi kernel;multi-kernel},
}

@InProceedings{Bioglio2018c,
  author    = {V. Bioglio and I. Land},
  title     = {On the Marginalization of Polarizing Kernels},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  publisher = {IEEE},
  year      = {2018},
  pages     = {1--5},
  month     = dec,
  abstract  = {In this paper, we analyze the decoding complexity of polarizing kernels of size larger than p = 2, proposing a general procedure which can be used to marginalize kernels of any size. We use this method to evaluate the decoding equations for optimal binary kernels up to size p = 8. Finally, we experimentally show that, even if T_2 and T_8 share the same polarization exponent, their performance may vary under list decoding, making large kernels of interest for long polar codes.},
  doi       = {10.1109/ISTC.2018.8625378},
  file      = {:pdf/Bioglio2018c - On the Marginalization of Polarizing Kernels.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {2165-4719},
  keywords  = {decoding, optimisation, polar codes, multi kernel, multi-kernel},
}

@Article{Ardakani2019,
  author   = {Maryam Haghighi Ardakani and Muhammad Hanif and Masoud Ardakani and Chintha Tellambura},
  title    = {Fast Successive-Cancellation Based Decoders of Polar Codes},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {2019},
  pages    = {1},
  abstract = {The successive-cancellation list (SCL) and successive-cancellation flip (SCF) decoding can be used to improve the performance of polar codes, especially for short to moderate length codes. However, their serial decoding nature results in significant decoding latencies. Implementing some operations in parallel can reduce their decoding latency. This work presents fast implementations of the SCL and SCF decoders. In particular, we propose fast parallel list decoders for five newly-identified types of nodes in the decoding tree of a polar code, which significantly improve the decoding latency. We also present novel fast SCF decoders that decode some special nodes in the decoding tree of a polar code without serially computing bit log-likelihood ratios. Using our proposed fast parallel SCF decoders, we observed an improvement up to 81\% with respect to the original SCF decoder. This significant reduction in the decoding latency is observed without sacrificing the bit-error-rate performance of the code.},
  doi      = {10.1109/tcomm.2019.2906232},
  file     = {:pdf/Ardakani2019 - Fast Successive-Cancellation Based Decoders of Polar Codes.pdf:PDF},
}

@Article{Cassagne2019a,
  author   = {A. Cassagne and O. Hartmann and M. L\'eonardon and K. He and C. Leroux and R. Tajan and O. Aumage and D. Barthou and T. Tonnellier and V. Pignoly and B. {Le Gal} and C. J\'ego},
  title    = {{AFF3CT}: A Fast Forward Error Correction Toolbox!},
  journal  = {Elsevier SoftwareX},
  year     = {2019},
  volume   = {10},
  pages    = {100345},
  month    = oct,
  issn     = {2352-7110},
  abstract = {AFF3CT is an open source toolbox dedicated to Forward Error Correction (FEC or channel coding). It supports a broad range of codes: from widespread turbo codes and Low-Density Parity-Check (LDPC) codes to more recent polar codes. The toolbox is written in C++ and can be used either as a simulator to quickly evaluate algorithms characteristics, or as a library in Software Defined Radio (SDR) systems or for other specific needs. Most of the decoding algorithm implementations aim at low latency and high throughput, targeting multiple Gb/s on modern CPUs. This is crucial in both simulation and SDR use cases: Monte Carlo simulations require high performance implementation as they commonly target the estimation of approximately 10^{12} bits. On the other hand, the implementations in real systems have to be very efficient to be competitive against dedicated hardware ones. Finally, AFF3CT emphasizes the reproducibility of state-of-the-art results by providing public references and open, modular source code.},
  doi      = {10.1016/j.softx.2019.100345},
  file     = {:pdf/Cassagne2019a - AFF3CT\: A Fast Forward Error Correction Toolbox.pdf:PDF;:pdf/Cassagne2019a - AFF3CT\: A Fast Forward Error Correction Toolbox [poster].pdf:PDF},
  groups   = {C/C++ Librairies},
  keywords = {Communication chain, Channel coding, Monte Carlo simulation, Forward error correction library, Digital modulation, Reproducible science, Multi-node, Multi-thread, Vectorization, Cassagne},
}

@Article{LeGal2019a,
  author   = {B. {Le Gal} and C. J\'ego},
  title    = {Low-latency and High-throughput Software Turbo Decoders on Multi-core Architectures},
  journal  = {Springer Annals of Telecommunications},
  year     = {2019},
  volume   = {75},
  pages    = {27--42},
  month    = aug,
  issn     = {1958-9395},
  abstract = {In the last few years, with the advent of a software-defined radio (SDR), the processor cores were stated to be an efficient solution to execute the physical layer components. Indeed, multi-core architectures provide both high-processing performance and flexibility, such that they are used in current base station systems instead of dedicated FPGA or ASIC devices. Currently, an extension of the SDR concept is running. Indeed, cloud platforms become attractive for the virtualization of radio access network functions. Actually, they improve the efficiency of the computational resource usage, and thus the global power efficiency. However, the implementation of a physical layer on a Cloud-RAN platform as discussed by Wubben and Paul (2016); Checko et al. (JAMA 17(1):405–426, 2015); Inc (2015); and Wubben et al. (JAMA 31(6):35–44, 2014) or FlexRAN platform as discussed by Wilson (2018); Foukas et al. (2017); Corp. (2017); Foukas et al. (2016) is a challenging task according to the drastic latency and throughput constraints as discussed by Yu et al. (2017) and Parvez (2018). Processing latencies from 10 μ s up to hundred of μ s are required for future digital communication systems. In this context, most of works about software implementations of ECC applications is based on massive frame parallelism to reach high throughput. Nonetheless, they produce unacceptable decoding latencies. In this paper, a new turbo decoder parallelization approach is proposed for x86 multi-core processors. It provides both: high-throughput and low-latency performances. In comparison with all CPU- and GPU-related works, the following results are observed: shorter processing latency, higher throughput, and lower energy consumption. Regarding to the best state-of-the-art x86 software implementations, 1.5 × to 2 × throughput improvements are reached, whereas a latency reduction of 50 × and an energy reduction of 2 × are observed.},
  day      = {03},
  doi      = {10.1007/s12243-019-00727-5},
  file     = {:pdf/LeGal2019a - Low-latency and High-throughput Software Turbo Decoders on Multi-core Architectures.pdf:PDF},
  groups   = {Software Decoders, Turbo Codes, HoF Turbo - MAP},
}

@InProceedings{Ebada2019,
  author    = {M. {Ebada} and S. {Cammerer} and A. {Elkelesh} and S. {ten Brink}},
  booktitle = {Allerton Conference on Communication, Control, and Computing},
  publisher = {IEEE},
  title     = {Deep Learning-Based Polar Code Design},
  year      = {2019},
  month     = {Sep.},
  pages     = {177--183},
  comment   = {Résultats pas fous, permet de converger un peu plus vite},
  doi       = {10.1109/ALLERTON.2019.8919804},
  file      = {:pdf/Ebada2019 - Deep Learning Based Polar Code Design.pdf:PDF},
  groups    = {Artificial Intelligence, Polar Codes},
  issn      = {null},
  keywords  = {AWGN channels;block codes;channel coding;decoding;error correction codes;error statistics;gradient methods;learning (artificial intelligence);linear codes;matrix algebra;neural nets;Rayleigh channels;telecommunication computing;deep learning-based polar code construction algorithm;learning iteration;soft-to-binary conversions;code rate;learning process;soft-valued vector;binary vector;Iterative decoding;Training;Complexity theory;Maximum likelihood decoding;Artificial neural networks;Machine learning;Polar Code Construction;Frozen Bit Vector;Machine Learning;Deep Learning;Binarizer},
}

@Article{Le2020,
  author   = {V. H. S. {Le} and C. A. {Nour} and E. {Boutillon} and C. {Douillard}},
  journal  = {IEEE Transactions on Communications (TCOM)},
  title    = {Revisiting the max-log-{MAP} Algorithm with {SOVA} Update Rules: New Simplifications for High-radix {SISO} Decoders},
  year     = {2020},
  issn     = {1558-0857},
  pages    = {1--1},
  doi      = {10.1109/TCOMM.2020.2966723},
  file     = {:pdf/Le2020 - Revisiting the Max-Log-Map algorithm with SOVA.pdf:PDF},
  groups   = {Turbo Codes},
  keywords = {convolutional codes;soft-input soft-ouput decoding;soft-ouput Viterbi algorithm;high-radix decoding;turbo codes;high throughput},
}

@InProceedings{Jiang2019,
  author    = {Y. {Jiang} and H. {Kim} and H. {Asnani} and S. {Kannan} and S. {Oh} and P. {Viswanath}},
  booktitle = {IEEE International Conference on Communications (ICC)},
  publisher = {IEEE},
  title     = {LEARN Codes: Inventing Low-Latency Codes via Recurrent Neural Networks},
  year      = {2019},
  month     = {May},
  pages     = {1--7},
  doi       = {10.1109/ICC.2019.8761286},
  file      = {:pdf/Jiang2019 - LEARN Codes\: Inventing Low-Latency Codes via Recurrent Neural Networks.pdf:PDF},
  groups    = {Artificial Intelligence},
  issn      = {1550-3607},
  keywords  = {5G mobile communication;AWGN channels;block codes;channel coding;convolutional codes;recurrent neural nets;telecommunication computing;LEARN codes;recurrent Neural networks;Additive White Gaussian Noise;nonAWGN channels;end-to-end learned neural code;canonical convolutional code;channel codes;deep learning;neural block code;5G standards;low-latency efficient adaptive robust neural codes;Decoding;Recurrent neural networks;Training;Convolutional codes;Deep learning;Robustness;Block codes},
}

@Article{Elkelesh2019a,
  author   = {A. {Elkelesh} and M. {Ebada} and S. {Cammerer} and S. t. {Brink}},
  journal  = {IEEE Transactions on Communications (TCOM)},
  title    = {Decoder-Tailored Polar Code Design Using the Genetic Algorithm},
  year     = {2019},
  issn     = {1558-0857},
  month    = {July},
  number   = {7},
  pages    = {4521--4534},
  volume   = {67},
  comment  = {Github : https://github.com/AhmedElkelesh/Genetic-Algorithm-based-Polar-Code-Construction
Question posée sur les issues, résultats étonnants Figure6},
  doi      = {10.1109/TCOMM.2019.2908870},
  file     = {:pdf/Elkelesh2019a - Decoder-Tailored Polar Code DesignUsing the Genetic Algorithm.pdf:PDF},
  groups   = {Artificial Intelligence, Polar Codes},
  keywords = {decoding;genetic algorithms;interference suppression;polar codes;successive cancellation decoding;genetic algorithm;information set;evolutionary transformations;individual error-rate performance;decoding behavior;defined channel;polar codes;CRC-aid;plain successive cancellation list decoding;CRC-aided SCL decoding;AWGN channel;Rayleigh channel;belief propagation-tailored construction;SCL error-rate performance;performance gains;decoding complexity;SCL list size;decoder-tailored polar code design;frozen bit positions;arbitrary channels;Iterative decoding;Maximum likelihood decoding;Complexity theory;Optimization;Polar codes;channel polarization;polar code construction;Reed–Muller codes;genetic algorithm;evolutionary algorithms;artificial intelligence},
}

@InProceedings{Parks1995,
  author    = {T. M. {Parks} and J. L. {Pino} and E. A. {Lee}},
  title     = {A Comparison of Synchronous and Cycle-Static Dataflow},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {1995},
  volume    = {1},
  pages     = {204--210},
  month     = oct,
  publisher = {IEEE},
  abstract  = {We compare synchronous dataflow (SDF) and cyclo-static dataflow (CSDF), which are each special cases of a model of computation we call dataflow process networks. In SDF actors have static firing rules: they consume and produce a fixed number of data tokens in each firing. This model is well suited to multirate signal processing applications and lends itself to efficient static scheduling, avoiding the run-time scheduling overhead incurred by general implementations of process networks. In CSDF which is a generalization of SDF actors have cyclically changing firing rules. In some situations, the added generality of CSDF can unnecessarily complicate the scheduling. We show how higher-order functions can be used to transform a CSDF graph into a SDF graph, simplifying the scheduling problem. In other situations, CSDF has a genuine advantage over SDF: simpler precedence constraints. We show how this makes it possible to eliminate unnecessary computations and expose additional parallelism. We use digital sample rate conversion as an example to illustrate these advantages of CSDF.},
  doi       = {10.1109/ACSSC.1995.540541},
  file      = {:pdf/Parks1995 - A Comparison of Synchronous and Cyclestatic Dataflow.pdf:PDF},
  groups    = {Dataflow, Models},
  issn      = {1058-6393},
  keywords  = {data flow computing, scheduling, data flow graphs, signal sampling, cycle-static dataflow, synchronous dataflow, computation model, dataflow process networks, static firing rules, data tokens, multirate signal processing applications, static scheduling, higher-order functions, CSDF graph, SDF graph, scheduling problem, precedence constraints, parallelism, digital sample rate conversion, Processor scheduling, Streaming media, Computer networks, Computational modeling, Communication channels, Signal processing, Concurrent computing, Parallel processing, Video signal processing, Fires},
}

@InCollection{DeOliveiraCastro2017,
  author    = {P. {De Oliveira Castro} and S. Louise and D. Barthou},
  title     = {{DSL} Stream Programming on Multicore Architectures},
  booktitle = {Programming Multi-core and Many-core Computing Systems},
  publisher = {John Wiley and Sons},
  year      = {2017},
  chapter   = {7},
  isbn      = {9781119332015},
  doi       = {10.1002/9781119332015.ch7},
  file      = {:pdf/DeOliveiraCastro2017 - DSL Stream Programming on Multicore Architectures.pdf:PDF},
  groups    = {Dataflow, Languages},
}

@InProceedings{Thies2010,
  author    = {W. {Thies} and S. {Amarasinghe}},
  title     = {An Empirical Characterization of Stream Programs and its Implications for Language and Compiler Design},
  booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  year      = {2010},
  pages     = {365--376},
  month     = sep,
  publisher = {ACM/IEEE},
  abstract  = {Stream programs represent an important class of high-performance computations. Defined by their regular processing of sequences of data, stream programs appear most commonly in the context of audio, video, and digital signal processing, though also in networking, encryption, and other areas. In order to develop effective compilation techniques for the streaming domain, it is important to understand the common characteristics of these programs. Prior characterizations of stream programs have examined legacy implementations in C, C++, or FORTRAN, making it difficult to extract the high-level properties of the algorithms. In this work, we characterize a large set of stream programs that was implemented directly in a stream programming language, allowing new insights into the high-level structure and behavior of the applications. We utilize the StreamIt benchmark suite, consisting of 65 programs and 33,600 lines of code. We characterize the bottlenecks to parallelism, the data reference patterns, the input/output rates, and other properties. The lessons learned have implications for the design of future architectures, languages and compilers for the streaming domain.},
  file      = {:pdf/Thies2010 - An Empirical Characterization of Stream Programsand its Implications for Language and Compiler Design [slides].pdf:PDF;:pdf/Thies2010 - An Empirical Characterization of Stream Programsand its Implications for Language and Compiler Design.pdf:PDF},
  groups    = {Dataflow, Languages},
  keywords  = {C++ language, program compilers, software performance evaluation, stream programs, compiler design, data sequence, digital signal processing, video signal processing, audio signal processing, networking, encryption, compilation techniques, C++, C, FORTRAN, stream programming language, data reference patterns, Benchmark testing, Finite impulse response filters, Streaming media, Computer languages, Computer architecture, Pipelines, Transform coding, Languages, Design, Experimentation, Performance},
  doi       = {10.1145/1854273.1854319}
}

@Article{Palkovic2012,
  author   = {M. Palkovic and J. Declerck and P. Avasare and M. Glassee and A. Dewilde and P. Raghavan and A. Dejonghe and L. {Van der Perre}},
  title    = {{DART} - a High Level Software-Defined Radio Platform Model for Developing the Run-Time Controller},
  journal  = {Springer Journal of Signal Processing Systems (JSPS)},
  year     = {2012},
  volume   = {69},
  pages    = {317--327},
  month    = mar,
  abstract = {Novel cognitive radio platforms, such as IMEC’s COgnitive Baseband RAdio (COBRA), should ensure the feasibility of multiple streams and their reconfigurability and scalability during run-time. The control over these tasks should be dedicated to a run-time controller that (re)allocates the resources on the platform. E.g., when the channel conditions change requiring a switching to different modulation and coding scheme or a user starts a new stream. Current transaction level models are too detailed for rapid exploration of all run-time options and the high-level data-flow frameworks (such as Kahn process networks) lack the dynamism and reconfigurability that is essential for the exploration. In this paper we propose the DAtaflow for Run-Time (DART), the high-level dynamic data-flow platform model framework, suited for rapid run-time control development. We sketch how to use this framework to develop such a controller in the reactive and more challenging, proactive way. We derive the component timing based on Instruction Set Simulator (ISS) simulation and the reconfiguration timing based on Transaction Level Modeling (TLM) simulation. Finally, we verify results of our DART approach with full TLM simulation of our platform.},
  doi      = {10.1007/s11265-012-0669-3},
  file     = {:pdf/Palkovic2012 - DART - a High Level Software-Defined Radio PlatformModel for Developing the Run-Time Controller.pdf:PDF},
  groups   = {Modeling, Software-Defined Radio (SDR)},
}

@InProceedings{Grayver2020,
  author        = {E. Grayver and A. Utter},
  title         = {Extreme Software Defined Radio -- {GH}z in Real Time},
  booktitle     = {Aerospace Conference (AeroConf)},
  year          = {2020},
  month         = mar,
  publisher     = {IEEE},
  abstract      = {Software defined radio is a widely accepted paradigm for design of reconfigurable modems. The continuing march of Moore’s law makes real-time signal processing on general purpose processors feasible for a large set of waveforms. Data rates in the low Mbps can be processed on low-power ARM processors, and much higher data rates can be supported on large x86 processors. The advantages of all-software development (vs. FPGA/DSP/GPU) are compelling – much wider pool of talent, lower development time and cost, and easier maintenance and porting. However, very high-rate systems (above 100 Mbps) are still firmly in the domain of custom and semi-custom hardware (mostly FPGAs). In this paper we describe an architecture and testbed for an SDR that can be easily scaled to support over 3 GHz of bandwidth and data rate up to 10 Gbps. The paper covers a novel technique to parallelize typically serial algorithms for phase and symbol tracking, followed by a discussion of data distribution for a massively parallel architecture. We provide a brief description of a mixed-signal front end and conclude with measurement results. To the best of the author’s knowledge, the system described in this paper is an order of magnitude faster than any prior published result.},
  archiveprefix = {arXiv},
  eprint        = {2001.03645},
  file          = {:pdf/Grayver2020 - Extreme Software Defined Radio - GHz in Real Time.pdf:PDF},
  groups        = {GPP, CPU, Software-Defined Radio (SDR)},
}

@Article{Akeela2018,
  author   = {R. Akeela and B. Dezfouli},
  title    = {Software-Defined Radios: Architecture, State-of-the-Art, and Challenges},
  journal  = {ACM Computer Communications},
  year     = {2018},
  volume   = {128},
  pages    = {106--125},
  issn     = {0140-3664},
  abstract = {Software-defined Radio (SDR) is a programmable transceiver with the capability of operating various wireless communication protocols without the need to change or update the hardware. Progress in the SDR field has led to the escalation of protocol development and a wide spectrum of applications, with a greater emphasis on programmability, flexibility, portability, and energy efficiency in cellular, WiFi, and M2M communication. Consequently, SDR has earned a lot of attention and is of great significance to both academia and industry. SDR designers intend to simplify the realization of communication protocols while enabling researchers to experiment with prototypes on deployed networks. This paper is a survey of the state-of-the-art SDR platforms in the context of wireless communication protocols. We offer an overview of SDR architecture and its basic components, and then discuss the significant design trends and development tools. In addition, we highlight key contrasts between SDR architectures with regards to energy, computing power, and area, based on a set of metrics. We also review existing SDR platforms and present an analytical comparison as a guide to developers. Finally, we recognize a few of the related research topics and summarize potential solutions.},
  doi      = {10.1016/j.comcom.2018.07.012},
  file     = {:pdf/Akeela2018 - Software-Defined Radios\: Architecture, State-of-the-Art, and Challenges.pdf:PDF},
  groups   = {Concepts, Software-Defined Radio (SDR)},
  keywords = {SDR, Wireless communication, Programmability, Co-design, LTE, WiFi, IoT},
}

@Article{Palkovic2010,
  author   = {M. {Palkovic} and P. {Raghavan} and M. {Li} and A. {Dejonghe} and L. {Van der Perre} and F. {Catthoor}},
  title    = {Future Software-Defined Radio Platforms and Mapping Flows},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2010},
  volume   = {27},
  number   = {2},
  pages    = {22--33},
  month    = mar,
  issn     = {1558-0792},
  abstract = {A software-defined radio (SDR) system is a radio communication system in which physical layer components are implemented on a programmable or reconfigurable platform. The modulation and demodulation is performed in software and thus the radio is able to support a broad ran: of frequencies and functions concurrently. In the ideal SDR transceiver scheme, an analog-to-digital converter (ADC) and a digital-to-analog converter (DAC) are attached to the antenna. This would imply that a digital signal processor (DSP) is connected to the ADC and the DAC, directly performing signal processing for the streams of data from/to antenna. Today, the ideal SDR transceiver scheme is still not feasible and thus some processing has to happen in the reconfigurable analog front end.},
  doi      = {10.1109/MSP.2009.935386},
  file     = {:pdf/Palkovic2010 - Future Software-Defined Radio Platforms and Mapping Flows.pdf:PDF},
  groups   = {Concepts, Software-Defined Radio (SDR)},
  keywords = {analogue-digital conversion, digital-analogue conversion, software radio, transceivers, software-defined radio, mapping flows, radio communication system, programmable platform, reconfigurable platform, transceiver scheme, analog-to-digital converter, digital-to-analog converter, digital signal processor, Transceivers, Digital signal processing, Radio communication, Physical layer, Demodulation, Software performance, Radio access networks, Frequency, Analog-digital conversion, Digital-analog conversion},
}

@Article{Amarasinghe2005,
  author   = {S. Amarasinghe and M. l. Gordon and M. Karczmarek and J. Lin and D. Maze and R. M. Rabbah and W. Thies},
  title    = {Language and Compiler Design for Streaming Applications},
  journal  = {Springer International Journal of Parallel Programming ({IJPP})},
  year     = {2005},
  volume   = {2},
  number   = {33},
  pages    = {261--278},
  month    = jun,
  abstract = {High-performance streaming applications are a new and distinct domain of programs that is increasingly important. The StreamIt language provides novel high-level representations to improve programmer productivity and program robustness within the streaming domain. At the same time, the StreamIt compiler aims to improve the performance of streaming applications via stream-specific analysis and optimizations. In this paper, we motivate, describes and justify the StreamIt language which include a structured model of streams, a messaging system for control, and a natural textual syntax.},
  doi      = {10.1007/s10766-005-3590-6},
  file     = {:pdf/Amarasinghe2005 - Language and Compiler Design for Streaming Applications.pdf:PDF},
  groups   = {Dataflow, Languages},
  keywords = {Stream computing, StreamIt, parallelizing compiler, tiled-processor architectures productivity, dataflow},
}

@Article{Buck2004,
  author     = {I. Buck and T. Foley and D. Horn and J. Sugerman and K. Fatahalian and M. Houston and P. Hanrahan},
  title      = {Brook for {GPU}s: Stream Computing on Graphics Hardware},
  journal    = {ACM Transactions on Graphics (TOG)},
  year       = {2004},
  volume     = {23},
  number     = {3},
  pages      = {777--786},
  month      = aug,
  issn       = {0730-0301},
  abstract   = {In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming co-processor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.},
  address    = {New York, NY, USA},
  doi        = {10.1145/1015706.1015800},
  file       = {:pdf/Buck2004 - Brook for GPUs\: Stream Computing on Graphics Hardware.pdf:PDF},
  groups     = {Dataflow, Languages},
  issue_date = {August 2004},
  keywords   = {Programmable Graphics Hardware, Stream Computing, Data Parallel Computing, GPU Computing, Brook},
  numpages   = {10},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{Liao2006,
  author    = {S.-W. Liao and Z. Du and G. Wu and G.-Y. Lueh},
  title     = {Data and Computation Transformations for Brook Streaming Applications on Multiprocessors},
  booktitle = {International Symposium on Code Generation and Optimization (CGO)},
  year      = {2006},
  pages     = {207--219},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Multicore processors are about to become prevalent in the PC world. Meanwhile, over 90\% of the computing cycles are estimated to be consumed by streaming media applications (Rixner et al., 1998). Although stream programming exposes parallelism naturally, we found that achieving high performance on multiprocessors is challenging. Therefore, we develop a parallel compiler for the Brook streaming language with aggressive data and computation transformations. First, we formulate fifteen Brook stream operators in terms of systems of inequalities. Our compiler optimizes the modeled operators to improve memory footprint and performance. Second, the stream computation including both kernels and operators is mapped to the affine partitioning model by modeling each kernel as an implicit loop nest over stream elements. Note that our general abstraction is not limited to Brook. Our modeling and transformations yield high performance on uniprocessors as well. The geometric mean of speedups is 4.7 on ten streaming applications on a Xeon. On multiprocessors, we show that exploiting the standard intra-kernel data parallelism is inferior to our general modeling. The former yields a speedup of 1.5 for ten applications on a 4-way Xeon, while the latter achieves a speedup of 6.4 over the same baseline. We show that our compiler effectively reduces memory footprint, exploits parallelism, and circumvents phase-ordering issues.},
  doi       = {10.1109/CGO.2006.13},
  file      = {:pdf/Liao2006 - Data and Computation Transformations for Brook Streaming Applications on Multiprocessors.pdf:PDF},
  groups    = {Dataflow, Languages},
  keywords  = {multiprocessing systems, optimising compilers, parallel processing, operating system kernels, data transformation, computation transformation, multiprocessor, stream programming, parallel compiler, Brook streaming language, Brook stream operator, optimizing compiler, uniprocessor, Xeon, intrakernel data parallelism, memory footprint, Application software, Parallel processing, Kernel, Multicore processing, Streaming media, Parallel programming, Concurrent computing, Optimizing compilers, Image coding, Graphics},
}

@Article{Glitia2010,
  author   = {C. Glitia and P. Dumont and P. Boulet},
  title    = {Array-{OL} with Delays, a Domain Specific Specification Language for Multidimensional Intensive Signal Processing},
  journal  = {Springer Multidimensional Systems and Signal Processing},
  year     = {2010},
  number   = {21},
  pages    = {105--131},
  month    = mar,
  abstract = {Intensive signal processing applications appear in many application domains such as video processing or detection systems. These applications handle multidimensional data structures (mainly arrays) to deal with the various dimensions of the data (space, time, frequency). A specification language allowing the direct manipulation of these different dimensions with a high level of abstraction is a key to handling the complexity of these applications and to benefit from their massive potential parallelism. The Array-OL specification language is designed to do just that. We introduce here an extension of Array-OL to deal with states or delays by the way of uniform inter-repetition dependences. We show that this specification language is able to express the main patterns of computation of the intensive signal processing domain.},
  doi      = {10.1007/s11045-009-0085-4},
  file     = {:pdf/Glitia2010 - Array-OL with Delays, a Domain Specific Specification Language for Multidimensional Intensive Signal Processing.pdf:PDF},
  groups   = {Dataflow, Languages},
}

@InProceedings{Black-Schaffer2010,
  author    = {D. {Black-Schaffer} and W. J. {Dally}},
  title     = {Block-Parallel Programming for Real-Time Embedded Applications},
  booktitle = {International Conference on Parallel Processing (ICPP)},
  year      = {2010},
  pages     = {297--306},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Embedded media applications have traditionally used custom ASICs to meet their real-time performance requirements. However, the combination of increasing chip design cost and availability of commodity many-core processors is making programmable devices increasingly attractive alternatives. Yet for these processors to be successful in this role, programming systems are needed that can automate the task of mapping the applications to the tens-to-hundreds of cores on current and future many-core processors, while simultaneously guaranteeing the real-time throughput constraints. This paper presents a block-parallel program description for embedded real-time media applications and automatic transformations including buffering and parallelization to ensure the program meets the throughput requirements. These transformations are enabled by starting with a high-level, yet intuitive, application description. The description builds on traditional stream programming structures by adding simple control and serialization constructs to enable a greater variety of applications. The result is an application description that provides a balance of flexibility and power to the programmer, while exposing the application structure to the compiler at a high enough level to enable useful transformations without heroic analysis.},
  doi       = {10.1109/ICPP.2010.37},
  file      = {:pdf/Black-Schaffer2010 - Block-Parallel Programming for Real-time Embedded Applications.pdf:PDF},
  groups    = {Dataflow, Languages},
  issn      = {2332-5690},
  keywords  = {application specific integrated circuits, embedded systems, integrated circuit reliability, microprocessor chips, multimedia computing, multiprocessing systems, parallel programming, block-parallel programming, real-time embedded media applications, ASIC, chip design cost, commodity many-core processor availability, programmable devices, real-time throughput constraints, stream programming structures, Kernel, Real time systems, Convolution, Histograms, Program processors, Programming, Throughput, parallel programming, synchronous data flow, image processing, parallelization, real-time constraints},
}

@Article{Lee1987,
  author   = {E. A. {Lee} and D. G. {Messerschmitt}},
  title    = {Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing},
  journal  = {IEEE Transactions on Computers (TC)},
  year     = {1987},
  volume   = {C-36},
  number   = {1},
  pages    = {24--35},
  month    = jan,
  issn     = {1557-9956},
  abstract = {Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of programmer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple processors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors.},
  doi      = {10.1109/TC.1987.5009446},
  file     = {:pdf/Lee1987 - Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing.pdf:PDF},
  groups   = {Dataflow, Models},
  keywords = {Signal processing;Topology;Schedules;Programming;Processor scheduling;Digital signal processing;Runtime;Block diagram;computation graphs;data flow digital signal processing;hard real-time systems;multiprocessing;Petri nets;static scheduling;synchronous data flow},
}

@Article{Ackerman1982,
  author  = {W.B. Ackerman},
  title   = {Data Flow Languages},
  journal = {IEEE Computer},
  year    = {1982},
  volume  = {15},
  pages   = {15--25},
  month   = feb,
  doi     = {10.1109/MC.1982.1653938},
  groups  = {Dataflow, Models},
}

@Article{Dennis1980,
  author   = {J.B. {Dennis}},
  title    = {Data Flow Supercomputers},
  journal  = {IEEE Computer},
  year     = {1980},
  volume   = {13},
  number   = {11},
  pages    = {48--56},
  month    = nov,
  issn     = {1558-0814},
  abstract = {Programmability with increased performance? New strategies to attain this goal include two approaches to data flow architecture: data flow multiprocessors and the cell block architecture.},
  doi      = {10.1109/MC.1980.1653418},
  groups   = {Dataflow, Models},
  keywords = {Supercomputers;Computer architecture;Data flow computing;Concurrent computing;Large-scale systems;Computer languages;Application software},
}

@InProceedings{Bilsen1995,
  author    = {G. {Bilsen} and M. {Engels} and R. {Lauwereins} and J. A. {Peperstraete}},
  title     = {Cyclo-Static Data Flow},
  booktitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {1995},
  volume    = {5},
  pages     = {3255--3258},
  month     = may,
  publisher = {IEEE},
  abstract  = {The high sample-rates involved in many DSP-applications, require the use of static schedulers wherever possible. The construction of static schedules however is classically limited to applications that fit in the synchronous data flow model. In this paper we present cyclo-static data flow as a model to describe applications with a cyclically changing behaviour. We give both a necessary and sufficient condition for the existence of a static schedule for a cyclo-static data flow graph and show how such a schedule can be constructed. The example of a video encoder is used to illustrate the importance of cyclo-static data flow for real-life DSP-systems.},
  doi       = {10.1109/ICASSP.1995.479579},
  file      = {:pdf/Bilsen1995 - Cyclo-Static Data Flow.pdf:PDF},
  groups    = {Dataflow, Models},
  issn      = {1520-6149},
  keywords  = {signal sampling, data flow graphs, processor scheduling, video coding, video equipment, high sample-rates, DSP-applications, static schedulers, synchronous data flow model, cyclically changing behaviour, sufficient condition, necessary condition, static schedule, cyclo-static data flow graph, video encoder, DSP systems, Processor scheduling, Flow graphs, Runtime, Sufficient conditions, Dynamic scheduling, Prototypes, Emulation, Digital signal processing, Costs, Hardware},
}

@InProceedings{Engels1994,
  author    = {M. {Engels} and G. {Bilsen} and R. {Lauwereins} and J. A. {Peperstraete}},
  title     = {Cycle-Static Dataflow: Model and Implementation},
  booktitle = {Asilomar Conference on Signals, Systems, and Computers (ACSSC)},
  year      = {1994},
  volume    = {1},
  pages     = {503--507},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Cycle-static dataflow (CSDF) is used for specifying digital signal processing algorithms with a cyclically changing, but predefined, behavior. Unlike other models for such applications, CSDF allows for static scheduling and hence a very efficient implementation. We review the CSDF paradigm and present the scheduling approach for CSDF graphs that is currently implemented in the Graphical Rapid Prototyping Environment GRAPE.},
  doi       = {10.1109/ACSSC.1994.471504},
  file      = {:pdf/Engels1994 - Cycle-Static Dataflow\: Model and Implementation.pdf:PDF},
  groups    = {Dataflow, Models},
  issn      = {1058-6393},
  keywords  = {signal processing, software prototyping, software engineering, data flow graphs, processor scheduling, parallel algorithms, computer graphics, cycle-static dataflow, digital signal processing algorithms, static scheduling, CSDF paradigm, CSDF graphs, Graphical Rapid Prototyping Environment, GRAPE, Digital signal processing, Signal processing algorithms, Prototypes, Processor scheduling, Hardware, Pipelines, Runtime, Real time systems, Signal design, Dynamic scheduling},
}

@InProceedings{Maheshwarappa2015,
  author    = {M. R. {Maheshwarappa} and M. {Bowyer} and C. P. {Bridges}},
  title     = {Software Defined Radio ({SDR}) Architecture to Support Multi-satellite Communications},
  booktitle = {Aerospace Conference (AeroConf)},
  year      = {2015},
  pages     = {1--10},
  month     = mar,
  publisher = {IEEE},
  abstract  = {Software Defined Radio (SDR) is a key area to realise new software implementations for adaptive and reconfigurable communication systems without changing any hardware device or feature. A review on efficient use of limited bandwidth and increasing distributed satellite missions can lead to the need for a generic yet configurable communication platform that can handle multiple signals from multiple satellites with various modulation techniques, data rates and frequency bands that must be compatible to typical small satellite requirements. SDR is beneficial for space applications as it can provide the flexibility and re-configurability and this is driven by fast development times, new found heritage, reduced cost, and low mass Commercial Off-The-Shelf (COTS) components. The implementation of a combined System-On-Chip (SoC) and SDR communication platform enables additional reduction in cost as well as mass. This paper proposes a SDR architecture in which Field Programmable Gate Array (FPGA) System-on-Chip (SoC) is paired with a Radio Frequency (RF) programmable transceiver SoC to solve back-end and front-end re-configurability challenges respectively. The test-bed is aimed at implementing the signal processing software functions in both the dual-core ARM processors and associated FPGA fabric. The distribution of the functions between the FPGA fabric and dual-processor is based on profiling experiments using signal processing blocks, implemented on the development platform, in order to identify where bottlenecks exist. This paper discusses further the results from the new multi-signal / multi-satellite pipeline architecture and the subsequent bandwidth, data rate and processing requirements. Aspects of implementing and testing signal processing chains needed for CubeSat Telecommand, Telemetry and Control (TT&C) are presented together with initial results. Thus the proposed technology not only contributes for a lightweight and portable ground station but also for an on-board satellite transceiver.},
  doi       = {10.1109/AERO.2015.7119186},
  file      = {:pdf/Maheshwarappa2015 - Software Defined Radio (SDR) Architecture to Support Multi-satellite Communications.pdf:PDF},
  groups    = {FPGA, Software-Defined Radio (SDR)},
  issn      = {1095-323X},
  keywords  = {field programmable gate arrays, microprocessor chips, radio transceivers, satellite ground stations, software radio, system-on-chip, software defined radio, adaptive communication systems, reconfigurable communication systems, distributed satellite missions, configurable communication platform, modulation techniques, data rates, frequency bands, small satellite requirements, space applications, mass commercial off-the-shelf components, COTS components, SDR communication platform, field programmable gate array system-on-chip, FPGA SoC, FPGA system-on-chip, radio frequency programmable transceiver SoC, RF programmable transceiver SoC, back-end re-configurability challenges, front-end re-configurability challenges, signal processing software functions, dual-core ARM processors, signal processing blocks, multi-signal pipeline architecture, multi-satellite pipeline architecture, CubeSat Telecommand Telemetry and Control, CubeSat TT&C, portable ground station, on-board satellite transceiver, Decoding, Transmitters, Portable computers, Doppler effect, Satellites, Real-time systems, Telemetry},
}

@InProceedings{Coulton2004,
  author    = {P. {Coulton} and D. {Carline}},
  title     = {An {SDR} Inspired Design for the {FPGA} Implementation of 802.11a Baseband System},
  booktitle = {International Symposium on Consumer Electronics (ISCE)},
  year      = {2004},
  pages     = {470--475},
  month     = sep,
  publisher = {IEEE},
  doi       = {10.1109/ISCE.2004.1375991},
  file      = {:pdf/Coulton2004 - An SDR Inspired Design for the FPGA Implementation of 802.11a Baseband System.pdf:PDF},
  groups    = {FPGA, Software-Defined Radio (SDR)},
  keywords  = {Field programmable gate arrays, Baseband, Hardware, Modulation coding, Logic devices, Wireless networks, Software tools, Runtime, Tiles, Resource management},
}

@InProceedings{Skey2006,
  author    = {K. {Skey} and J. {Bradley} and K. {Wagner}},
  title     = {A Reuse Approach for {FPGA}-Based {SDR} Waveforms},
  booktitle = {Military Communications Conference (MILCOM)},
  year      = {2006},
  pages     = {1--7},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Software targeting general purpose processing (GPP) elements has been successfully reused for software defined radio (SDR) platforms in support of low-bandwidth waveforms. The joint tactical radio system (JTRS) software communications architecture (SCA) promotes reuse of GPP-based software by providing a consistent framework for developing reusable waveform implementations. However, high-bandwidth waveforms, such as those used in above 2 GHz MILSATCOM terminals, overwhelm the capabilities of GPP-only radios making field programmable gate arrays (FPGAs) a necessity in high-bandwidth radio systems. The SCA does not address the development of reusable FPGA-based waveform implementations. This paper presents an approach supplementing the current SCA to address FPGA-based platforms using system-on-a-chip (SoC) best practices for design reuse, including common interfaces and a robust system simulation environment},
  doi       = {10.1109/MILCOM.2006.302391},
  file      = {:pdf/Skey2006 - A Reuse Approach for FPGA-Based SDR Waveforms.pdf:PDF},
  groups    = {FPGA, Software-Defined Radio (SDR)},
  issn      = {2155-7586},
  keywords  = {field programmable gate arrays, military communication, software architecture, software radio, system-on-chip, FPGA-based SDR waveform, field programmable gate array, software defined radio, SCA, software communication architecture, system-on-a-chip, SoC, joint tactical radio system, JTRS, Communication system software, Software reusability, Field programmable gate arrays, System-on-a-chip, Software radio, Software systems, Computer architecture, Current supplies, Best practices, Robustness},
}

@InProceedings{Nivin2016,
  author    = {R. {Nivin} and J. S. {Rani} and P. {Vidhya}},
  title     = {Design and Hardware Implementation of Reconfigurable Nano Satellite Communication System using {FPGA} based {SDR} for {FM}/{FSK} Demodulation and {BPSK} Modulation},
  booktitle = {International Conference on Communication Systems and Networks (ComNet)},
  year      = {2016},
  pages     = {1--6},
  month     = jul,
  publisher = {IEEE},
  abstract  = {Communication system is one of the major areas in which digital signal processing finds direct application. Recent advances in signal processing have helped in enormously reducing the complexity of communication system design and also in improving the performance of the system. Software Defined Radio (SDR) enables in-orbit re-configurability of frequency, modulation scheme, data rate, bandwidth and channel coding in the case of satellite communication systems where component change is not possible after launch. This paper describes the design and hardware implementation of SDR type communication system based on FPGA for a nano satellite. The major functions carried out onboard are FM/FSK demodulation for tele command uplink and BPSK modulation with raised cosine filtering for telemetry downlink. The full system is designed and implemented based on Microsemi Smartfusion2 FPGA. For hardware evaluation of the system, Virtex-6 FPGA with high speed analog daughter card is employed. Test results are also provided at the end of the paper along with implementation of re-configurability.},
  doi       = {10.1109/CSN.2016.7823976},
  file      = {:pdf/Nivin2016 - Design and Hardware Implementation of Reconfigurable Nano Satellite Communication System using FPGA based SDR for FM FSK Demodulation and BPSK Modulation.pdf:PDF},
  groups    = {FPGA, Software-Defined Radio (SDR)},
  keywords  = {demodulation, field programmable gate arrays, filtering theory, frequency shift keying, phase shift keying, satellite communication, signal processing, software radio, telemetry, reconfigurable nanosatellite communication system, frequency modulation, frequency shift keying, FM-FSK demodulation, binary phase shift keying, BPSK modulation, digital signal processing, software defined radio, SDR, modulation scheme, channel coding, telecommand uplink, cosine filtering, telemetry downlink, field programmable gate arrays, Microsemi Smartfusion2 FPGA, Virtex-6 FPGA, high-speed analog daughter card, Frequency shift keying, Demodulation, Binary phase shift keying, Field programmable gate arrays, Receivers, FM, FSK, BPSK, raised cosine filter, under sampling, quadrature sampling, FPGA, digital receiver, SDR, NCO, digital filter, FIR filter, satellite, nano satellite, communication},
}

@InProceedings{Meshram2019,
  author    = {S. {Meshram} and N. {Kolhare}},
  title     = {The Advent Software Defined Radio: {FM} Receiver with {RTL} {SDR} and {GNU} Radio},
  booktitle = {International Conference on Smart Systems and Inventive Technology (ICSSIT)},
  year      = {2019},
  pages     = {230--235},
  month     = nov,
  publisher = {IEEE},
  abstract  = {In recent years, Software defined radio has become a cost efficient and reliable communication paradigm where it's RF front end is simplest as compared to the conventional SCR (software controlled radio). In this paper, we studied the technological revolution that is SDR communication system consists of an antenna contained with the RTL SDR USB dongle RTLU3832 and freeware GNU radio software support which highly reduce the limitations of the controlled radio. The SDR device are widely used to examine the radio Spectrum and digitize I/Q signals that are being transmitted in the range 25 MHz to 1.75 GHz by the digital communications community. The frequency bands that contains signals such as FM radio, ISM signals, GSM, 3G and LTE mobile radio, GPS, and so on can be sampled in this wide operating range. SDR give us wide scope to hold the experimentations on the real world signals with cost efficient hardware solution and GNU radio flowgraph. FM reception with SDR model is studied and illustrated with an example extent so that one can demonstrate and explore the desired frequency spectrum.},
  doi       = {10.1109/ICSSIT46314.2019.8987588},
  file      = {:pdf/Meshram2019 - The Advent Software Defined Radio\: FM Receiver with RTL SDR and GNU Radio.pdf:PDF},
  groups    = {GPP, CPU, Software-Defined Radio (SDR)},
  keywords  = {3G mobile communication, cellular radio, frequency modulation, Global Positioning System, Long Term Evolution, signal processing, software radio, telecommunication computing, telecommunication control, digital communications community, FM radio, ISM signals, GNU radio flowgraph, FM reception, SDR model, software defined radio, FM receiver, SCR, software controlled radio, SDR communication system, RTL SDR USB, RTLU3832, freeware GNU radio software support, SDR device, I-Q signals, GSM, LTE mobile radio, 3G mobile communication, GPS, frequency spectrum, frequency 25.0 MHz to 1.75 GHz, Software defined radio, RTL SDR, DSP, Raspberry pi, GNU radio, Spectrum},
}

@InProceedings{Kaur2008,
  author    = {G. {Kaur} and V. {Raj}},
  title     = {Multirate Digital Signal Processing for Software Defined Radio ({SDR}) Technology},
  booktitle = {International Conference on Emerging Trends in Engineering and Technology (ICETET)},
  year      = {2008},
  pages     = {110--115},
  month     = jul,
  publisher = {IEEE},
  abstract  = {In the past, radio systems were designed to communicate using one or two wave forms. As a result, two groups of people with different types of traditional radio were not able to communicate due to incompatibility. The need to communicate with people using different types of equipment can only be solved by using software defined radio (SDR). SDR is a wireless device that works with any communication system, be it a cellular phone, a pager, a WI-FI transceiver an AM or FM radio, a satellite communications etc. In both hardware and software Digital Signal Processing (DSP) techniques are used to design a real time system. For SDR applications involving high complexity, it becomes impossible to stay with single sampling rate and so altering the sampling rate at different stages is required for low cost DSP hardware. Thus variable sampling rates i.e. Multirate Digital Signal Processing is required.},
  doi       = {10.1109/ICETET.2008.207},
  file      = {:pdf/Kaur2008 - Multirate Digital Signal Processing for Software Defined Radio (SDR) Technology.pdf:PDF},
  groups    = {DSP, Software-Defined Radio (SDR)},
  issn      = {2157-0485},
  keywords  = {cellular radio, digital signal processing chips, real-time systems, satellite communication, software radio, wireless LAN, multirate digital signal processing, software defined radio technology, radio systems, wireless device, communication system, cellular phone, WI-FI transceiver, satellite communications, real time system, DSP hardware, Digital signal processing, Hardware, Software radio, Computer architecture, Software, Optical switches, Switches, SDR, Multirate, DSP},
}

@InProceedings{Karlsson2013,
  author    = {A. {Karlsson} and J. {Sohl} and J. {Wang} and D. {Liu}},
  title     = {e{PUMA}: A Unique Memory Access based Parallel {DSP} Processor for {SDR} and {CR}},
  booktitle = {Global Conference on Signal and Information Processing (GlobalSIP)},
  year      = {2013},
  pages     = {1234--1237},
  month     = dec,
  publisher = {IEEE},
  abstract  = {This paper presents ePUMA, a master-slave heterogeneous DSP processor for communications and multimedia. We introduce the ePUMA VPE, a vector processing slave-core designed for heavy DSP workloads and demonstrate how its features can used to implement DSP kernels that efficiently overlap computing, data access and control to achieve maximum datapath utilization. The efficiency is evaluated by implementing a basic set of kernels commonly used in SDR. The experiments show that all kernels asymptotically reach above 90\% effective datapath utilization. while many approach 100\%, thus the design effectively overlaps computing, data access and control. Compared to popular VLIW solutions, the need for a large register file with many ports is eliminated, thus saving power and chip area. When compared to a commercial VLIW solution, our solution also achieves code size reductions of up to 30 times and a significantly simplified kernel implementation.},
  doi       = {10.1109/GlobalSIP.2013.6737131},
  file      = {:pdf/Karlsson2013 - ePUMA\: A Unique Memory Access based Parallel DSP Processor for SDR and CR.pdf:PDF},
  groups    = {DSP, Software-Defined Radio (SDR)},
  keywords  = {digital signal processing chips, parallel processing, memory access based parallel DSP processor, SDR, CR, master-slave heterogeneous DSP processor, ePUMA VPE, vector processing slave-core, DSP kernels, maximum datapath utilization, data access, vector processing element, Kernel, Vectors, Registers, Digital signal processing, Computer architecture, VLIW, Assembly, ePUMA, VPE, DSP, SDR},
}

@InProceedings{Mitola1992,
  author    = {J. {Mitola}},
  title     = {Software Radios-Survey, Critical Evaluation and Future Directions},
  booktitle = {NTC-92: National Telesystems Conference},
  year      = {1992},
  pages     = {13/15--13/23},
  month     = may,
  publisher = {IEEE},
  abstract  = {Relates the performance of enabling hardware technologies to software radio requirements, portending a decade of shift from hardware radios toward software intensive approaches. Such approaches require efficient use of computational resources through topological consistency of radio functions and host architectures. This leads to a layered topology oriented design approach encapsulated in a canonical open architecture software radio model. This model underscores challenges in simulation and computer-aided design (CAD) tools for radio engineering. It aso provides a unified mathematical framework for quantitative analysis of algorithm structures, host architectures, and system performance for radio engineering CAD environments of the 1990s.},
  doi       = {10.1109/NTC.1992.267870},
  groups    = {Concepts},
  keywords  = {CAD, performance evaluation, technological forecasting, telecommunications computing, transceivers, critical evaluation, future directions, performance, software radio requirements, software intensive approaches, topological consistency, host architectures, layered topology oriented design approach, canonical open architecture software radio model, computer-aided design, CAD, radio engineering, mathematical framework, algorithm structures, Design automation, Hardware, Software radio, Software performance, Topology, Computational modeling, Computer simulation, Design engineering, Performance analysis, Algorithm design and analysis},
}

@Article{Mitola1993,
  author   = {J. {Mitola}},
  title    = {Software Radios: Survey, Critical Evaluation and Future Directions},
  journal  = {IEEE Aerospace and Electronic Systems Magazine},
  year     = {1993},
  volume   = {8},
  number   = {4},
  pages    = {25--36},
  month    = apr,
  issn     = {1557-959X},
  abstract = {A software radio is defined as a set of digital signal processing (DSP) primitives, a metalevel system for combining the primitives into communication system functions (transmitter, channel model, receiver, etc.), and a set of target processors on which the software radio is hosted for real-time communications. The performance of enabling hardware technologies is related to software radio requirements, portending a decade of shift from hardware radios toward software intensive approaches. Computational models and architecture are discussed, stressing the need for topological consistency of radio functions and host architectures. A layered topology-oriented design approach encapsulated in a canonical open architecture software radio model is presented. The model provides a unified mathematical framework for quantitative analysis of algorithm structures, host architectures, and system performance for CAD.},
  doi      = {10.1109/62.210638},
  file     = {:pdf/Mitola1993 - Software Radios\: Survey, Critical Evaluation and Future Directions.pdf:PDF},
  groups   = {Concepts},
  keywords = {CAD, digital radio systems, open systems, signal processing, telecommunications computing, computational models, software radio, digital signal processing, metalevel, communication system functions, transmitter, channel model, receiver, target processors, real-time communications, topological consistency, host architectures, canonical open architecture software, quantitative analysis, algorithm structures, CAD, Software radio, Digital signal processing, Hardware, Radio transmitters, Receivers, Real time systems, Software performance, Computational modeling, Mathematical model, Performance analysis},
}

@InProceedings{Xu2019,
  author    = {Y. {Xu} and W. {Wang} and Z. {Xu} and X. {Gao}},
  title     = {{AVX-512}~Based Software Decoding for {5G} {LDPC} Codes},
  booktitle = {International Workshop on Signal Processing Systems (SiPS)},
  year      = {2019},
  pages     = {54--59},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this paper, we investigate how the 5G NR LDPC codes can be decoded by GPP effectively with single instruction-multiple-data (SIMD) acceleration and evaluate the corresponding achievable throughput on newly released Intel Xeon CPUs. Firstly, a general software implementation architecture with SIMD acceleration for horizontal-layered LDPC decoding is presented, where the parallelism can be achieved in an intra-block manner. By utilizing Intel advanced vector extended 512 (AVX-512) instruction set, the efficiency of parallelism are maximized and therefore the capacity of x86 processors can be fully exploited. In addition, new features of AVX-512 are further exploited to optimize load and store operations as well as preprocessing to reduce the operation cost. Experiments results also show that Intel Xeon Gold 6154 processors can achieve 42 to 272 Mbps throughput with a single core for ten layered decoding iterations for various code rate and block length. The typical processing latency is below 100 $\mu s$. Consequently, an 18-core Intel Xeon CPU can achieve up to 5 Gbps decoding throughput.},
  doi       = {10.1109/SiPS47522.2019.9020587},
  file      = {:pdf/Xu2019 - AVX-512 Based Software Decoding for 5G LDPC Codes.pdf:PDF},
  groups    = {Software Decoders, HoF LDPC - BP},
  issn      = {2374-7390},
  keywords  = {Parity check codes, Decoding, 5G mobile communication, Parallel processing, Throughput, Instruction sets, LDPC decoding, single instruction multiple data (SIMD), 5G New Radio, GPP},
}

@InProceedings{Grayver2019,
  author        = {E. {Grayver}},
  title         = {Scaling the Fast x86 {DVB-S}2 Decoder to 1~{Gb}ps},
  booktitle     = {Aerospace Conference (AeroConf)},
  year          = {2019},
  pages         = {1--9},
  month         = mar,
  publisher     = {IEEE},
  abstract      = {Software implementation of LDPC decoders has been an active area of development for the last 10 years. Researchers have focused on implementing the computationally expensive algorithm on both GPPs and GPUs. A major leap in performance was reported in the groundbreaking paper by Bertrand le Gal [2]. This paper builds on the work in [2] by considering the scaling of that implementation on modern many-core processors. We look at the performance of LDPC code specified in the DVB-S2 standard. The large block size of the DVB-S2 code makes the memory architecture of the processor just as important as the clock rate and instruction set. We present results for two generations of Intel Xeons, an Intel Phi (KNL), the recently released AMD EPYC. The key finding is that performance scaling is limited by the amount of available cache memory rather than the number of cores. We also find that heavily multi-threaded, but deterministic software architecture benefits from explicit allocation of threads to cores vs. allowing the operating system to manage threading. The maximum throughput of 1 Gbps was achieved on a mid-range AMD server - issuing a new era of all-software receivers for very high rate waveforms. We also present the performance of the algorithm ported to a low-power ARM processor and compare that to a low-end Intel Core.},
  doi           = {10.1109/AERO.2019.8742225},
  file          = {:pdf/Grayver2019 - Scaling the Fast x86 DVB-S2 Decoder to 1 Gbps.pdf:PDF},
  groups        = {Software Decoders, HoF LDPC - BP},
  issn          = {1095-323X},
  keywords      = {cache storage, decoding, digital video broadcasting, instruction sets, memory architecture, multiprocessing systems, parity check codes, Fast x86 DVB-S2 Decoder, LDPC decoders, DVB-S2 standard, DVB-S2 code, memory architecture, instruction set, Intel Xeons, Intel Phi, all-software receivers, many-core processors, cache memory, deterministic software architecture, AMD EPYC, Instruction sets, Parity check codes, Decoding, Throughput, Benchmark testing, Forward error correction},
}

@Online{CRCWiki2017,
  author = {Wikipedia},
  title  = {Cyclic redundancy check},
  url    = {https://en.wikipedia.org/wiki/Cyclic_redundancy_check},
  year   = {2017}
}

@Article{Gallager1962,
  author   = {R. {Gallager}},
  title    = {Low-Density Parity-Check Codes},
  journal  = {IRE Transactions on Information Theory},
  year     = {1962},
  volume   = {8},
  number   = {1},
  pages    = {21--28},
  month    = jan,
  issn     = {2168-2712},
  abstract = {A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed numberj \geq 3of l's and each row contains a small fixed numberk > jof l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixedj. When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixedj. A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. Forj > 3and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound.},
  doi      = {10.1109/TIT.1962.1057683},
  file     = {:pdf/Gallager1962 - Low-Density Parity-Check Codes.pdf:PDF},
  groups   = {LDPC Codes},
  keywords = {Error-correcting codes, Parity checks, Parity check codes, Maximum likelihood decoding, Equations, Channel capacity, Information theory, Error probability, Linear approximation, Data communication, Error correction codes, Communication systems},
}

@Article{Islam2017,
  author   = {S. M. R. {Islam} and N. {Avazov} and O. A. {Dobre} and K. {Kwak}},
  title    = {Power-Domain Non-Orthogonal Multiple Access ({NOMA}) in {5G} Systems: Potentials and Challenges},
  journal  = {IEEE Communications Surveys Tutorials},
  year     = {2017},
  volume   = {19},
  number   = {2},
  pages    = {721--742},
  month    = sec,
  issn     = {1553-877X},
  abstract = {Non-orthogonal multiple access (NOMA) is one of the promising radio access techniques for performance enhancement in next-generation cellular communications. Compared to orthogonal frequency division multiple access, which is a well-known high-capacity orthogonal multiple access technique, NOMA offers a set of desirable benefits, including greater spectrum efficiency. There are different types of NOMA techniques, including power-domain and code-domain. This paper primarily focuses on power-domain NOMA that utilizes superposition coding at the transmitter and successive interference cancellation at the receiver. Various researchers have demonstrated that NOMA can be used effectively to meet both network-level and user-experienced data rate requirements of fifth-generation (5G) technologies. From that perspective, this paper comprehensively surveys the recent progress of NOMA in 5G systems, reviewing the state-of-the-art capacity analysis, power allocation strategies, user fairness, and user-pairing schemes in NOMA. In addition, this paper discusses how NOMA performs when it is integrated with various proven wireless communications techniques, such as cooperative communications, multiple-input multiple-output, beamforming, space-time coding, and network coding among others. Furthermore, this paper discusses several important issues on NOMA implementation and provides some avenues for future research.},
  doi      = {10.1109/COMST.2016.2621116},
  file     = {:pdf/Islam2017 - Power-Domain Non-Orthogonal Multiple Access (NOMA) in 5G Systems\: Potentials and Challenges.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, cellular radio, encoding, interference suppression, multi-access systems, next generation networks, radio receivers, radio transmitters, radiofrequency interference, power-domain nonorthogonal multiple access, NOMA, 5G system, radio access technique, next-generation cellular communication, spectrum efficiency, code-domain, superposition coding, successive interference cancellation, receiver, transmitter, technologies, power allocation strategies, user fairness, user-pairing schemes, wireless communication techniques, NOMA, 5G mobile communication, OFDM, Multiaccess communication, Receivers, Tutorials, Encoding, Non-orthogonal multiple access (NOMA), orthogonal multiple access (OMA), 5G, NOMA solutions, NOMA performance, research challenges, implementation issues},
}

@InProceedings{Nikopour2013,
  author    = {H. {Nikopour} and H. {Baligh}},
  title     = {Sparse Code Multiple Access},
  booktitle = {International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)},
  year      = {2013},
  pages     = {332--336},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Multicarrier CDMA is a multiplexing approach in which modulated QAM symbols are spread over multiple OFDMA tones by using a generally complex spreading sequence. Effectively, a QAM symbol is repeated over multiple tones. Low density signature (LDS) is a version of CDMA with low density spreading sequence allowing us to take advantage of a near optimal ML receiver with practically feasible complexity. In this paper, we propose a new multiple access scheme so called sparse code multiple access (SCMA) which still enjoys the low complexity reception technique but with better performance compared to LDS. In SCMA, the procedure of bit to QAM symbol mapping and spreading are combined together and incoming bits are directly mapped to a multidimensional codeword of an SCMA codebook set. Each layer or user has its dedicated codebook. Shaping gain of a multidimensional constellation is the main source of the performance improvement in comparison to the simple repetition of QAM symbols in LDS. In general, SCMA codebook design is an optimization problem. A systematic sub-optimal approach is proposed here for SCMA codebook design.},
  doi       = {10.1109/PIMRC.2013.6666156},
  file      = {:pdf/Nikopour2013 - Sparse Code Multiple Access.pdf:PDF},
  groups    = {SCMA},
  issn      = {2166-9589},
  keywords  = {code division multiple access, frequency division multiple access, OFDM modulation, quadrature amplitude modulation, sparse code multiple access, multicarrier CDMA, multiplexing approach, modulated QAM symbol mapping, multiple OFDMA, general complex spreading sequence, multiple tones, low density signature, LDS, low density spreading sequence, near optimal ML receiver, multiple access scheme, low complexity reception technique, multidimensional codeword, SCMA codebook set, multidimensional constellation, shaping gain, SCMA codebook design, systematic suboptimal approach, Decision support systems, Land mobile radio, SCMA, OFDMA, LDS, CDMA, MPA, factor graph, codebook, multidimensional constellation, shaping gain},
}

@Online{Altera2015,
  author  = {{Altera Innovate Asia FPGA Design Contest}},
  title   = {{{5G} Algorithm Innovation Competition.}},
  year    = {2015},
  groups  = {SCMA, 5G},
  url     = {http://www.innovateasia.com/5g/en/gp2.html},
  urldate = {2018-09-16},
}

@Misc{Alliance2015,
  author = {{NGMN Alliance}},
  title  = {{5G} White Paper},
  year   = {2015},
  file   = {:pdf/Alliance2015 - 5G White Paper.pdf:PDF},
  groups = {SCMA, 5G},
  url    = {https://www.ngmn.org/wp-content/uploads/NGMN_5G_White_Paper_V1_0.pdf},
}

@Article{Lu2015,
  author   = {L. {Lu} and Y. {Chen} and W. {Guo} and H. {Yang} and Y. {Wu} and S. {Xing}},
  title    = {Prototype for {5G} New Air Interface Technology {SCMA} and Performance Evaluation},
  journal  = {IEEE China Communications},
  year     = {2015},
  volume   = {12},
  number   = {Supplement},
  pages    = {38--48},
  month    = dec,
  issn     = {1673-5447},
  abstract = {Sparse code multiple access (SCMA) is a novel non-orthogonal multiple access scheme proposed to meet the challenging demand of the future 5G communications, especially in support of the massive connections. The coded bits from each data stream will be directly mapped as multi-dimensional SCMA codeword in complex domain and then spread onto the physical resource elements in a sparse manner. The number of codewords that can be non-orthogonally multiplexed in one SCMA block can be made much larger than the number of orthogonal resource elements therein, resulting in an overloaded system. The sparsity in the spreading pattern and the design in the multidimensional modulator jointly ensure the SCMA codewords can be robustly decoded with low complexity. In this paper, we focus on the low complexity receiver design and verified the superior of an SCMA system via simulations and real-time prototyping. Lab tests and field tests all show that SCMA is a promising candidate for 5G non-orthogonal multiple access which can provide up to 300\% overloading that triples the whole system throughput while still enjoying the link performance close to orthogonal transmissions.},
  doi      = {10.1109/CC.2015.7386169},
  file     = {:pdf/Lu2015 - Prototype for 5G New Air Interface Technology SCMA and Performance Evaluation.pdf:PDF},
  groups   = {SCMA, 5G},
  keywords = {5G mobile communication, radio receivers, subcarrier multiplexing, 5G new air interface technology, performance evaluation, sparse code multiple access, novel nonorthogonal multiple access scheme, 5G communications, data stream, physical resource elements, orthogonal resource elements, spreading pattern, multidimensional modulator, SCMA codewords, low complexity receiver design, real-time prototyping, 5G non-orthogonal multiple access, orthogonal transmissions, Complexity theory, 5G mobile communication, Modulation, Receivers, Decoding, Prototypes, 5G, non-orthogonal multiple access, sparse code multiple access (SCMA), MAX-Log MPA, massive connectivity, low latency, high reliability, Prototype verification},
}

@InProceedings{Zhang2014a,
  author    = {S. {Zhang} and X. {Xu} and L. {Lu} and Y. {Wu} and G. {He} and Y. {Chen}},
  title     = {Sparse Code Multiple Access: An Energy Efficient Uplink Approach for {5G} Wireless Systems},
  booktitle = {Global Communications Conference (GLOBECOM)},
  year      = {2014},
  pages     = {4782--4787},
  month     = dec,
  publisher = {IEEE},
  abstract  = {The rapid traffic growth and ubiquitous access requirements make it essential to explore the next generation (5G) wireless communication networks. In the current 5G research area, non-orthogonal multiple access has been proposed as a paradigm shift of physical layer technologies. Among all the existing non-orthogonal technologies, the recently proposed sparse code multiple access (SCMA) scheme is shown to achieve a better link level performance. In this paper, we extend the study by proposing an unified framework to analyze the energy efficiency of SCMA scheme and a low complexity decoding algorithm which is critical for prototyping. We show through simulation and prototype measurement results that SCMA scheme provides extra multiple access capability with reasonable complexity and energy consumption, and hence, can be regarded as an energy efficient approach for 5G wireless communication systems.},
  doi       = {10.1109/GLOCOM.2014.7037563},
  file      = {:pdf/Zhang2014a - Sparse Code Multiple Access\: An Energy Efficient Uplink Approach for 5G Wireless Systems.pdf:PDF},
  groups    = {SCMA, 5G},
  issn      = {1930-529X},
  keywords  = {5G mobile communication, multi-access systems, network coding, telecommunication traffic, sparse code multiple access, rapid traffic growth, ubiquitous access requirements, nonorthogonal multiple access, physical layer technologies, sparse code multiple access scheme, SCMA scheme, low complexity decoding algorithm, 5G wireless communication systems, Uplink, Complexity theory, Wireless communication, Artificial neural networks, Aggregates, Prototypes, Signal processing algorithms, 5G, sparse code multiple access (SCMA), energy efficiency, non-orthogonal multiple access, low complexity implementation, prototype},
}

@Article{Liu2016,
  author   = {J. {Liu} and G. {Wu} and S. {Li} and O. {Tirkkonen}},
  title    = {On Fixed-Point Implementation of log-{MPA} for {SCMA} Signals},
  journal  = {IEEE Wireless Communications Letters (WCL)},
  year     = {2016},
  volume   = {5},
  number   = {3},
  pages    = {324--327},
  month    = jun,
  issn     = {2162-2345},
  abstract = {In this letter, we present a design framework for fixed-point implementation of the log-domain message passing algorithm (Log-MPA) for sparse code multiple access signals, and make a detailed comparative analysis on the complexity of Log-MPA and traditional MPA. We also investigate the impact of the number of massage-passing iterations within the Log-MPA decoding process on the performance of Log-MPA. Simulation results demonstrate that Log-MPA achieves favorable performance with low-complexity hardware implementation as compared to MPA. We also implement Log-MPA on FPGA evaluation board to verify the performance of Log-MPA.},
  doi      = {10.1109/LWC.2016.2554557},
  file     = {:pdf/Liu2016 - On Fixed-Point Implementation of Log-MPA for SCMA Signals.pdf:PDF},
  groups   = {SCMA},
  keywords = {code division multiple access, decoding, field programmable gate arrays, iterative methods, message passing, Log-MPA fixed-point implementation, SCMA signal, log-domain message passing algorithm, sparse code multiple access signal, massage passing iteration, Log-MPA decoding process, low-complexity hardware implementation, FPGA evaluation board, Complexity theory, Hardware design languages, Maximum likelihood decoding, Field programmable gate arrays, Indexes, OFDM, Sparse code multiple access, iterative decoding, massage passing, log-Likehood ratio, bipartite graph, Sparse code multiple access, iterative decoding, massage passing, log-Likehood ratio, bipartite graph},
}

@InProceedings{Bayesteh2015,
  author    = {A. {Bayesteh} and H. {Nikopour} and M. {Taherzadeh} and H. {Baligh} and J. {Ma}},
  title     = {Low Complexity Techniques for {SCMA} Detection},
  booktitle = {Globecom Workshops (GC Wkshps)},
  year      = {2015},
  pages     = {1--6},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Sparse code multiple access (SCMA) is a codebook- based non-orthogonal multiplexing technique. In SCMA, the procedure of bit to QAM symbol mapping and spreading of CDMA are combined together and incoming bits are directly mapped to multi-dimensional codewords of SCMA codebook sets. Due to the sparse nature of codewords, SCMA enjoys the low complexity reception, taking advantage of a near optimal message passing algorithm (MPA). This makes SCMA a candidate for supporting massive connectivity in future 5G networks, where the number of users can potentially be higher than the codeword length (spreading factor). To this end, more efficient reception techniques are needed on top of what MPA delivers. In this paper, some complexity reduction techniques are presented to further reduce the SCMA decoding complexity. These techniques are considered from two perspectives: i) transmitter-side technique, by designing SCMA codebooks with a specific structure providing low complexity of detections, and ii) low complexity decoding techniques taking advantage of the SCMA codebook structure. The proposed techniques are evaluated in terms of both complexity and performance. It is shown that significant amount of complexity reduction is possible using the proposed techniques with negligible performance penalty, which paves the way of supporting various applications in future 5G systems using SCMA.},
  doi       = {10.1109/GLOCOMW.2015.7414184},
  file      = {:pdf/Bayesteh2015 - Low Complexity Techniques for SCMA Detection.pdf:PDF},
  groups    = {SCMA},
  keywords  = {5G mobile communication, communication complexity, decoding, message passing, multi-access systems, low complexity decoding techniques, transmitter-side technique, SCMA decoding complexity, complexity reduction techniques, reception techniques, future 5G networks, massive connectivity, MPA, near optimal message passing algorithm, SCMA codebook sets, multi-dimensional codewords, bit to QAM symbol mapping, codebook-based non-orthogonal multiplexing technique, sparse code multiple access, Complexity theory, Multiplexing, Decoding, Phase shift keying, Receivers, 5G mobile communication},
}

@Article{Du2016,
  author   = {Y. {Du} and B. {Dong} and Z. {Chen} and J. {Fang} and P. {Gao} and Z. {Liu}},
  title    = {Low-Complexity Detector in Sparse Code Multiple Access Systems},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2016},
  volume   = {20},
  number   = {9},
  pages    = {1812--1815},
  month    = sep,
  issn     = {1558-2558},
  abstract = {One of the challenges in the design of sparse code multiple access systems is developing low-complexity detectors. To achieve this goal, we propose a novel low-complexity detector based on an edge selection approach, which remarkably reduces the computational complexity. First, the proposed detector applies adaptive Gaussian approximation to the unselected edges that have smaller modulus of the channel coefficients, on the basis of the different channel qualities. As a result, the original factor graph can be simplified. In addition, a mean and variance feedback mechanism is employed to further compensate the information loss brought by unselected edges. Simulations show that, compared with the original message passing algorithm-based detector, the computational complexity is reduced substantially with negligible bit error rate performance degradation.},
  doi      = {10.1109/LCOMM.2016.2592912},
  file     = {:pdf/Du2016 - Low-Complexity Detector in Sparse Code Multiple Access Systems.pdf:PDF},
  groups   = {SCMA},
  keywords = {approximation theory, computational complexity, Gaussian processes, graph theory, multi-access systems, multiuser detection, low-complexity detector, sparse code multiple access systems, edge selection approach, computational complexity, adaptive Gaussian approximation, channel coefficient modulus, channel quality, factor graph, variance feedback mechanism, mean feedback mechanism, information loss compensation, multiuser detection, Detectors, Computational complexity, Image edge detection, Gaussian approximation, Bit error rate, 5G mobile communication, Sparse code multiple access (SCMA), multiuser detection, message passing algorithm (MPA), adaptive Gaussian approximation, mean and variance feedback},
}

@InProceedings{Taherzadeh2014,
  author    = {M. {Taherzadeh} and H. {Nikopour} and A. {Bayesteh} and H. {Baligh}},
  title     = {{SCMA} Codebook Design},
  booktitle = {International Conference on Vehicular Technology (VTC)},
  year      = {2014},
  pages     = {1--5},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Multicarrier CDMA is a multiple access scheme in which modulated QAM symbols are spread over OFDMA tones by using a generally complex spreading sequence. Effectively, a QAM symbol is repeated over multiple tones. Low density signature (LDS) is a version of CDMA with low density spreading sequences allowing us to take advantage of a near optimal message passing algorithm (MPA) receiver with practically feasible complexity. Sparse code multiple access (SCMA) is a multi-dimensional codebook-based non-orthogonal spreading technique. In SCMA, the procedure of bit to QAM symbol mapping and spreading are combined together and incoming bits are directly mapped to multi-dimensional codewords of SCMA codebook sets. Each layer has its dedicated codebook. Shaping gain of a multi-dimensional constellation is one of the main sources of the performance improvement in comparison to the simple repetition of QAM symbols in LDS. Meanwhile, like LDS, SCMA enjoys the low complexity reception techniques due to the sparsity of SCMA codewords. In this paper a systematic approach is proposed to design SCMA codebooks mainly based on the design principles of lattice constellations. Simulation results are presented to show the performance gain of SCMA compared to LDS and OFDMA.},
  doi       = {10.1109/VTCFall.2014.6966170},
  groups    = {SCMA},
  issn      = {1090-3038},
  keywords  = {code division multiple access, codes, frequency division multiple access, OFDM modulation, SCMA codebook design, multicarrier CDMA, multiple access scheme, QAM symbols, OFDMA tones, low density signature, LDS, generally complex spreading sequence, low density spreading sequences, near optimal message passing algorithm, MPA receiver, symbol mapping, symbol spreading, multidimensional codewords, shaping gain, multidimensional constellation, performance improvement, low complexity reception techniques, lattice constellations, design principles, performance gain, sparse code multiple access, Complexity theory, Constellation diagram, Fading, Signal to noise ratio, Quadrature amplitude modulation, Vectors, Gain},
}

@Article{Peng2017,
  author   = {J. {Peng} and W. {Chen} and B. {Bai} and X. {Guo} and C. {Sun}},
  title    = {Joint Optimization of Constellation With Mapping Matrix for {SCMA} Codebook Design},
  journal  = {IEEE Signal Processing Letters},
  year     = {2017},
  volume   = {24},
  number   = {3},
  pages    = {264--268},
  month    = mar,
  issn     = {1558-2361},
  abstract = {Sparse code multiple access (SCMA) is being considered as a promising multiple access solution for 5G systems. A distinguishing feature of SCMA is that it combines the procedures of bit to constellation symbol mapping and subsequent spreading using multidimensional codebooks differentiated by users. Such codebooks dominate the system implementation as a main source of not only performance gain but also design complexity. This letter presents a joint constellation with mapping matrix design for SCMA codebooks, which formulates the constellations optimization as a nonconvex quadratically constrained quadratic programming problem based on a set of well-constructed mapping matrices. We elaborately solve the problem to achieve outperformance over existing SCMA design in terms of bit error rate (BER). For improving practicality, an approximate approach is further proposed to reduce the complexity significantly with a limited BER loss.},
  doi      = {10.1109/LSP.2017.2653845},
  file     = {:pdf/Peng2017 - Joint Optimization of Constellation With Mapping Matrix for SCMA Codebook Design.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, code division multiple access, error statistics, quadratic programming, joint constellation optimization, SCMA codebook design, sparse code multiple access, multiple access solution, 5G systems, constellation symbol mapping, subsequent spreading, BER, bit error rate, well-constructed mapping matrix, nonconvex quadratically constrained quadratic programming, multidimensional codebooks, Optimization, Constellation diagram, Multiplexing, Matrices, Algorithm design and analysis, Sparse matrices, Complexity theory, Codebook design, constellations, mapping matrices, semidefinite relaxation (SDR), sparse code multiple access (SCMA)},
}

@Article{Yan2017,
  author   = {C. {Yan} and G. {Kang} and N. {Zhang}},
  title    = {A Dimension Distance-Based {SCMA} Codebook Design},
  journal  = {IEEE Access},
  year     = {2017},
  volume   = {5},
  pages    = {5471--5479},
  issn     = {2169-3536},
  abstract = {For sparse code multiple access (SCMA) with traditional codebooks, the initial information of message passing algorithm (MPA) receiver is easily susceptible to noise and multipath fading, and the convergence reliability of the first detected user in each decision process is unsatisfactory. Driven by these problems, an optimized codebook design for SCMA is presented in this paper. In the proposed SCMA codebook design, we first use turbo trellis coded modulation technology to design a basic complex multi-dimension constellation, which can increase the minimum Euclidean distance. Then, phase rotation and coordinate interleaving are added on the constellation to increase diversity and coordinate product distance between any constellation points. Based on these, we propose a novel criterion to select the most appropriate permutation set, which can capture as large as the sum of distance between dimensions of interfering codewords multiplexed on each resource node and maximize the diversity over the set of the sums of distance between dimensions of interfering codewords multiplexed on all resource nodes. Benefiting from the proposed codebook design, the quality of initial information of MPA receiver on each resource node and the convergence reliability of the first detected user in each decision process will be improved. Simulation results show that the bit error rate performance of SCMA with the proposed codebooks outperforms SCMA with traditional codebooks, low-density signature, and orthogonal frequency division multiple access under the same load.},
  doi      = {10.1109/ACCESS.2017.2685618},
  file     = {:pdf/Yan2017 - A Dimension Distance-Based SCMA Codebook Design.pdf:PDF},
  groups   = {SCMA},
  keywords = {error statistics, multi-access systems, receivers, trellis coded modulation, turbo codes, bit error rate, Euclidean distance, basic complex multidimension constellation, turbo trellis coded modulation technology, MPA receiver, message passing algorithm receiver, sparse code multiple access, dimension distance-based SCMA codebook design, Multiplexing, Receivers, Euclidean distance, Reliability, Encoding, Fading channels, Convergence, Codebook, LDS, MPA, permutation set, SCMA},
}

@Article{Jia2018,
  author   = {M. {Jia} and L. {Wang} and Q. {Guo} and X. {Gu} and W. {Xiang}},
  title    = {A Low Complexity Detection Algorithm for Fixed Up-Link {SCMA} System in Mission Critical Scenario},
  journal  = {IEEE Internet of Things Journal},
  year     = {2018},
  volume   = {5},
  number   = {5},
  pages    = {3289--3297},
  month    = oct,
  issn     = {2327-4662},
  abstract = {Sparse code multiple access (SCMA), as one of the most promising candidate techniques for the fifth generation communications system, is a nonorthogonal multiple access scheme which can provide large scale connections. Its philosophy is to map coded bits directly to multidimensional sparse codewords, and the message passing algorithm (MPA) is utilized to detect the multiuser signals. However, the relatively high computation of MPA detection may lower the performance when SCMA is implemented in practical applications. The partial marginalization MPA (PM-MPA) helps to reduce the computation of original MPA detection. In this paper, an improved detection scheme based on PM-MPA is proposed. Our analysis and simulation shows that compared with PM-MPA, the improved PM-MPA (IPM-MPA) can obtain a lower bit error ratio. Besides, the simulation also shows that, to achieve the same performance, the IPM-MPA is less complex than PM-MPA.},
  doi      = {10.1109/JIOT.2017.2696028},
  file     = {:pdf/Jia2018 - A Low Complexity Detection Algorithm for Fixed Up-Link SCMA System in Mission Critical Scenario.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, message passing, multi-access systems, signal detection, multiuser signal detection, sparse code multiple access, low complexity detection algorithm, lower bit error ratio, IPM-MPA, improved PM-MPA, improved detection scheme, original MPA detection, partial marginalization MPA, message passing algorithm, multidimensional sparse codewords, map coded bits, nonorthogonal multiple access scheme, fifth generation communications system, mission critical scenario, Fixed Up-Link SCMA System, Complexity theory, NOMA, Detectors, Message passing, Algorithm design and analysis, Iterative decoding, Internet, Internet of Things (IoT), low complexity, message passing algorithm (MPA), sparse code multiple access (SCMA)},
}

@Article{Yang2016,
  author   = {L. {Yang} and Y. {Liu} and Y. {Siu}},
  title    = {Low Complexity Message Passing Algorithm for {SCMA} System},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2016},
  volume   = {20},
  number   = {12},
  pages    = {2466--2469},
  month    = dec,
  issn     = {1558-2558},
  abstract = {Sparse code multiple access (SCMA) is a novel non-orthogonal air-interface technology in which several users share a same frequency resource simultaneously. The SCMA decoder is based on a message passing algorithm (MPA) for the sparsity of codeword. In the original MPA, all the users update a message until the maximum number of iterations is reached, resulting in high computational complexity. This letter puts forward a threshold-based MPA, where a belief threshold is applied to control the algorithm process. Simulation results show that the proposed scheme obtains a low computational complexity with only a slight performance degradation when the threshold is set appropriately.},
  doi      = {10.1109/LCOMM.2016.2609382},
  file     = {:pdf/Yang2016 - Low Complexity Message Passing Algorithm for SCMA System.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, compressed sensing, message passing, multi-access systems, numerical analysis, low complexity message passing algorithm, MPA, sparse code multiple access, SCMA system, nonorthogonal air-interface technology, frequency resource, codeword sparsity, Encoding, Reliability, Decoding, Computational complexity, Convergence, Numerical models, 5G, iterative multiuser detection, MPA, SCMA},
}

@Article{Du2016a,
  author   = {Y. {Du} and B. {Dong} and Z. {Chen} and J. {Fang} and L. {Yang}},
  title    = {Shuffled Multiuser Detection Schemes for Uplink Sparse Code Multiple Access Systems},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2016},
  volume   = {20},
  number   = {6},
  pages    = {1231--1234},
  month    = jun,
  issn     = {1558-2558},
  abstract = {The existing multiuser detection schemes for uplink (UP) sparse code multiple access (SCMA) systems are based on a parallel message update for message passing algorithm (MPA). In this letter, a shuffled MPA (S-MPA) scheme for UP SCMA systems is proposed, based on a serial message update strategy. Since the updated messages can join the message propagation immediately in current iteration, the convergence rate is accelerated, so that the complexity of the proposed S-MPA scheme can be substantially reduced with negligible bit error rate (BER) degradation. Simulations show that the proposed S-MPA scheme with two iterations provides similar BER performance to the original MPA scheme with six iterations. Furthermore, a parallel form of the proposed S-MPA scheme, termed group S-MPA, is developed to decrease the detection delay of the proposed S-MPA scheme, which offers a good BER-latency tradeoff.},
  doi      = {10.1109/LCOMM.2016.2551742},
  file     = {:pdf/Du2016a - Shuffled Multiuser Detection Schemes for Uplink Sparse Code Multiple Access Systems.pdf:PDF},
  groups   = {SCMA},
  keywords = {convergence of numerical methods, error statistics, iterative methods, multi-access systems, multiuser detection, shuffled multiuser detection schemes, uplink sparse code multiple access systems, UP SCMA systems, shuffled MPA scheme, S-MPA scheme, message propagation, convergence rate, negligible bit error rate degradation, BER-latency tradeoff, parallel message update, message passing algorithm, Bit error rate, Multiuser detection, Convergence, Computational complexity, Uplink, Acceleration, 5G mobile communication, Sparse code multiple access (SCMA), multiuser detection, message passing algorithm (MPA), shuffled strategy, group shuffled message passing algorithm (G-S-MPA), Sparse code multiple access (SCMA), multiuser detection, message passing algorithm (MPA), shuffled strategy, group shuffled message passing algorithm (G-S-MPA)},
}

@Article{Chen2016,
  author   = {J. {Chen} and Z. {Zhang} and S. {He} and J. {Hu} and G. E. {Sobelman}},
  title    = {Sparse Code Multiple Access Decoding Based on a {Monte} {Carlo} {Markov} Chain Method},
  journal  = {IEEE Signal Processing Letters},
  year     = {2016},
  volume   = {23},
  number   = {5},
  pages    = {639--643},
  month    = may,
  issn     = {1558-2361},
  abstract = {Nonorthogonal multiple access technology has been proposed for use in 5G communications systems. In particular, the sparse code multiple access (SCMA) scheme is believed to be one of the most promising techniques among the various nonorthogonal approaches that have been investigated. In this letter, we focus on reducing the complexity of SCMA decoding and we propose a Monte Carlo Markov Chain (MCMC) based SCMA decoder. Benefiting from the linearly increasing complexity of the MCMC method, the proposed SCMA decoder has only 10\% of the computational load compared to previous state-of-the-art methods when the codebook size is 64. Consequently, the MCMC SCMA decoder has great potential for use in practical system implementations.},
  doi      = {10.1109/LSP.2016.2544792},
  file     = {:pdf/Chen2016 - Sparse Code Multiple Access Decoding Based on a Monte Carlo Markov Chain Method.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, decoding, Markov processes, Monte Carlo methods, sparse code multiple access decoding, Monte Carlo Markov chain method, nonorthogonal multiple access technology, 5G communication system, SCMA decoding complexity reduction, nonorthogonal approach, state-of-the-art method, codebook size, MCMC SCMA decoder, Decoding, Complexity theory, Monte Carlo methods, Signal processing algorithms, Iterative decoding, Niobium, Markov processes, non-orthogonal multiple access, sparse code multiple access (SCMA), Monte Carlo Markov Chain (MCMC), low computational complexity, Low computational complexity, Monte Carlo Markov Chain (MCMC), nonorthogonal multiple access, sparse code multiple access (SCMA)},
}

@Article{Yang2017,
  author   = {L. {Yang} and X. {Ma} and Y. {Siu}},
  title    = {Low Complexity {MPA} Detector Based on Sphere Decoding for {SCMA}},
  journal  = {IEEE Communications Letters (COMML)},
  year     = {2017},
  volume   = {21},
  number   = {8},
  pages    = {1855--1858},
  month    = aug,
  issn     = {1558-2558},
  abstract = {Sparse code multiple access (SCMA) has been proposed to obtain high capacity and support massive connections. Due to the sparse feature of codewords, SCMA achieves low complexity multiuser detection by utilizing message passing algorithm (MPA). However, the complexity reduction of MPA is still a significant issue. In this letter, a novel MPA based on sphere decoding, called SD-MPA, is proposed, which narrows down the range of believable superposed constellation points. Furthermore, simulation results show that SD-MPA detector can reach low complexity detection and maintain negligible bit error ratio performance decline compared with conventional MPA.},
  doi      = {10.1109/LCOMM.2017.2697425},
  file     = {:pdf/Yang2017 - Low Complexity MPA Detector Based on Sphere Decoding for SCMA.pdf:PDF},
  groups   = {SCMA},
  keywords = {code division multiple access, decoding, message passing, multiuser detection, low complexity MPA detector, sphere decoding, SCMA, sparse code multiple access, message passing algorithm, low complexity multiuser detection, complexity reduction, believable superposed constellation points, Decoding, Detectors, AWGN, Iterative decoding, Computational complexity, Simulation, Sparse code multiple access (SCMA), message passing algorithm (MPA), sphere decoding, superposed constellation points (SCPs)},
}

@Article{Wei2017,
  author   = {F. {Wei} and W. {Chen}},
  title    = {Low Complexity Iterative Receiver Design for Sparse Code Multiple Access},
  journal  = {IEEE Transactions on Communications},
  year     = {2017},
  volume   = {65},
  number   = {2},
  pages    = {621--634},
  month    = feb,
  issn     = {1558-0857},
  abstract = {Sparse code multiple access (SCMA) is one of the most promising methods among all the non-orthogonal multiple access techniques in the future 5G communication. Compared with some other non-orthogonal multiple access techniques, such as low density signature, SCMA can achieve better performance due to the shaping gain of the SCMA code words. However, despite the sparsity of the code words, the decoding complexity of the current message passing algorithm utilized by SCMA is still prohibitively high. In this paper, by exploring the lattice structure of SCMA code words, we propose a low-complexity decoding algorithm based on list sphere decoding (LSD). The LSD avoids the exhaustive search for all possible hypotheses and only considers signal within a hypersphere. As LSD can be viewed a depth-first tree search algorithm, we further propose several methods to prune the redundancy-visited nodes in order to reduce the size of the search tree. Simulation results show that the proposed algorithm can reduce the decoding complexity substantially while the performance loss compared with the existing algorithm is negligible.},
  doi      = {10.1109/TCOMM.2016.2631468},
  file     = {:pdf/Wei2017 - Low Complexity Iterative Receiver Design for Sparse Code Multiple Access.pdf:PDF},
  groups   = {SCMA},
  keywords = {5G mobile communication, iterative decoding, message passing, tree searching, low complexity iterative receiver design, sparse code multiple access, non-orthogonal multiple access techniques, 5G communication, low density signature, shaping gain, SCMA code words, decoding complexity, current message passing algorithm, lattice structure, list sphere decoding, LSD, depth-first tree search algorithm, redundancy-visited nodes, Complexity theory, Maximum likelihood decoding, NOMA, Lattices, Message passing, Iterative decoding, Non-orthogonal multiple access, SCMA, message passing algorithm, list sphere decoding, node pruning},
}

@online{Gccfp2018,
  author  = {{GCC}},
  title   = {{Semantics of Floating Point Math in {GCC}}},
  year    = 2018,
  url     = {https://gcc.gnu.org/wiki/FloatingPointMath},
  urldate = {2018-09-16}
}

@InProceedings{Chrysos2012,
  author    = {G. {Chrysos}},
  title     = {Intel Xeon Phi coprocessor (codename Knights Corner)},
  booktitle = {Hot Chips Symposium (HCS)},
  year      = {2012},
  pages     = {1--31},
  month     = aug,
  publisher = {IEEE},
  abstract  = {This article consists of a collection of slides from the author's conference presentation on Intel's Xeon Phi (Knights Corner] coprocessor. Some of the specific topics discussed include: the special features and applications supported by Xeon Phi; Intel's integrated core architecture; coprocessor system specifications and architecture; memory management; media streaming capabilities; and system performance evaluations.},
  doi       = {10.1109/HOTCHIPS.2012.7476487},
  file      = {:pdf/Chrysos2012 - Intel Xeon Phi coprocessor (codename Knights Corner).pdf:PDF},
  groups    = {SCMA},
  keywords  = {conformance testing, coprocessors, media streaming, performance evaluation, storage management chips, Intel, Xeon Phi coprocessor, Knights Corner, integrated core architecture, coprocessor system specifications, memory management, media streaming capabilities, system performance evaluations, Optimization, Microprocessors, Network architecture, Parallel processing, Mission critical systems, Instruction sets},
}

@Article{Aulin1981a,
  author   = {T. Aulin and C. Sundberg},
  title    = {Continuous Phase Modulation - Part I: Full Response Signaling},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {1981},
  volume   = {29},
  number   = {3},
  pages    = {196--209},
  month    = {March},
  issn     = {0090-6778},
  abstract = {The continuous phase modulation (CPM) signaling scheme has gained interest in recent years because of its attractive spectral properties. Data symbol pulse shaping has previously been studied with regard to spectra, for binary data and modulation index 0.5. In this paper these results have been extended to the<tex>M</tex>-ary case, where the pulse shaping is over a one symbol interval, the so-called full response systems. Results are given for modulation indexes of practical interest, concerning both performance and spectrum. Comparisons are made with minimum shift keying (MSK) and systems have been found which are significantly better in<tex>E_{b}/N_{0}</tex>for a large signal-to-noise ratio (SNR) without expanded bandwidth. Schemes with the same bit error probability as MSK but with considerably smaller bandwidth have also been found. Significant improvement in both power and bandwidth are obtained by increasing the number of levels<tex>M</tex>from 2 to 4.},
  doi      = {10.1109/TCOM.1981.1095001},
  file     = {:pdf/Aulin1981a - Continuous Phase Modulation - Part I\: Full Response Signaling.pdf:PDF},
  groups   = {CPM},
  keywords = {MSK modulation/demodulation;PM modulation/demodulation;Phase modulation;Signal to noise ratio;Bandwidth;Error probability;Phase shift keying;Frequency;Detectors;Continuous phase modulation;Pulse shaping methods;Additive noise},
}

@Article{Aulin1981b,
  author   = {T. Aulin and N. Rydbeck and C. Sundberg},
  title    = {Continuous Phase Modulation - Part II: Partial Response Signaling},
  journal  = {IEEE Transactions on Communications (TCOM)},
  year     = {1981},
  volume   = {29},
  number   = {3},
  pages    = {210--225},
  month    = {March},
  issn     = {0090-6778},
  abstract = {An analysis of constant envelope digital partial response continuous Phase modulation (CPM) systems is reported. Coherent detection is assumed and the channel is Gaussian. The receiver observes the received signal over more than one symbol interval to make use of the correlative properties of the transmitted signal. The Systems are<tex>M</tex>-ary, and baseband pulse shaping over several symbol intervals is considered. An optimum receiver based on the Viterbi algorithm is presented. Constant envelope digital modulation schemes with excellent spectral tail properties are given. The spectra have extremely low sidelobes. It is concluded that partial response CPM systems have spectrum compaction properties. Furthermore, at equal or even smaller bandwidth than minimum shift keying (MSK), a considerable gain in transmitter power can be obtained. This gain increases with<tex>M</tex>. Receiver and transmitter configurations are presented.},
  doi      = {10.1109/TCOM.1981.1094985},
  file     = {:pdf/Aulin1981b - Continuous Phase Modulation - Part II\: Partial Response Signaling.pdf:PDF},
  groups   = {CPM},
  keywords = {MSK modulation/demodulation;PM modulation/demodulation;Partial-response coding;Phase modulation;Partial response signaling;Transmitters;Continuous phase modulation;Baseband;Pulse shaping methods;Viterbi algorithm;Digital modulation;Tail;Compaction},
}

@Misc{AlteraSCMA,
  author = {{Altera University Program}},
  title  = {The 1st {5G} Algorithm Innovation Competition-{SCMA}},
  file   = {:pdf/AlteraSCMA - The 1st 5G Algorithm Innovation Competition-SCMA.pdf:PDF},
  groups = {5G, SCMA},
  year   = {2015},
  url    = {http://www.innovateasia.com/5g/images/pdf/1st 5G Algorithm Innovation Competition-ENV1.0 - SCMA.pdf},
}

@InProceedings{Wu2015,
  author    = {Y. Wu and S. Zhang and Y. Chen},
  title     = {Iterative Multiuser Receiver in Sparse Code Multiple Access Systems},
  booktitle = {International Conference on Communications (ICC)},
  year      = {2015},
  pages     = {2918--2923},
  month     = jun,
  publisher = {IEEE},
  abstract  = {Sparse code multiple access (SCMA) is a novel non-orthogonal multiple access scheme, in which multiple users access the same channel with user-specific sparse codewords. In this paper, we consider an uplink SCMA system employing channel coding, and develop an iterative multiuser receiver which fully utilizes the diversity gain and coding gain in the system. The simulation results demonstrate the superiority of the proposed iterative receiver over the non-iterative one, and the performance gain increases with the system load. It is also shown that SCMA can work well in highly overloaded scenario, and the link-level performance does not degrade even if the load is as high as 300\%.},
  doi       = {10.1109/ICC.2015.7248770},
  file      = {:pdf/Wu2015 - Iterative Multiuser Receiver in Sparse Code Multiple Access Systems.pdf:PDF},
  groups    = {SCMA},
  issn      = {1550-3607},
  keywords  = {channel coding;iterative methods;receivers;iterative multiuser receiver;sparse code multiple access systems;novel nonorthogonal multiple access scheme;uplink SCMA system;channel coding;diversity gain;coding gain;OFDM;Decoding;Complexity theory},
}

@InProceedings{Cheng2015,
  author    = {M. Cheng and Y. Wu and Y. Chen},
  title     = {Capacity Analysis for Non-orthogonal Overloading Transmissions under Constellation Constraints},
  booktitle = {International Conference on Wireless Communications Signal Processing (WCSP)},
  year      = {2015},
  pages     = {1--5},
  month     = oct,
  publisher = {IEEE},
  abstract  = {In this work, constellation constrained (CC) capacities of a series of non-orthogonal overloading transmission schemes are derived in AWGN channels. All these schemes follow a similar transmission structure, in which modulated symbols are spread on to a group of resource elements (REs) in a sparse manner, i.e., only a part of the REs have nonzero components while the others are filled with zeros. The multiple access schemes follow this structure is called sparse code multiple access (SCMA) in general. In particular, a complete SCMA scheme would combine multi-dimensional modulation and the low density spreading (LDS) together such that the symbols from the same data layer on different REs are different but dependent. If the spread symbols are the same, it is a simplified implementation of SCMA and is called LDS. Furthermore, depending on whether the numbers of non-zero components for each data layer are equal or not, there are regular LDS (LDS in short) and irregular LDS (IrLDS), respectively. The paper would show from theoretical derivation and simulation results that the complete SCMA schemes outperform the simplified version LDS/IrLDS. Moreover, we also show that the application of phase rotation in the modulator can significantly boost the link performance of such non-orthogonal multiple access schemes.},
  doi       = {10.1109/WCSP.2015.7341294},
  file      = {:pdf/Cheng2015 - Capacity Analysis for Non-orthogonal Overloading Transmissions under Constellation Constraints.pdf:PDF},
  groups    = {SCMA},
  keywords  = {AWGN channels;code division multiple access;nonorthogonal overloading transmission capacity analysis;constellation constraint;AWGN channel;resource element;sparse code multiple access scheme;SCMA scheme;multidimensional modulation;low density spreading;nonorthogonal multiple access scheme link performance;code division multiple access;Nickel;Modulation;5G mobile communication;Multiaccess communication;Sparse matrices;AWGN channels;Simulation;SCMA;LDS;IrLDS;Constellation Constrained capacity;Non-orthogonal multiple access},
}

@InProceedings{Zhang2016,
  author    = {S. Zhang and K. Xiao and B. Xiao and Z. Chen and B. Xia and D. Chen and S. Ma},
  title     = {A Capacity-based Codebook Design Method for Sparse Code Multiple Access Systems},
  booktitle = {International Conference on Wireless Communications Signal Processing (WCSP)},
  year      = {2016},
  pages     = {1--5},
  month     = oct,
  publisher = {IEEE},
  abstract  = {Sparse code multiple access (SCMA) is a novel non-orthogonal multiple access scheme, which exploits a multidimensional constellation based on the non-orthogonal spreading technique. The SCMA multi-user codebook design is the bottleneck of system performance and there is no design guideline from the perspective of capacity. In this paper, a novel SCMA codebook design scheme is proposed to maximize the sum rate, where we transfer the design of multi-dimensional constellation sets to the optimizing of series of 1-dimensional complex codewords. Specifically, a basic M-order pulse-amplitude modulation (PAM) is optimized and then the angles of rotation between the input 1-dimensional constellation and the basic M-PAM constellation can be obtained within a feasible calculation complexity to improve the sum-rate. Finally, the series of 1-dimensional complex codewords are combined to construct multi-dimensional codebooks based on Latin square criterion. Numerical results illustrate that the proposed codebook outperforms the existing codebook by 1.3 dB over AWGN channel and 1.1 dB over Rayleigh channel in terms of bit error rate (BER) performance.},
  doi       = {10.1109/WCSP.2016.7752620},
  file      = {:pdf/Zhang2016 - A Capacity-based Codebook Design Method for Sparse Code Multiple Access Systems.pdf:PDF},
  groups    = {SCMA},
  issn      = {2472-7628},
  keywords  = {AWGN channels;error statistics;multi-access systems;pulse amplitude modulation;Rayleigh channels;capacity-based codebook design;sparse code multiple access systems;non-orthogonal multiple access scheme;non-orthogonal spreading;SCMA;multi-user codebook design;M-order pulse-amplitude modulation;PAM;Latin square criterion;AWGN channel;Rayleigh channel;bit error rate;BER;Design methodology;Modulation;Signal to noise ratio;NOMA;Uplink;Optimization;Receivers},
}

@InProceedings{Klimentyev2016,
  author    = {V. P. Klimentyev and A. B. Sergienko},
  title     = {Detection of {SCMA} Signal with Channel Estimation Error},
  booktitle = {Conference of Open Innovations Association and Seminar on Information Security and Protection of Information Technology (FRUCT-ISPIT)},
  year      = {2016},
  pages     = {106--112},
  month     = apr,
  publisher = {IEEE},
  abstract  = {Fifth generation wireless communication systems should support, among other things, very large number of simultaneous connections. To address this requirement, various schemes of non-orthogonal multiple access (NOMA) were proposed that allow to increase the number of simultaneously active users. One of NOMA schemes is sparse code multiple access (SCMA), where sparse multidimensional codewords allow to use iterative detecting algorithms with reasonable complexity. In the paper, SCMA detection is investigated in the presence of channel estimation error. Uncoded and turbo coded SCMA is analyzed. Uplink channel with Rayleigh flat block fading is assumed. Simulation results show that required accuracy of the channel estimation depends on the turbo code block length. For full utilization of turbo code error-correction capability with short blocks (40 bits) normalized variance of channel estimation error should be less than 10<sup>-3</sup>, the same value applies to the case of uncoded SCMA. For turbo code with long blocks (1024 bits), estimation can be less accurate, with normalized variance up to 10<sup>-2</sup>. With such channel estimation accuracy, power loss is about 0.6-0.7 dB compared with the case of perfect estimation. Two different types of codebooks have shown the same performance for coded SCMA, that leads to conclusion that codebook with more simple structure that provides less complexity of detection algorithm is a good candidate for use in SCMA schemes. The comparison with traditional orthogonal multiple access scheme with the same overall spectral efficiency is provided for both uncoded and coded SCMA systems. In case of coded system, SCMA scheme is shown to have smaller BER in the range of bit error probabilities below 10<sup>-4</sup>-10<sup>-5</sup> for long blocks. The power gain is 0.5-1 dB for long blocks and a few tenths of dB for short blocks.},
  doi       = {10.1109/FRUCT-ISPIT.2016.7561515},
  file      = {:pdf/Klimentyev2016 - Detection of SCMA Signal with Channel Estimation Error.pdf:PDF},
  groups    = {SCMA},
  issn      = {2305-7254},
  keywords  = {block codes;channel coding;channel estimation;error correction codes;error statistics;multi-access systems;Rayleigh channels;signal detection;turbo codes;SCMA signal detection;channel estimation error;fifth generation wireless communication system;nonorthogonal multiple access scheme;NOMA scheme;sparse code multiple access;sparse multidimensional codeword;iterative detecting algorithm;turbo coded SCMA analyzed;uncoded SCMA analysis;uplink channel;Rayleigh flat block fading;turbo code error-correction capability;BER;bit error probability;Channel estimation;Fading channels;NOMA;Complexity theory;Turbo codes;Estimation;5G mobile communication},
}

@Article{Song2017,
  author   = {G. Song and X. Wang and J. Cheng},
  title    = {Signature Design of Sparsely Spread Code Division Multiple Access Based on Superposed Constellation Distance Analysis},
  journal  = {IEEE Access},
  year     = {2017},
  volume   = {5},
  pages    = {23809--23821},
  month    = oct,
  issn     = {2169-3536},
  abstract = {Sparsely spread code division multiple access (SCDMA) is a non-orthogonal superposition coding scheme that allows concurrent communications between a base station and multiple users over a common channel. However, the detection performance of an SCDMA system is mainly determined by its signature matrix, which should be sparse to facilitate the belief propagation (BP) detection. On the other hand, to guarantee good maximum likelihood (ML) detection performance, the minimum Euclidean distance for the equivalent signal constellation after multi-user superposition should be maximized. In this paper, a code distance analysis is proposed for SCDMA systems with a finite number of users and spreading lengths. Based on this analysis, good signature matrices whose factor graphs have very few short cycles and possess large superposed signal constellation distances are designed. The proposed signature matrices have both good BP and ML detection performances. Moreover, their BP detection performances exactly converge to their ML detection performances with few iterations. It is worth pointing out that the proposed signature matrix design could be directly applied to the 5G non-orthogonal multiple access systems.},
  doi      = {10.1109/ACCESS.2017.2765346},
  file     = {:pdf/Song2017 - Signature Design of Sparsely Spread Code Division Multiple Access Based on Superposed Constellation Distance Analysis.pdf:PDF},
  keywords = {channel coding;code division multiple access;interference suppression;matrix algebra;radio links;radio receivers;radiofrequency interference;signal detection;signature design;sparsely spread code division multiple access;superposed constellation distance analysis;nonorthogonal superposition coding scheme;concurrent communications;base station;multiple users;common channel;SCDMA system;belief propagation detection;minimum Euclidean distance;equivalent signal constellation;multiuser superposition;code distance analysis;spreading lengths;superposed signal constellation distances;ML detection performances;BP detection performances;signature matrix design;5G nonorthogonal multiple access systems;maximum likelihood detection performance;Multiaccess communication;Sparse matrices;Quadrature amplitude modulation;NOMA;Base stations;Complexity theory;Constellation diagram;Non-orthogonal multiple access;sparsely spread;signature design;code distance},
}

@InProceedings{Klimentyev2017,
  author    = {V. P. Klimentyev and A. B. Sergienko},
  title     = {{SCMA} Codebooks Optimization Based on Genetic Algorithm},
  booktitle = {European Wireless Conference},
  year      = {2017},
  pages     = {1--6},
  month     = may,
  publisher = {IEEE},
  abstract  = {Sparse code multiple access (SCMA) is a nonorthogonal multiple access scheme based on joint modulation and spread spectrum procedure. This scheme allows to increase the number of active users inside a given time-frequency resource. Design of SCMA codebooks is a challenging problem. The paper considers the design of SCMA codebooks based on Genetic Algorithm (GA). In general case, mother constellation is not required for GA, and optimization is performed directly on the codebooks. We present the obtained structured codebooks containing two antipodal codeword pairs. Optimization was carried out to improve SCMA signal performance in additive white Gaussian noise (AWGN) channel by constraining the average energy of codewords. The obtained codebooks have minimum Euclidean distance equal to 0.87 (at unit average power of SCMA signal) and outperform known codebooks in AWGN channel with both Maximum Likelihood (ML) receiver or Message Passing Algorithm (MPA) detection. The comparison of existing and proposed codebooks is performed using computer simulation and union bounds. The asymptotic gain over the best known codebooks is 0.8 dB. As a result of the simulation, proposed codebooks demonstrate similar performance for ML and MPA detection algorithms. The proposed method is suitable for codebook design for other non-orthogonal schemes, e. g., Pattern Division Multiple Access (PDMA).},
  groups    = {SCMA},
  url       = {https://ieeexplore.ieee.org/document/8011314},
}

@InProceedings{Florian2018,
  author    = {F. Florian},
  title     = {{PHYSIM} - A Physical Layer Simulation Software},
  booktitle = {International Conference on Consumer Electronics (ICCE)},
  year      = {2018},
  pages     = {1--6},
  month     = sep,
  publisher = {IEEE},
  abstract  = {Modern communication systems use different physical layer (PHY) techniques with diverse performance characteristics. Each PHY configuration option introduces trade-offs for spectral efficiency, robustness, out-of-band emissions, peak-to-average power ratio, and implementation complexity. This paper introduces the cross-platform PHYSIM software framework. PHYSIM can compare established modulations like orthogonal frequency-division multiplexing (OFDM) with modern alternatives, e.g., filter bank multi-carrier, universal filtered multi-carrier, and filtered-OFDM. Additionally, PHYSIM can draw performance comparisons between new coding schemes like polar codes and more established methods like low-density parity-check codes and turbo codes. The PHYSIM graphical user interface (GUI) assists in the design of physical layer parameters. The GUI offers plots to compare the performance of PHY configurations in transmission channels. The plots display the transmitted spectra, the received constellation diagrams, as well as the modulation, bit, and frame error ratios. Hence, the PHYSIM simulation framework assists the designer of a physical layer system and helps to find PHY settings for a target application.},
  doi       = {10.1109/ICCE-Berlin.2018.8576187},
  file      = {:pdf/Florian2018 - PHYSIM - A Physical Layer Simulation Software.pdf:PDF},
  groups    = {Modem},
  issn      = {2166-6822},
  keywords  = {graphical user interfaces;multiplexing;OFDM modulation;parity check codes;software engineering;software packages;turbo codes;physical layer simulation software;PHY configuration option;cross-platform PHYSIM software framework;orthogonal frequency-division multiplexing;filter bank multicarrier;universal filtered multicarrier;filtered-OFDM;polar codes;low-density parity-check codes;turbo codes;GUI;physical layer parameters;PHY configurations;frame error ratios;PHYSIM simulation framework;physical layer system;PHY settings;PHYSIM graphical user interface;Modulation;Physical layer;OFDM;Graphical user interfaces;Forward error correction;Parity check codes;Transmitters},
}

@InProceedings{Pignoly2018,
  author    = {V. Pignoly and B. {Le Gal} and C. J\'ego and B. Gadat},
  title     = {High Data Rate and Flexible Hardware {QC}-{LDPC} Decoder for Satellite Optical Communications},
  booktitle = {International Symposium on Turbo Codes and Iterative Information Processing (ISTC)},
  year      = {2018},
  pages     = {1--5},
  month     = dec,
  publisher = {IEEE},
  abstract  = {Satellite optical communications are trendy forfuture satellite wireless communications and introduce newchallenges concerning throughput and error correction for thechannel decoder. This paper details a custom solution basedon LDPC code and architecture co-design. This solution wasdesigned to deliver an efficient trade-off in terms of decoderthroughput and error correction performance. A flexible archi-tecture that implements an Offset Min-Sum algorithm on Field-Programmable Gate Array (FPGA) is proposed. A resultingsystem composed of 5 elementary LDPC decoders that fit ina single Zynq Ultrascale+ FPGA device reaches more than 10Gbps with error correction performance similar or better thanpresent current satellite communication standards.},
  doi       = {10.1109/ISTC.2018.8625274},
  file      = {:pdf/Pignoly2018 - High Data Rate and Flexible Hardware QC-LDPC Decoder for Satellite Optical Communications.pdf:PDF},
  groups    = {Hardware Decoders, LDPC Codes},
  issn      = {2165-4719},
  keywords  = {Decoding;Throughput;Iterative decoding;Hardware;Standards;Error correction codes},
}

@Article{Ghanaatian2018,
  author   = {R. Ghanaatian and A. Balatsoukas-Stimming and T. C. Müller and M. Meidlinger and G. Matz and A. Teman and A. Burg},
  title    = {A 588-{Gb/s} {LDPC} Decoder Based on Finite-Alphabet Message Passing},
  journal  = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year     = {2018},
  volume   = {26},
  number   = {2},
  pages    = {329-340},
  month    = {Feb},
  issn     = {1063--8210},
  abstract = {An ultrahigh throughput low-density parity-check (LDPC) decoder with an unrolled full-parallel architecture is proposed, which achieves the highest decoding throughput compared to previously reported LDPC decoders in the literature. The decoder benefits from a serial message-transfer approach between the decoding stages to alleviate the well-known routing congestion problem in parallel LDPC decoders. Furthermore, a finite-alphabet message passing algorithm is employed to replace the VN update rule of the standard min-sum (MS) decoder with lookup tables, which are designed in a way that maximizes the mutual information between decoding messages. The proposed algorithm results in an architecture with reduced bit-width messages, leading to a significantly higher decoding throughput and to a lower area compared to an MS decoder when serial message transfer is used. The architecture is placed and routed for the standard MS reference decoder and for the proposed finite-alphabet decoder using a custom pseudo-hierarchical backend design strategy to further alleviate routing congestions and to handle the large design. Postlayout results show that the finite-alphabet decoder with the serial message-transfer architecture achieves a throughput as large as 588 Gb/s with an area of 16.2 mm 2 and dissipates an average power of 22.7 pJ per decoded bit in a 28-nm fully depleted silicon on isulator library. Compared to the reference MS decoder, this corresponds to 3.1 times smaller area and 2 times better energy efficiency.},
  doi      = {10.1109/TVLSI.2017.2766925},
  file     = {:pdf/Ghanaatian2018 - A 588-Gbps LDPC Decoder Based on Finite-Alphabet Message Passing.pdf:PDF},
  groups   = {LDPC Codes, Hardware Decoders},
  keywords = {decoding;message passing;parallel architectures;parity check codes;silicon-on-insulator;table lookup;telecommunication computing;telecommunication network routing;ultrahigh throughput low-density parity-check decoder;full-parallel architecture;highest decoding throughput;decoder benefits;decoding stages;parallel LDPC decoders;finite-alphabet message passing algorithm;standard min-sum decoder;decoding messages;reduced bit-width messages;higher decoding throughput;standard MS reference decoder;finite-alphabet decoder;custom pseudohierarchical backend design strategy;serial message-transfer architecture;reference MS decoder;serial message transfer approach;lookup tables;routing congestion alleviation;fully depleted silicon on isulator library;Decoding;Throughput;Routing;Iterative decoding;Complexity theory;Computer architecture;28-nm FD-SOI;finite-alphabet decoder;low-density parity-check (LDPC) code;min-sum (MS) decoding;unrolled architecture},
}

@PhdThesis{Cenova2019,
  author   = {T. Cenova},
  title    = {Exploring {HLS} Coding Techniques to Achieve Desired Turbo Decoder Architectures},
  school   = {Rochester Institute of Technology},
  year     = {2019},
  abstract = {Software defined radio (SDR) platforms implement many digital signal processing algorithms. These can be accelerated on an FPGA to meet performance requirements. Due to the flexibility of SDR's and continually evolving communications protocols, high level synthesis (HLS) is a promising alternative to standard handcrafted design flows. A crucial component in any SDR is the error correction codes (ECC). Turbo codes are a common ECC that are implemented on an FPGA due to their computational complexity. The goal of this thesis is to explore the HLS coding techniques required to produce a design that targets the desired hardware architecture and can reach handcrafted levels of performance.

This work implemented three existing turbo decoder architectures with HLS to produce quality hardware which reaches handcrafted performance. Each targeted design was analyzed to determine its functionality and algorithm so a C implementation could be developed. Then the C code was modified and HLS directives were added to refine the design through the HLS tools. The process of code modification and processing through the HLS tools continued until the desired architecture and performance were reached.

Each design was implemented and the bottlenecks were identified and dealt with through appropriate usage of directives and C style. The use of pipelining to bypass bottlenecks added a small overhead from the ramp-up and ramp-down of the pipeline, reducing the performance by at most 1.24\%. The impact of the clock constraint set within the HLS tools was also explored. It was found that the clock period and resource usage estimate generated by the HLS tools is not accurate and all evaluations should occur after hardware synthesis.},
  file     = {:pdf/Cenova2019 - Exploring HLS Coding Techniques to Achieve Desired Turbo Decoder Architectures Decoder Architectures.pdf:PDF},
  groups   = {Turbo Codes, Hardware Decoders},
  keywords = {Turbo codes, Hardware architecture, HLS},
  url      = {https://scholarworks.rit.edu/theses/10256/},
}

@TechReport{Guermouche2019,
  author      = {A. Guermouche and A-C. Orgerie},
  title       = {Experimental Analysis of Vectorized Instructions Impact on Energy and Power Consumption under Thermal Design Power Constraints},
  institution = {T\'el\'ecom SudParis and Inria Rennes - Bretagne Atlantique},
  year        = {2019},
  type        = {resreport},
  month       = jun,
  note        = {working paper or preprint},
  abstract    = {Vectorized instructions were introduced to improve the performance of applications. However, they come with an increase in the power consumption cost. As a consequence, processors are designed to limit the frequency of the processors when such instructions are used in order to maintain the thermal design power.

In this paper, we study and compare the impact of thermal design power and SIMD instructions on performance, power and energy consumption of processors and memory. The study is performed on three different architectures providing different characteristics and four applications with different profiles (including one application with different phases, each phase having a different profile).

The study shows that, because of processor frequency, performance and power consumption are strongly related under thermal design power. It also shows that AVX512 has unexpected behavior regarding processor power consumption, while DRAM power consumption is impacted by SIMD instructions because of the generated memory throughput.},
  file        = {:pdf/Guermouche2019 - Experimental Analysis of Vectorized Instructions Impacton Energy and Power Consumption under Thermal Design Power Constraints.pdf:PDF},
  groups      = {Single Instruction Multiple Data (SIMD)},
  keywords    = {TDP ; SIMD instructions ; Power consumption ; Memory ; Energy efficiency},
  url         = {https://hal.archives-ouvertes.fr/hal-02167083v2},
}

@Misc{Hsieh2020,
  author        = {K. Hsieh and R. Venkataramanan},
  title         = {Modulated Sparse Superposition Codes for the Complex {AWGN} Channel},
  month         = apr,
  year          = {2020},
  abstract      = {This paper studies a generalization of sparse superposition codes (SPARCs) for communication over the complex AWGN channel. In a SPARC, the codebook is defined in terms of a design matrix, and each codeword is a generated by multiplying the design matrix with a sparse message vector. In the standard SPARC construction, information is encoded in the locations of the non-zero entries of the message vector. In this paper we generalize the construction and consider modulated SPARCs, where information in encoded in both the locations and the values of the non-zero entries of the message vector. We focus on the case where the non-zero entries take values from a Phase Shift Keying (PSK) constellation. We propose a computationally efficient Approximate Message Passing (AMP) decoder, and obtain analytical bounds on the state evolution parameters which predict the error performance of the decoder. Using these bounds we show that PSK-modulated SPARCs are asymptotically capacity achieving for the complex AWGN channel, with either spatial coupling or power allocation. We also provide numerical simulation results to demonstrate the error performance at finite code lengths. These results show that introducing modulation to the SPARC design can significantly reduce decoding complexity without sacrificing error performance.},
  archiveprefix = {arXiv},
  eprint        = {2004.09549},
  file          = {:pdf/Hsieh2020 - Modulated Sparse Superposition Codes for the Complex AWGN Channel.pdf:PDF},
  groups        = {Modem},
  keywords      = {Sparse regression codes, AWGN channel, approximate message passing, spatial coupling, compressed sensing, capacity-achieving codes, low-complexity decoding},
  primaryclass  = {cs.IT},
}

@Conference{Krainyk2019,
  author    = {Y. Krainyk and I. Sidenko and O Kylymovych},
  title     = {Software Models for Investigation of Turbo-Product-codes Decoding},
  booktitle = {International Conference on ICT in Education, Research, and Industrial Applications (ICTERI)},
  year      = {2019},
  month     = jun,
  abstract  = {In the following paper we provide analysis on testing procedure and development of software models, and application architecture for investigation of error-correcting codes’ parameters and decoding algorithms. The proposed models can be applied for the development of software for decoding Turbo-Product Codes (TPC). They allow simplifying development process and retrieve universal solution for TPC investigation. The models are described in the Unified Modeling Language (UML) and follow design pattern recommendations. They can be used for software implementation in various programming languages that support object-oriented model. Heatmap visualization tool is supposed to be the main part for visual investigation of the decoding process. In this work, we propose metrics for heatmap organization and explained behavior of the cells to deliver comprehensible presentation of the message state during ecoding process. The combination of metric and heatmap component provides effective way to observe impact of each decoding algorithm element on the process and gives abundant information for comparative analysis about algorithm improvements.},
  file      = {:pdf/Krainyk2019 - Software Models for Investigation of Turbo-Product-codes Decoding.pdf:PDF},
  groups    = {Software Decoders},
  url       = {http://ceur-ws.org/Vol-2387/20190152.pdf},
}

@InProceedings{Poulenard2018,
  author       = {S. Poulenard and B. Gadat and J. F. Chouteau and T. Anfray and C. Poulliat and C. Jego and O. Hartmann and G. Artaud and H. Meric},
  title        = {Forward Error Correcting Code for High Data Rate {LEO} Satellite Optical Downlinks},
  booktitle    = {International Conference on Space Optics (ICSO)},
  year         = {2018},
  editor       = {Zoran Sodnik and Nikos Karafolas and Bruno Cugny},
  volume       = {11180},
  pages        = {2029--2038},
  organization = {International Society for Optics and Photonics},
  publisher    = {SPIE},
  abstract     = {A simulation framework based on a physical-layer based abstraction to predict physical layer performances and to compare different forward error correcting (FEC) codes is presented. This framework is used to jointly design interleaving and FEC schemes for free space optical link. A sub-class of regular Low-Density Parity-Check codes is shown to be an interesting alternative to current space communication standard for optical links that require low error floor and high decoder throughput. End-to-end simulations show the feasibility of error free link from a LEO satellite to a high complexity ground station at 25Gbits/s and from a LEO satellite to low complexity optical ground station at 10 Gbits/s. The proposed protection scheme is composed of FG LDPC code and a bit interleaver to span the burst of errors.},
  doi          = {10.1117/12.2536120},
  file         = {:pdf/Poulenard2018 - Forward Error Correcting Code for High Data Rate LEO Satellite Optical Downlinks.pdf:PDF},
  groups       = {LDPC Codes},
  keywords     = {Optical link, Error Correcting Code, LDPC Code, Decoder Design},
}

@Misc{Rush2020,
  author        = {C. Rush and K. Hsieh and R. Venkataramanan},
  title         = {Capacity-achieving Spatially Coupled Sparse Superposition Codes with {AMP} Decoding},
  month         = feb,
  year          = {2020},
  abstract      = {Sparse superposition codes (SPARCs) are a class of codes for efficient communication over the AWGN channel at rates approaching the channel capacity. In a standard SPARC, codewords are sparse linear combinations of columns of an i.i.d. Gaussian design matrix, while in a spatially coupled SPARC, the design matrix has a block-wise structure where the variance of the Gaussian entries can be varied across blocks. A well-designed spatial coupling structure can enhance the error performance of iterative decoding algorithms such as Approximate Message Passing (AMP). In this paper, we obtain a non-asymptotic bound on the probability of error of spatially coupled SPARCs with AMP decoding. Applying this bound to a simple band-diagonal design matrix, we prove that spatially coupled SPARCs with AMP decoding achieve the capacity of the AWGN channel. The bound also highlights how the decay of error probability depends on each design parameter of the spatially coupled SPARC.
The asymptotic mean squared error (MSE) of the AMP decoder can be predicted via a deterministic recursion called state evolution. Our result provides the first proof that the MSE concentrates on the state evolution prediction for spatially coupled SPARCs. Combined with the state evolution prediction, the result implies that spatially coupled SPARCs are capacity-achieving. The proof technique used to establish the main result is also used to obtain a concentration inequality for the MSE of AMP applied to compressed sensing with spatially coupled design matrices. Finally, we provide numerical simulation results that demonstrate the finite length error performance of spatially coupled SPARCs. The performance is compared with coded modulation schemes that use LDPC codes from the DVB-S2 standard.},
  archiveprefix = {arXiv},
  eprint        = {2002.07844},
  file          = {:pdf/Rush2020 - Capacity-achieving Spatially Coupled Sparse Superposition Codeswith AMP Decoding.pdf:PDF},
  groups        = {Error-Correcting Codes (ECC)},
  primaryclass  = {cs.IT},
}

@Misc{Tasdighi2020,
  author        = {A. Tasdighi and E. Boutillon},
  title         = {Integer Ring Sieve ({IRS}) for Constructing Compact {QC-LDPC} Codes with Large Girth},
  month         = mar,
  year          = {2020},
  abstract      = {This paper proposes a new method of construction of compact fully-connected Quasi-Cyclic Low Density Parity Check (QC-LDPC) code with girth g = 10 and g = 12. The originality of the proposed method is to impose constraint on the exponent matrix P to reduce the search space drastically. For a targeted expansion factor of N, the first step of the method is to sieve the integer ring Z_N to make a particular sub-group with specific properties to construct the second column of P (the first column being filled with zeros). The remaining columns of P are determined recursively as multiples of the second column thanks to an adaptation of the sequentially multiplied column (SMC) method where a controlled greedy search is applied at each step. The codes constructed with the proposed semi-algebraic method have lengths that can be significantly shorter than the best counterparts in the literature. To illustrate the great potential of the SMC method, we give the explicit construction of a rate 0.75 irregular LDPC code of size 65,220 that allows a gain of 0.15 dB compared to the code of same rate and size 64,800 of the DVB-S2.},
  archiveprefix = {arXiv},
  eprint        = {2003.08707},
  file          = {:pdf/Tasdighi2020 - Integer Ring Sieve (IRS) for Constructing Compact QC-LDPC Codes with Large Girth.pdf:PDF},
  groups        = {LDPC Codes},
  primaryclass  = {cs.IT},
}

@InProceedings{Wang2019,
  author    = {Y. {Wang} and L. {Chen} and Q. {Wang} and Y. {Zhang} and Z. {Xing}},
  title     = {Algorithm and Architecture for Path Metric Aided Bit-Flipping Decoding of Polar Codes},
  booktitle = {Wireless Communications and Networking Conference (WCNC)},
  year      = {2019},
  pages     = {1--6},
  month     = apr,
  abstract  = {Polar codes attract more and more attention of researchers in recent years, since its capacity achieving property. However, their error-correction performance under successive cancellation (SC) decoding is inferior to other modern channel codes at short or moderate blocklengths. SC-Flip (SCF) decoding algorithm shows higher performance than SC decoding by identifying possibly erroneous decisions made in initial SC decoding and flipping them in the sequential decoding attempts. However, it performs not well when there are more than one erroneous decisions in a codeword. In this paper, we propose a path metric aided bit-flipping decoding algorithm to identify and correct more errors efficiently. In this algorithm, the bit-flipping list is generated based on both log likelihood ratio (LLR) based path metric and bit-flipping metric. The path metric is used to verify the effectiveness of bit-flipping. In order to reduce the decoding latency and computational complexity, its corresponding pipeline architecture is designed. By applying these decoding algorithm and pipeline architecture, an improvement on error-correction performance can be got up to 0.25dB compared with SCF decoding at frame error rate of 10-4, with low average decoding latency.},
  doi       = {10.1109/WCNC.2019.8885419},
  file      = {:pdf/Wang2019 - Algorithm and Architecture for Path Metric Aided Bit-Flipping Decoding of Polar Codes.pdf:PDF},
  groups    = {Polar Codes},
  issn      = {1558-2612},
  keywords  = {channel coding, computational complexity, error correction codes, error statistics, parity check codes, sequential decoding, path metric aided bit-flipping decoding, polar codes, error-correction performance, successive cancellation decoding, modern channel codes, short blocklengths, moderate blocklengths, SC-Flip decoding algorithm, SC decoding, sequential decoding, bit-flipping list, log likelihood ratio based path metric, SCF decoding, low average decoding, Decoding, Measurement, Pipelines, Computer architecture, Reliability, Conferences, successive cancellation flip, path metric, bit-flipping metric, pipeline architecture, polar codes},
}

@Article{Wang2019a,
  author   = {Y. {Wang} and L. {Chen} and C. {Liu} and Z. {Xing}},
  title    = {An Improved Concatenation Scheme of {BCH}-Polar Codes With Low-Latency Decoding Architecture},
  journal  = {IEEE Access},
  year     = {2019},
  volume   = {7},
  pages    = {95867--95877},
  issn     = {2169-3536},
  abstract = {Polar codes have drawn much research attention in the last ten years for their capacity-achieving property. However, their conventional successive cancellation decoding method performs not well at a short or moderate length. In order to improve the performance, concatenation with other error-correction codes has been proved an effective approach, whereas current concatenation schemes using rate-optimized method are too complex to implement with long decoding latency. In this paper, we propose a critical set protected BCH-Polar code with its corresponding decoding architecture. In the proposed concatenation scheme, we only provide extra protection to partial information bits in the critical set, which is constructed based on the channel reliability. For its corresponding decoding architecture, we redesign some components and adopt the Look-up table decoding method for BCH codes, resulting in much degradation of decoding latency. Compared with existing decoders, the hardware implementation shows low decoding latency and high throughput-area efficiency.},
  doi      = {10.1109/ACCESS.2019.2929188},
  file     = {:pdf/Wang2019a - An Improved Concatenation Scheme of BCH-PolarCodes With Low-Latency Decoding Architecture.pdf:PDF},
  groups   = {Polar Codes},
  keywords = {concatenated codes, decoding, error correction codes, polar codes, capacity-achieving property, conventional successive cancellation, error-correction codes, rate-optimized method, long decoding, table decoding method, BCH codes, low decoding, improved concatenation scheme, BCH-polar codes, Decoding, Reliability, Iterative decoding, Complexity theory, Concatenated polar codes, low latency, critical set, successive cancellation decoding, BCH codes},
}

@Online{Aicodix,
  author = {{aicodix GmbH}},
  title  = {Reusable {C++} Not-{DSP}-related Code Library},
  year   = {2018},
  groups = {C/C++ Librairies},
  url    = {https://github.com/aicodix/code},
}

@Online{ECCpage,
  author = {R. H. {Morelos-Zaragoza}},
  title  = {The Error Correcting Codes ({ECC}) Page},
  year   = {1989},
  groups = {C/C++ Librairies},
  url    = {http://www.eccpage.com},
}

@Online{EZPWDRS,
  author = {P. Kundert},
  title  = {{EZPWD} {Reed-Solomon}},
  year   = {2014},
  groups = {C/C++ Librairies},
  url    = {https://github.com/pjkundert/ezpwd-reed-solomon},
}

@Online{FastECC,
  author = {B. Ziganshin},
  title  = {{FastECC}},
  year   = {2015},
  groups = {C/C++ Librairies},
  url    = {https://github.com/Bulat-Ziganshin/FastECC},
}

@Online{FEC-AL,
  author = {C. Taylor},
  title  = {{FEC-AL}: Forward Error Correction at the Application Layer in {C}},
  year   = {2018},
  groups = {C/C++ Librairies},
  url    = {https://github.com/catid/fecal},
}

@Online{FECpp,
  author = {J. Lloyd},
  title  = {{FECpp}: Erasure Codes based on {Vandermonde} Matrices},
  year   = {2009},
  groups = {C/C++ Librairies},
  url    = {https://github.com/randombit/fecpp},
}

@Online{GNURadio,
  author = {T. Rondeau and J. Blum and J. Corgan and S. Koslowski and E. Blossom and M. Müller and T. O'Shea and B. Reynwar and M. Dickens and A. Rode and R. Economos and M. Braun and others},
  title  = {{GNURadio}: the Free and Open Software Radio Ecosystem},
  year   = {2006},
  groups = {C/C++ Librairies, GPU},
  url    = {https://github.com/gnuradio/gnuradio},
}

@Online{Inan-LDPC,
  author = {A. Inan and J. Schiefer},
  title  = {Playing with Low-Density Parity-Check Codes},
  year   = {2018},
  groups = {C/C++ Librairies},
  url    = {https://github.com/xdsopl/LDPC},
}

@Online{ITpp,
  author = {B. Cristea and T. Ottosson and A. Piątyszek and others},
  title  = {{IT++}},
  year   = {2005},
  groups = {C/C++ Librairies},
  url    = {http://itpp.sourceforge.net},
}

@Online{Neal-LDPC,
  author = {R. {Neal}},
  title  = {Software for Low Density Parity Check codes},
  year   = {2006},
  groups = {C/C++ Librairies},
  url    = {https://github.com/radfordneal/LDPC-codes},
}

@Online{LeGal-LDPC,
  author = {B. {Le Gal}},
  title  = {Fast {LDPC} Decoder for x86},
  year   = {2015},
  groups = {C/C++ Librairies},
  url    = {https://github.com/blegal/Fast_LDPC_decoder_for_x86},
}

@Online{Leopard,
  author = {C. Taylor and E. Nemerson and M. Corallo and M. Al-Bassam},
  title  = {{Leopard-RS}: {MDS} {Reed-Solomon} Erasure Correction Codes for Large Data in {C}},
  year   = {2017},
  groups = {C/C++ Librairies},
  url    = {https://github.com/catid/leopard},
}

@Online{Libcorrect,
  author = {B. Armstrong and L. Teske and P. Noordhuis and T. Petazzoni and J. Carlson and E. Betts},
  title  = {{libcorrect}},
  year   = {2016},
  groups = {C/C++ Librairies},
  url    = {https://github.com/quiet/libcorrect},
}

@Online{OpenAir,
  author = {EURECOM},
  title  = {{OpenAirInterface (OAI)}},
  year   = {2013},
  groups = {C/C++ Librairies},
  url    = {https://gitlab.eurecom.fr/oai/openairinterface5g},
}

@Online{OpenFEC,
  author = {M. Cunche and J. Detchart and J. Lacan and V. Roca and others},
  title  = {{OpenFEC}},
  year   = {2009},
  groups = {C/C++ Librairies},
  url    = {http://openfec.org},
}

@Online{Schifra,
  author = {A. Partow},
  title  = {{Schifra}: {Reed-Solomon} Error Correcting Code Library for Software Applications Implemented in {C++}},
  year   = {2010},
  groups = {C/C++ Librairies},
  url    = {https://github.com/ArashPartow/schifra},
}

@Online{Siamese,
  author = {C. Taylor},
  title  = {{Siamese}: Infinite-Window Streaming Erasure Code ({HARQ})},
  year   = {2018},
  groups = {C/C++ Librairies},
  url    = {https://github.com/catid/siamese},
}

@Online{Tavildar-Polar,
  author = {S. Tavildar},
  title  = {{C} and {MATLAB} Implementations for Polar Codes},
  year   = {2016},
  groups = {C/C++ Librairies},
  url    = {https://github.com/tavildar/Polar},
}

@Online{Tavildar-LDPC,
  author = {S. Tavildar},
  title  = {{C} and {MATLAB} Implementations for {LDPC} Codes},
  year   = {2016},
  groups = {C/C++ Librairies},
  url    = {https://github.com/tavildar/LDPC},
}

@Online{The-art-of-ecc,
  author = {R. H. {Morelos-Zaragoza}},
  title  = {{the-art-of-ecc.com}},
  year   = {2006},
  groups = {C/C++ Librairies},
  url    = {http://www.the-art-of-ecc.com},
}

@Online{TurboFEC,
  author = {T. T. {Pham} and T. Tsou and others},
  title  = {{TurboFEC} - {SIMD} Vectorized {LTE} Turbo and Convolutional Encoders and Decoders},
  year   = {2015},
  groups = {C/C++ Librairies},
  url    = {https://github.com/ttsou/turbofec},
}

@Misc{ETSI2005,
  author   = {ETSI},
  title    = {{EN} 302 307 - Digital Video Broadcasting ({DVB}); Second Generation Framing Structure, Channel Coding and Modulation Systems for Broadcasting, Interactive Services, News Gathering and Other Broadband Satellite Applications ({DVB-S2})},
  month    = mar,
  year     = {2005},
  file     = {:pdf/ETSI2005 - EN 302 307 - Digital Video Broadcasting (DVB)\; Second Generation Framing Structure, Channel Coding and Modulation Systems for Broadcasting, Interactive Services, News Gathering and Other Broadband Satellite Applications (DVB-S2) (V.1.2.1).pdf:PDF},
  groups   = {DVB},
  keywords = {DVB-S2},
  url      = {https://www.etsi.org/deliver/etsi_en/302300_302399/302307/01.02.01_60/en_302307v010201p.pdf},
}

@InProceedings{Broquedis2010,
  author    = {F. {Broquedis} and J. {Clet-Ortega} and S. {Moreaud} and N. {Furmento} and B. {Goglin} and G. {Mercier} and S. {Thibault} and R. {Namyst}},
  title     = {hwloc: A Generic Framework for Managing Hardware Affinities in {HPC} Applications},
  booktitle = {Euromicro Conference on Parallel, Distributed and Network-based Processing (PDP)},
  year      = {2010},
  pages     = {180--186},
  month     = feb,
  publisher = {IEEE},
  abstract  = {The increasing numbers of cores, shared caches and memory nodes within machines introduces a complex hardware topology. High-performance computing applications now have to carefully adapt their placement and behavior according to the underlying hierarchy of hardware resources and their software affinities. We introduce the Hardware Locality (hwloc) software which gathers hardware information about processors, caches, memory nodes and more, and exposes it to applications and runtime systems in a abstracted and portable hierarchical manner. hwloc may significantly help performance by having runtime systems place their tasks or adapt their communication strategies depending on hardware affinities. We show that hwloc can already be used by popular high-performance OpenMP or MPI software. Indeed, scheduling OpenMP threads according to their affinities or placing MPI processes according to their communication patterns shows interesting performance improvement thanks to hwloc. An optimized MPI communication strategy may also be dynamically chosen according to the location of the communicating processes in the machine and its hardware characteristics.},
  doi       = {10.1109/PDP.2010.67},
  file      = {:pdf/Broquedis2010 - hwloc\: A Generic Framework for Managing Hardware Affinities in HPC Applications.pdf:PDF},
  issn      = {2377-5750},
  keywords  = {application program interfaces, message passing, multi-threading, scheduling, hwloc, hardware affinities management, shared caches, memory nodes, complex hardware topology, high-performance computing, software affinity, hardware locality software, multicore processor, runtime system, MPI software, OpenMP thread scheduling, Multicore processing, Application software, Computer architecture, Software libraries, Bandwidth, Concurrent computing, Memory management, Hardware Topology Affinities Placement MPI OpenMP},
}

@Online{gr-dvbs2rx,
  author   = {R. Economos},
  title    = {gr-dvbs2rx: {GNU} Radio Extensions for the {DVB-S2} and {DVB-T2} Standards},
  year     = {2018},
  abstract = {The goal of this project is to build GNU Radio blocks that implement the BICM (Bit-Interleaved Coded Modulation) portion of the DVB-S2 and DVB-T2 standards.},
  groups   = {DVB},
  url      = {https://github.com/drmpeg/gr-dvbs2rx},
}

@Online{leansdr,
  author   = {pabr},
  title    = {leansdr: Lightweight, Portable Software-defined Radio.},
  year     = {2016},
  abstract = {leandvb is a lightweight implementation of portions of the DVB-S and DVB-S2 standards in plain C++. It is developed primarily for receiving Digital Amateur TV from the ISS and from the QO-100 geostationary transponder. },
  groups   = {DVB},
  url      = {https://github.com/pabr/leansdr},
}

@InProceedings{Elias1955,
  author    = {P. Elias},
  title     = {Coding for Two Noisy Chanels},
  booktitle = {IRE Convention Record},
  year      = {1955},
  month     = apr,
  publisher = {IEEE},
  file      = {:pdf/Elias1955 - Coding for Two Noisy Chanels.pdf:PDF},
  keywords  = {Convolutional},
  url       = {http://web.mit.edu/6.441/www/reading/hd2.pdf},
}

@Article{Forney1973,
  author   = {G. D. {Forney}},
  title    = {The Viterbi Algorithm},
  journal  = {Proceedings of the IEEE},
  year     = {1973},
  volume   = {61},
  number   = {3},
  pages    = {268--278},
  month    = mar,
  issn     = {1558-2256},
  abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
  doi      = {10.1109/PROC.1973.9030},
  file     = {:pdf/Forney1973 - The Viterbi Algorithm.pdf:PDF},
  keywords = {Viterbi algorithm, Markov processes, State estimation, Recursive estimation, Digital communication, Algorithm design and analysis, Decoding, Convolutional codes, Stochastic processes},
}

@Article{Elias1954,
  author   = {P. {Elias}},
  title    = {Error-free Coding},
  journal  = {Transactions of the IRE Professional Group on Information Theory},
  year     = {1954},
  volume   = {4},
  number   = {4},
  pages    = {29--37},
  month    = sep,
  issn     = {2168-2704},
  doi      = {10.1109/TIT.1954.1057464},
  file     = {:pdf/Elias1954 - Error-Free Coding.pdf:PDF},
  keywords = {Error probability, Telegraphy, Computer errors, Laboratories, Iterative decoding, Error analysis, Error correction codes, Information theory, Transmitters, Costs},
}

@InProceedings{Shokrollahi2004,
  author    = {A. {Shokrollahi}},
  title     = {Raptor Codes},
  booktitle = {International Symposium on Information Theory (ISIT)},
  year      = {2004},
  pages     = {36-},
  month     = jun,
  publisher = {IEEE},
  abstract  = {This paper exhibits a class of universal Raptor codes: for a given integer k, and any real $\epsiv$>0, Raptor codes in this class produce a potentially infinite stream of symbols such that any subset of symbols of size k(1 + $\epsiv$) is sufficient to recover the original k symbols, with high probability. Each output symbol is generated using O(log(1/$\epsiv$)) operations, and the original symbols are recovered from the collected ones with O(klog(1/$\epsiv$)) operations.},
  doi       = {10.1109/ISIT.2004.1365073},
  file      = {:pdf/Shokrollahi2004 - Raptor Codes.pdf:PDF},
  keywords  = {probability, linear codes, Hamming codes, error statistics, universal Raptor codes, infinite symbol stream, probability distribution, belief propagation decoding algorithm, linear time codes, Decoding, Encoding, Probability distribution, Sampling methods, Hamming weight, Application software, Computer networks, Algorithm design and analysis, Belief propagation, Arithmetic},
}

@Article{Muller1954,
  author   = {D. E. {Muller}},
  title    = {Application of Boolean Algebra to Switching Circuit Design and to Error Detection},
  journal  = {Transactions of the IRE Professional Group on Electronic Computers},
  year     = {1954},
  volume   = {EC-3},
  number   = {3},
  pages    = {6--12},
  month    = sep,
  issn     = {2168-1759},
  abstract = {A solution is sought to the general problem of simplifying switching circuits that have more than one output. The mathematical treatment of the problem applies only to circuits that may be represented by “polynomials” in Boolean algebra. It is shown that certain parts of the multiple output problem for such circuits may be reduced to a single output problem whose inputs are equal in number to the sum of the numbers of inputs and outputs in the original problem. A particularly simple reduction may be effected in the case of two outputs. Various techniques are described for simplifying Boolean expressions, called “+ polynomials,” in which the operation “exclusive or” appears between terms. The methods described are particularly suitable for use with an automatic computer, and have been tested on the Illiac. An unexpected metric relationship is shown to exist between the members of certain classes of “+ polynomials” called “nets.” This relationship may be used for constructing error-detecting codes, provided the number of bits in the code is a power of two.},
  doi      = {10.1109/IREPGELC.1954.6499441},
  file     = {:pdf/Muller1954 - Application of Boolean Algebra to Switching Circuit Design and to Error Detection.pdf:PDF},
}

@Article{Reed1954,
  author   = {I. {Reed}},
  title    = {A Class of Multiple-error-correcting Codes and the Decoding Scheme},
  journal  = {Transactions of the IRE Professional Group on Information Theory},
  year     = {1954},
  volume   = {4},
  number   = {4},
  pages    = {38--49},
  month    = sep,
  issn     = {2168-2704},
  doi      = {10.1109/TIT.1954.1057465},
  file     = {:pdf/Reed1954 - A Class of Multiple-error-correcting Codes and the Decoding Scheme.pdf:PDF},
  keywords = {Decoding, Error correction codes, Laboratories, Testing, Mathematical analysis, Hamming distance, Boolean algebra},
}

@InProceedings{Hocquenghem1959,
  author    = {A. Hocquenghem},
  title     = {Codes correcteurs d'erreurs},
  booktitle = {Chiffres},
  year      = {1959},
  volume    = {2},
  pages     = {147--156},
  address   = {Paris},
  month     = sep,
  file      = {:pdf/Hocquenghem1959 - Codes correcteurs d'erreurs.pdf:PDF},
  url       = {http://kom.aau.dk/~heb/kurser/NOTER/KOFA02.PDF},
}

@Article{Checko2015a,
  author   = {A. {Checko} and H. L. {Christiansen} and Y. {Yan} and L. {Scolari} and G. {Kardaras} and M. S. {Berger} and L. {Dittmann}},
  title    = {Cloud RAN for Mobile Networks -- A Technology Overview},
  journal  = {IEEE Communications Surveys Tutorials},
  year     = {2015},
  volume   = {17},
  number   = {1},
  pages    = {405--426},
  month    = fir,
  issn     = {1553-877X},
  abstract = {Cloud Radio Access Network (C-RAN) is a novel mobile network architecture which can address a number of challenges the operators face while trying to support growing end-user's needs. The main idea behind C-RAN is to pool the Baseband Units (BBUs) from multiple base stations into centralized BBU Pool for statistical multiplexing gain, while shifting the burden to the high-speed wireline transmission of In-phase and Quadrature (IQ) data. C-RAN enables energy efficient network operation and possible cost savings on baseband resources. Furthermore, it improves network capacity by performing load balancing and cooperative processing of signals originating from several base stations. This paper surveys the state-of-the-art literature on C-RAN. It can serve as a starting point for anyone willing to understand C-RAN architecture and advance the research on C-RAN.},
  doi      = {10.1109/COMST.2014.2355255},
  file     = {:pdf/Checko2015a - Cloud RAN for Mobile Networks -- A Technology Overview.pdf:PDF},
  keywords = {cloud computing, cooperative communication, energy conservation, mobile radio, radio access networks, resource allocation, statistical multiplexing, telecommunication computing, telecommunication power management, cloud radio access network, mobile network architecture, baseband unit, multiple base stations, centralized BBU pool, statistical multiplexing gain, high-speed wireline transmission, in-phase and quadrature data, IQ data, energy efficient network, cost saving, baseband resource, load balancing, cooperative signal processing, C-RAN architecture, Base stations, Mobile communication, Mobile computing, Computer architecture, Baseband, Radio access networks, Antennas, Cloud RAN, mobile networks, small cells, eICIC, CoMP, virtualization, in-phase and quadrature (IQ) compression, CPRI},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:AFF3CT\;0\;1\;\;\;A Fast Forward Error Correction Toolbox\;;
1 StaticGroup:Error-Correcting Codes (ECC)\;2\;1\;\;\;\;;
2 StaticGroup:Software Decoders\;0\;1\;\;\;\;;
2 StaticGroup:Hardware Decoders\;0\;1\;\;\;\;;
2 StaticGroup:Polar Codes\;0\;1\;\;\;\;;
2 StaticGroup:Turbo Codes\;0\;1\;\;\;\;;
2 StaticGroup:LDPC Codes\;0\;1\;\;\;\;;
2 StaticGroup:HoF Software Decoders\;2\;0\;\;\;Hall of Fame for Software Decoders\;;
3 StaticGroup:HoF Polar\;2\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SC\;0\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SCAN\;0\;1\;\;\;\;;
4 StaticGroup:HoF Polar - SCL\;0\;1\;\;\;\;;
3 StaticGroup:HoF LDPC\;2\;1\;\;\;\;;
4 StaticGroup:HoF LDPC - BP\;0\;1\;\;\;\;;
4 StaticGroup:HoF LDPC - LP\;0\;1\;\;\;\;;
3 StaticGroup:HoF Turbo\;2\;1\;\;\;\;;
4 StaticGroup:HoF Turbo - MAP\;0\;1\;\;\;\;;
4 StaticGroup:HoF Turbo - FPTD\;0\;1\;\;\;\;;
2 StaticGroup:C/C++ Librairies\;0\;1\;\;\;\;;
1 StaticGroup:Modem\;2\;1\;\;\;\;;
2 StaticGroup:CPM\;0\;1\;\;\;Continuous Phase Modulation\;;
2 StaticGroup:SCMA\;0\;1\;\;\;Sparse Code Multiple Access\;;
1 StaticGroup:Single Instruction Multiple Data (SIMD)\;2\;0\;\;\;\;;
2 StaticGroup:Sort\;2\;1\;\;\;\;;
2 StaticGroup:Wrapper\;2\;1\;\;\;\;;
2 StaticGroup:Kernel\;2\;1\;\;\;\;;
1 StaticGroup:C++\;0\;1\;\;\;\;;
1 StaticGroup:Pseudo-Random Number Generator (PRNG)\;2\;1\;\;\;\;;
1 StaticGroup:Factor Graphs\;2\;1\;\;\;\;;
1 StaticGroup:Cloud-RAN\;0\;1\;\;\;\;;
1 StaticGroup:Standards\;2\;1\;\;\;\;;
2 StaticGroup:4G\;0\;1\;\;\;\;;
2 StaticGroup:5G\;0\;1\;\;\;\;;
2 StaticGroup:DVB\;2\;1\;\;\;\;;
1 StaticGroup:ASIP\;0\;1\;\;\;\;;
1 StaticGroup:Artificial Intelligence\;0\;1\;0xffffffff\;\;\;;
1 StaticGroup:Software-Defined Radio (SDR)\;2\;1\;\;\;\;;
2 StaticGroup:FPGA\;0\;1\;\;\;\;;
2 StaticGroup:DSP\;0\;1\;\;\;\;;
2 StaticGroup:GPP\;2\;1\;\;\;\;;
3 StaticGroup:CPU\;0\;1\;\;\;\;;
3 StaticGroup:GPU\;0\;1\;\;\;\;;
2 StaticGroup:Modeling\;0\;1\;\;\;\;;
2 StaticGroup:Concepts\;0\;1\;\;\;\;;
1 StaticGroup:Dataflow\;2\;1\;\;\;\;;
2 StaticGroup:Languages\;0\;1\;\;\;\;;
2 StaticGroup:Models\;0\;1\;\;\;\;;
}
